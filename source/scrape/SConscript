Import('*')

"""
First, set up google authentication credentials.

Then, get download data from popular python packages by running
- python source/scrape/pypi_downloads/docs/query_monthly_pip_downloads.py
- python source/scrape/pypi_downloads/docs/get_popular_packages.py

To link python packages to github repos, run
- bash source/scrape/pypi_site_info/docs/get_library_json.sh
- python source/derived/collect_github_repos/link_pypi_github.py

To scrape github data, we need to
- Map projects whose names and id's changed by running python source/scrape/extract_github_data/docs/unify_repo_identities.py 
- Find the sequence of project identities that makes sense by running python source/scrape/extract_github_data/docs/filter_repo_identities.py
- Using the project names from the list of project identities, run python source/scrape/extract_github_data/docs/query_github_data.py


DOWNLOAD STUFF
- add this to the above source/scrape/pypi_downloads/docs/pypi_package_downloads.py


then in `push_pr_commit_data` run 
- get_pull_request_commits.py (CONSIDER RERUNNING AT SOME POINT)
- get_push_commits.py (CONSIDER RERUNNING AT SOME POINT)
- get_pull_request_commits_graphql.py (CONSIDER RERUNNING AT SOME POINT)
- get_push_commits_graphql.py (CONSIDER RERUNNING AT SOME POINT)


- source/scrape/pull_request_test_data/get_test_data.py
- source/scrape/github_file_data/collect_file_data.py



then in `collect_commits` run (in parallel)
- get_commit_data_pr.py
- get_commit_data_push.py

then, in parallel, 
in `get_linkedin_profiles` run
- get_linkedin_profile.py
in `get_standardized_locations` run 
- get_standardized_locations.py
in `get_weekly_scorecard_data` run
- `get_ossf_scorecard.py` 
in `get_weekly_truck_factor` run
- get_truck_factor.py
in `link_issue_pull_request` run
- link_using_issue.py (HAVE STARTED)
- link_using_pull_request
"""


#SConscript('get_linkedin_profile/SConscript')

