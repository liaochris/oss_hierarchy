import os
import random
import time
import asyncio
import httpx
import pandas as pd
from collections import defaultdict
from tqdm.asyncio import tqdm

# -------------------------------
# Config
# -------------------------------
API_URL = "https://api.github.com/graphql"

TOKENS = [
    (os.environ["BACKUP2_GITHUB_USERNAME"], os.environ["BACKUP2_GITHUB_TOKEN"]),
    (os.environ["BACKUP3_GITHUB_USERNAME"], os.environ["BACKUP3_GITHUB_TOKEN"]),
    (os.environ["BACKUP7_GITHUB_USERNAME"], os.environ["BACKUP7_GITHUB_TOKEN"]),
    (os.environ["BACKUP8_GITHUB_USERNAME"], os.environ["BACKUP8_GITHUB_TOKEN"]),
    (os.environ["BACKUP9_GITHUB_USERNAME"], os.environ["BACKUP9_GITHUB_TOKEN"])
]

repo_batch_size = defaultdict(lambda: 100)
repo_alias_batch_size = defaultdict(lambda: 100)
query_counter = defaultdict(int)

# -------------------------------
# Queries
# -------------------------------
COMMITS_QUERY = """
query($owner: String!, $repo: String!, $after: String, $batchSize: Int!) {
  repository(owner: $owner, name: $repo) {
    defaultBranchRef {
      target {
        ... on Commit {
          history(first: $batchSize, after: $after) {
            pageInfo { hasNextPage endCursor }
            nodes {
              oid
              committedDate
              messageHeadline
              message
              changedFiles
              additions
              deletions
              author {
                name email user { login }
              }
              committer {
                name email user { login }
              }
              associatedPullRequests(first: 1) {
                nodes {
                  number
                  title
                  createdAt
                  merged
                  mergedAt
                }
              }
            }
          }
        }
      }
    }
  }
}
"""

async def RunQuery(client, user, query, variables, retries=3, delay_seconds=5):
    attempt = 0
    while attempt < retries:
        try:
            resp = await client.post(API_URL, json={"query": query, "variables": variables}, timeout=90)
            try:
                data = resp.json()
            except ValueError:
                raise Exception("TransientError: Empty JSON response")

            # --- Rate limiting ---
            if (resp.status_code == 403 and "rate limit" in resp.text.lower()) or \
               ("errors" in data and any(err.get("type") == "RATE_LIMITED" for err in data["errors"])):
                reset = int(resp.headers.get("X-RateLimit-Reset", time.time() + 60))
                sleep_for = max(0, reset - int(time.time())) + 5
                print(f"‚è≥ [{user}] Rate limit hit, sleeping {sleep_for}s...")
                await asyncio.sleep(sleep_for)
                continue  # do not count against retries

            # --- Gateway errors ---
            if resp.status_code in (502, 504):
                raise Exception(f"GatewayError: {resp.status_code} Gateway error")

            # --- Other HTTP errors ---
            if resp.status_code != 200:
                raise Exception(f"TransientError: HTTP {resp.status_code}: {resp.text[:200]}")

            # --- GraphQL errors ---
            if "errors" in data:
                if any(err.get("type") == "NOT_FOUND" for err in data["errors"]):
                    raise Exception(f"FatalError: GraphQL Error: {data['errors']}")
                raise Exception(f"TransientError: GraphQL Error: {data['errors']}")

            # --- Missing data block ---
            if "data" not in data or data["data"] is None:
                raise Exception("TransientError: Missing data block in GraphQL response")

            return data

        except Exception as error:
            msg = str(error)
            # Fatal errors ‚Üí stop immediately
            if "FatalError" in msg or "NOT_FOUND" in msg:
                print(f"‚ùå [{user}] Fatal error, not retrying: {msg}")
                raise

            attempt += 1
            if attempt < retries:
                print(f"‚ö†Ô∏è [{user}] {msg}, retrying in {delay_seconds}s... (attempt {attempt}/{retries})")
                await asyncio.sleep(delay_seconds)
            else:
                print(f"‚ùå [{user}] Failed after {retries} attempts: {msg}")
                raise


async def SafeRunQuery(client, user, query, variables, batch_size, retries=3, owner_repo=None, is_alias=False):
    gateway_retries = 2  # fewer retries when batch_size == 1

    for attempt in range(retries):
        try:
            return await RunQuery(client, user, query, variables)
        except Exception as e:
            msg = str(e)

            # --- Gateway errors ---
            if "GatewayError" in msg:
                if batch_size > 1 and owner_repo:
                    new_batch = max(1, batch_size // 2)
                    if is_alias:
                        repo_alias_batch_size[owner_repo] = new_batch
                    else:
                        repo_batch_size[owner_repo] = new_batch
                    print(f"‚ö†Ô∏è [{user}] {owner_repo}: Shrinking batch size {batch_size} ‚Üí {new_batch} after gateway error")
                    return await SafeRunQuery(client, user, query, variables, new_batch, retries, owner_repo, is_alias)

                if batch_size == 1:
                    if attempt < gateway_retries:
                        sleep_for = 2 * (attempt + 1)
                        print(f"‚ö†Ô∏è [{user}] Gateway error at batch=1, retry {attempt+1}/{gateway_retries}, sleeping {sleep_for}s...")
                        await asyncio.sleep(sleep_for)
                        continue
                    print(f"‚ùå [{user}] Skipped page after repeated gateway errors at batch=1")
                    return None

            # --- Missing data block with batch_size=1 ‚Üí incremental sleep ---
            if batch_size == 1 and "Missing data block" in msg and attempt < retries - 1:
                sleep_for = 2 * (attempt + 1)
                print(f"‚ö†Ô∏è [{user}] Missing data block, retry {attempt+1}/{retries}, sleeping {sleep_for}s...")
                await asyncio.sleep(sleep_for)
                continue

            # --- Fatal errors bubble up immediately ---
            if "FatalError" in msg or "NOT_FOUND" in msg:
                raise

            # --- Other transient errors ‚Üí normal retry cycle ---
            if attempt < retries - 1:
                sleep_for = 3
                print(f"‚ö†Ô∏è [{user}] {msg}, retrying in {sleep_for}s... (attempt {attempt+1}/{retries})")
                await asyncio.sleep(sleep_for)
                continue

            print(f"‚ùå [{user}] Failed after {retries} attempts: {msg}")
            return None

    return None

# -------------------------------
# Commit Tests Query
# -------------------------------
def BuildCommitTestsQuery(shas):
    parts = []
    for i, sha in enumerate(shas):
        alias = f"c{i}"
        parts.append(f"""
          {alias}: object(oid: "{sha}") {{
            ... on Commit {{
              checkSuites(first: 6) {{
                nodes {{
                  checkRuns(first: 100) {{
                    nodes {{ conclusion }}
                  }}
                }}
              }}
            }}
          }}
        """)
    return f"""
    query($owner: String!, $repo: String!) {{
      repository(owner: $owner, name: $repo) {{
        {"".join(parts)}
      }}
    }}
    """


# -------------------------------
# Fetch Commit Tests
# -------------------------------
async def FetchCommitTestsBatchDynamic(client, user, owner, repo, shas, retries=3):
    batch_size = repo_alias_batch_size[f"{owner}/{repo}"]
    results, start = {}, 0

    while start < len(shas):
        sub_shas = shas[start:start + batch_size]
        query = BuildCommitTestsQuery(sub_shas)
        variables = {"owner": owner, "repo": repo}
        query_counter[f"{owner}/{repo}"] += 1

        data = await SafeRunQuery(client, user, query, variables, batch_size)
        if data is None:
            print(f"‚ùå [{user}] Skipped tests page for {owner}/{repo} after 3 retries (batch size=1)")
            start += batch_size
            continue

        repo_data = data["data"]["repository"]
        for i, sha in enumerate(sub_shas):
            alias = f"c{i}"
            node = repo_data.get(alias)
            passed = failed = skipped = 0
            if node:
                for suite in node["checkSuites"]["nodes"]:
                    for run in suite["checkRuns"]["nodes"]:
                        if run["conclusion"] == "SUCCESS":
                            passed += 1
                        elif run["conclusion"] == "FAILURE":
                            failed += 1
                        elif run["conclusion"] == "SKIPPED":
                            skipped += 1
            results[sha] = (passed, failed, skipped)

        start += batch_size
    return results


# -------------------------------
# Fetch Repo Commits
# -------------------------------
async def FetchRepoCommits(client, user, owner, repo):
    all_results, cursor, commit_counter = [], None, 0
    batch_size = repo_batch_size[f"{owner}/{repo}"]

    while True:
        variables = {"owner": owner, "repo": repo, "after": cursor, "batchSize": batch_size}
        query_counter[f"{owner}/{repo}"] += 1

        data = await SafeRunQuery(client, user, COMMITS_QUERY, variables, batch_size)
        if data is None:
            print(f"‚ùå [{user}] Skipped commits page for {owner}/{repo} after 3 retries (batch size=1)")
            cursor = None if not cursor else cursor  # try moving to next page
            continue

        repo_data = data["data"]["repository"]
        if not repo_data or not repo_data["defaultBranchRef"]:
            break

        history = repo_data["defaultBranchRef"]["target"]["history"]
        commits = history["nodes"]
        shas = [c["oid"] for c in commits]

        test_results = await FetchCommitTestsBatchDynamic(client, user, owner, repo, shas)

        for c in commits:
            commit_counter += 1
            pr = c["associatedPullRequests"]["nodes"]
            passed, failed, skipped = test_results.get(c["oid"], (0, 0, 0))
            all_results.append({
                "repo_name": f"{owner}/{repo}",
                "commit_sha": c["oid"],
                "commit_date": c["committedDate"],
                "commit_message": c["messageHeadline"],
                "commit_message_full": c["message"],
                "files_changed": c["changedFiles"],
                "additions": c["additions"],
                "deletions": c["deletions"],
                "author_name": c["author"]["name"] if c["author"] else None,
                "author_email": c["author"]["email"] if c["author"] else None,
                "author_login": c["author"]["user"]["login"] if c["author"] and c["author"]["user"] else None,
                "committer_name": c["committer"]["name"] if c["committer"] else None,
                "committer_email": c["committer"]["email"] if c["committer"] else None,
                "committer_login": c["committer"]["user"]["login"] if c["committer"] and c["committer"]["user"] else None,
                "pr_number": pr[0]["number"] if pr else None,
                "pr_title": pr[0]["title"] if pr else None,
                "pr_created_at": pr[0]["createdAt"] if pr else None,
                "pr_merged": pr[0]["merged"] if pr else None,
                "pr_merged_at": pr[0]["mergedAt"] if pr else None,
                "tests_passed": passed,
                "tests_failed": failed,
                "tests_skipped": skipped,
            })

        if not history["pageInfo"]["hasNextPage"]:
            break
        cursor = history["pageInfo"]["endCursor"]

    qcount = query_counter[f"{owner}/{repo}"]
    print(f"üèÅ [{user}] {owner}/{repo}: {commit_counter} commits ‚Üí {qcount} queries")
    return all_results


# -------------------------------
# Worker per Token
# -------------------------------
async def Worker(user, token, repos):
    headers = {"Authorization": f"bearer {token}"}
    async with httpx.AsyncClient(http2=True, headers=headers) as client:
        all_results = []
        for repo_name in repos:
            if "/" not in repo_name:
                continue
            owner, repo = repo_name.split("/", 1)
            try:
                results = await FetchRepoCommits(client, user, owner, repo)
                all_results.extend(results)
            except Exception as e:
                print(f"‚ùå [{user}] Failed {repo_name}: {e}")
        return all_results


# -------------------------------
# Coordinator
# -------------------------------
async def ProcessReposAsync(all_repos, user, token):
    if not all_repos:
        return pd.DataFrame()

    results = await Worker(user, token, all_repos)
    return pd.DataFrame(results)


# -------------------------------
# File Processing
# -------------------------------
def ResolveRepoName(repo_name):
    import requests
    url = f"https://github.com/{repo_name}"
    try:
        resp = requests.head(url, allow_redirects=True, timeout=5)
        final_url = resp.url.rstrip("/")
        return "/".join(final_url.split("/")[-2:])
    except Exception:
        return repo_name


import concurrent.futures
from tqdm import tqdm

def ProcessOneFile(fname, input_dir, out_dir, token_tuple):
    in_path = os.path.join(input_dir, fname)
    out_path = os.path.join(out_dir, fname)

    df = pd.read_parquet(in_path)
    if "repo_name" not in df.columns:
        print(f"‚ö†Ô∏è Skipping {fname}, no repo_name column")
        return None

    repos = list(df["repo_name"].dropna().unique())
    repos = list(set([ResolveRepoName(r) for r in repos]))

    user, token = token_tuple
    print(f"‚ñ∂Ô∏è [{user}] Processing {fname} with {len(repos)} repos...")

    try:
        result_df = asyncio.run(ProcessReposAsync(repos, user, token))
    except Exception as e:
        print(f"‚ùå [{user}] {fname} failed: {e}")
        return None

    if not result_df.empty:
        result_df.to_parquet(out_path, index=False)
        print(f"üíæ Wrote commit history for {fname} ‚Üí {out_path}")
        return fname
    return None


def ProcessRepoFiles(input_dir="drive/output/derived/data_export/pr",
                     out_dir="drive/output/scrape/push_pr_commit_data/push_graphql"):
    os.makedirs(out_dir, exist_ok=True)
    fnames = [f for f in os.listdir(input_dir) if f.endswith(".parquet")]
    random.shuffle(fnames)
    fnames = [f for f in fnames if not os.path.exists(os.path.join(out_dir, f))]
    if not fnames:
        print("‚úÖ No new files to process.")
        return

    total_files = len(fnames)
    with concurrent.futures.ProcessPoolExecutor(max_workers=len(TOKENS)) as executor:
        futures = {
            executor.submit(ProcessOneFile, fname, input_dir, out_dir, TOKENS[i % len(TOKENS)]): fname
            for i, fname in enumerate(fnames)
        }

        with tqdm(total=total_files, desc="Processing files", unit="file") as pbar:
            completed, exported = 0, 0
            for f in concurrent.futures.as_completed(futures):
                try:
                    result = f.result()
                    if result:
                        exported += 1
                        pbar.update(1)
                    else:
                        pbar.total -= 1
                        pbar.refresh()
                except Exception as e:
                    print(f"‚ùå File worker crashed: {e}")
                    pbar.total -= 1
                    pbar.refresh()
                completed += 1
                pbar.set_postfix_str(f"Exported {exported}/{pbar.total}")
    print("‚úÖ Finished processing.")


# -------------------------------
# Entrypoint
# -------------------------------
if __name__ == "__main__":
    ProcessRepoFiles()
