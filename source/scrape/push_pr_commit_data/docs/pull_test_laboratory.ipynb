{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2d5a269e-f50c-4e9a-839a-bd3ca0df3572",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup, SoupStrainer\n",
    "import requests\n",
    "from pandarallel import pandarallel\n",
    "import random\n",
    "import os\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65b4ca3f-6195-4426-99bc-bbe7ce68368f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Main():\n",
    "    pandarallel.initialize(progress_bar=True)\n",
    "    \n",
    "    indir_pull = Path('drive/output/scrape/extract_github_data/pull_request_data/')\n",
    "    outdir_pull = Path('drive/output/scrape/push_pr_commit_data/pull_request_data/')\n",
    "    \n",
    "    pr_dirs = []\n",
    "    for subset_year in np.arange(2011, 2024, 1): \n",
    "        for subset_month in np.arange(1, 13, 1):\n",
    "            if (subset_year != 2023) or (subset_year == 2023 and subset_month < 9):\n",
    "                pr_dirs.append(f\"pull_request_{subset_year}_{subset_month}.csv\")\n",
    "    random.shuffle(pr_dirs) # enables parallelization\n",
    "\n",
    "    for pr_data in pr_dirs:\n",
    "        already_scraped_list = os.listdir(outdir_pull)\n",
    "        if pr_data not in already_scraped_list:\n",
    "            df_pull = pd.read_csv(indir_pull / pr_data)[['Unnamed: 0', 'repo_name','pr_commits_url']]\n",
    "            df_pull.to_csv(outdir_pull / pr_data) # as a placeholder\n",
    "            \n",
    "            df_pull['commit_list'] = df_pull.parallel_apply(lambda x: grabCommits(x['repo_name'], x['pr_commits_url']), axis = 1 )\n",
    "            print(f\"Commits for pull_request_data_{subset_year}_{subset_month}.csv obtained\")\n",
    "            df_pull.to_csv(outdir_pull / f\"pull_request_data_{subset_year}_{subset_month}.csv\") # as a placeholder\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c5acb221-4215-4f2b-9f46-eb19aa62f426",
   "metadata": {},
   "outputs": [],
   "source": [
    "def grabCommits(repo, pr_commits_url):\n",
    "    try:\n",
    "        pull_info = \"/\".join(pr_commits_url.split(\"/\")[-3:-1]).replace(\"pulls\",\"pull\")\n",
    "        scrape_url = f\"https://github.com/{repo}/{pull_info}/commits\"\n",
    "        product = SoupStrainer('div', {'id': 'commits_bucket'})\n",
    "        sesh = requests.Session() \n",
    "        page = sesh.get(scrape_url)\n",
    "        page_text = str(page.text)\n",
    "        if \"Please wait a few minutes before you try again\" in page_text:\n",
    "            print('pausing, rate limit hit')\n",
    "            time.sleep(120)\n",
    "        soup = BeautifulSoup(page.content,parse_only = product,features=\"html.parser\")\n",
    "        commits = soup.find_all(\"a\", attrs={\"id\":re.compile(r'commit-details*')})\n",
    "        commit_urls = [c['href'].split(\"/\")[-1] for c in commits]\n",
    "        return commit_urls\n",
    "    except Exception as e:\n",
    "        error = str(e)\n",
    "        return error"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
