{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6e7353ca-7a0a-49bf-b138-d7ea242d1972",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir('../')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "252286ad-d984-4fb0-8e31-b0da5354bee8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import warnings\n",
    "from source.lib.helpers import *\n",
    "from pandarallel import pandarallel\n",
    "import ast\n",
    "from itertools import combinations\n",
    "from collections import defaultdict\n",
    "from functools import reduce\n",
    "import re\n",
    "\n",
    "pd.set_option('display.max_columns', None)\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7ac22f5f-f1a9-425a-b0af-6c29cf722dad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def WeightedQuantile(values, weights, quantiles):\n",
    "    values = np.array(values)\n",
    "    weights = np.array(weights)\n",
    "    sorter = np.argsort(values)\n",
    "    values = values[sorter]\n",
    "    weights = weights[sorter]\n",
    "    cumulative_weight = np.cumsum(weights)\n",
    "    total_weight = cumulative_weight[-1]\n",
    "    return np.interp(np.array(quantiles) * total_weight, cumulative_weight, values)\n",
    "\n",
    "def Assign3Bin(repo):\n",
    "    val = base_wm[repo]\n",
    "    if val <= q33:\n",
    "        return 0\n",
    "    elif val <= q67:\n",
    "        return 1\n",
    "    else:\n",
    "        return 2\n",
    "\n",
    "def ConvertLogKeysToInt(log_dict):\n",
    "    return {int(float(key)): value for key, value in log_dict.items()}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "97954a0d-146a-419d-9ade-65fcd5762c4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_problems_contr_filtered = pd.read_parquet('issue/filtered_problem_data.parquet')\n",
    "df_problems_contr_filtered = df_problems_contr_filtered.query('time_period>=\"2015-01-01\"')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ac211772-3ee9-4ef4-bea7-0884b6f948b5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def ProjectSummaryStats(df_problems_contr_filtered):\n",
    "    df_problems_contr_filtered['contr_count'] = df_problems_contr_filtered['all_actors_period'].apply(lambda x: len(np.fromstring(x)))\n",
    "    df_problems_contr_filtered['problem_count'] = 1\n",
    "    df_problems_contr_filtered['unlinked_issue_count'] = df_problems_contr_filtered['type'].apply(lambda x: x == \"unlinked issue\")\n",
    "    df_problems_contr_filtered['unlinked_pr_count'] = df_problems_contr_filtered['type'].apply(lambda x: x == \"unlinked pr\")\n",
    "    df_problems_contr_filtered['linked_issue_pr_count'] = df_problems_contr_filtered['type'].apply(lambda x: x == \"linked\")\n",
    "    \n",
    "    df_problems_repo_period = (\n",
    "        df_problems_contr_filtered\n",
    "          .groupby(['repo_name', 'time_period'], as_index=False)\n",
    "          .agg(\n",
    "              contr_count=('contr_count','first'),\n",
    "              problem_count=('problem_count','sum'),\n",
    "              unlinked_issue_count=('unlinked_issue_count','sum'),\n",
    "              unlinked_pr_count=('unlinked_pr_count','sum'),\n",
    "              linked_issue_pr_count=('linked_issue_pr_count','sum'),\n",
    "              discussion_count=('total_prob_contr', 'sum'),\n",
    "              contributor_count_problem=('problem_contr_count', 'sum'),\n",
    "          )\n",
    "    )\n",
    "    df_problems_repo_period['discussion_count_per_problem'] = df_problems_repo_period['discussion_count']/df_problems_repo_period['problem_count']\n",
    "    df_problems_repo_period['contributor_count_problem_per_problem'] = df_problems_repo_period['contributor_count_problem']/df_problems_repo_period['problem_count']\n",
    "\n",
    "    # 1) Compute mean + exact percentiles for your metrics\n",
    "    metrics = df_problems_repo_period[[\n",
    "        'contr_count',\n",
    "        'problem_count',\n",
    "        'unlinked_issue_count',\n",
    "        'unlinked_pr_count',\n",
    "        'linked_issue_pr_count',\n",
    "        'discussion_count_per_problem',\n",
    "        'contributor_count_problem_per_problem'\n",
    "    ]]\n",
    "    means = metrics.mean()\n",
    "    qs = metrics.quantile([0.1, 0.25, 0.5, 0.75, 0.9], interpolation='nearest')\n",
    "    metrics_summary = pd.DataFrame({\n",
    "        'Mean': means,\n",
    "        '10th': qs.loc[0.1],\n",
    "        '25th': qs.loc[0.25],\n",
    "        '50th': qs.loc[0.5],\n",
    "        '75th': qs.loc[0.75],\n",
    "        '90th': qs.loc[0.9],\n",
    "    })\n",
    "    \n",
    "    # 2) Compute time periods per project summary\n",
    "    periods = df_problems_repo_period.groupby('repo_name')['time_period'].count()\n",
    "    p_qs = periods.quantile([0.1, 0.25, 0.5, 0.75, 0.9], interpolation='nearest')\n",
    "    periods_summary = pd.DataFrame({\n",
    "        'Mean': periods.mean(),\n",
    "        '10th': p_qs.loc[0.1],\n",
    "        '25th': p_qs.loc[0.25],\n",
    "        '50th': p_qs.loc[0.5],\n",
    "        '75th': p_qs.loc[0.75],\n",
    "        '90th': p_qs.loc[0.9],\n",
    "    }, index=['Time periods per project'])\n",
    "    \n",
    "    # 3) Combine summaries and set labels\n",
    "    summary = pd.concat([metrics_summary, periods_summary])\n",
    "    summary.index = [\n",
    "        'Contributors',\n",
    "        'Problems',\n",
    "        'Unlinked issues',\n",
    "        'Unlinked pull requests',\n",
    "        'Linked issue–pull request pairs',\n",
    "        'Discussions per problem',\n",
    "        'Contributors per problem',\n",
    "        'Time periods per project'\n",
    "    ]\n",
    "    \n",
    "    # 4) Define formatter: drop .00 for integers, else two decimals\n",
    "    def format_value(x):\n",
    "        if pd.isna(x):\n",
    "            return ''\n",
    "        if float(x).is_integer():\n",
    "            return f\"{int(x)}\"\n",
    "        return f\"{x:.2f}\"\n",
    "    \n",
    "    # 5) Build each row of the table\n",
    "    rows = []\n",
    "    for label, row in summary.iterrows():\n",
    "        formatted_values = [format_value(row[col]) for col in summary.columns]\n",
    "        row_line = f\"    {label:<30} & \" + \" & \".join(f\"{val:<6}\" for val in formatted_values) + r\" \\\\\"\n",
    "        rows.append(row_line)\n",
    "    \n",
    "    body = \"\\n\".join(rows)\n",
    "    \n",
    "    # 6) Assemble the subfigure LaTeX\n",
    "    subfigure_code = r\"\"\"\\begin{subfigure}[b]{0.9\\textwidth}\n",
    "      \\centering\n",
    "      \\footnotesize\n",
    "      \\caption{Project summary statistics}\n",
    "      \\label{fig:project-summary-stats}\n",
    "      \\begin{tabular}{@{}l r *{5}{r}@{}}\n",
    "        \\toprule\n",
    "        Metric                        & Mean   & \\multicolumn{5}{c}{Percentiles} \\\\\n",
    "        \\cmidrule(lr){3-7}\n",
    "                                      &        & 10th   & 25th   & 50th   & 75th   & 90th   \\\\\n",
    "        \\midrule\n",
    "    \"\"\" + body + r\"\"\"\n",
    "        \\bottomrule\n",
    "      \\end{tabular}\n",
    "    \\end{subfigure}\"\"\"\n",
    "    \n",
    "    print(subfigure_code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b6a906a8-d4db-4d96-a691-2dd728a562bb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\begin{subfigure}[b]{0.9\\textwidth}\n",
      "      \\centering\n",
      "      \\footnotesize\n",
      "      \\caption{Project summary statistics}\n",
      "      \\label{fig:project-summary-stats}\n",
      "      \\begin{tabular}{@{}l r *{5}{r}@{}}\n",
      "        \\toprule\n",
      "        Metric                        & Mean   & \\multicolumn{5}{c}{Percentiles} \\\\\n",
      "        \\cmidrule(lr){3-7}\n",
      "                                      &        & 10th   & 25th   & 50th   & 75th   & 90th   \\\\\n",
      "        \\midrule\n",
      "        Contributors                   & 106.72 & 9      & 20     & 49     & 120    & 239    \\\\\n",
      "    Problems                       & 204.99 & 11     & 35     & 89     & 257    & 562    \\\\\n",
      "    Unlinked issues                & 110.61 & 4      & 15     & 44     & 146    & 292    \\\\\n",
      "    Unlinked pull requests         & 73.77  & 3      & 10     & 28     & 80     & 170    \\\\\n",
      "    Linked issue–pull request pairs & 20.61  & 0      & 1      & 5      & 20     & 53     \\\\\n",
      "    Discussions per problem        & 4.03   & 2      & 2.76   & 3.67   & 4.82   & 6.46   \\\\\n",
      "    Contributors per problem       & 1.95   & 1.45   & 1.68   & 1.96   & 2.19   & 2.45   \\\\\n",
      "    Time periods per project       & 13.86  & 8      & 10     & 15     & 18     & 18     \\\\\n",
      "        \\bottomrule\n",
      "      \\end{tabular}\n",
      "    \\end{subfigure}\n"
     ]
    }
   ],
   "source": [
    "ProjectSummaryStats(df_problems_contr_filtered)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9ab1264e-b79d-43b2-99e8-db440f027aaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "repo_list = df_problems_contr_filtered['repo_name'].unique().tolist()\n",
    "def GetCommunicationLogs(df_contributors):\n",
    "    df_contributors['communication_log'] = df_contributors['communication_log'].apply(ast.literal_eval)\n",
    "    df_contributors['communication_log'] = df_contributors['communication_log'].apply(ConvertLogKeysToInt)\n",
    "    \n",
    "    df_contr_comm = df_contributors[['repo_name','time_period','actor_id','communication_log']]\n",
    "    df_contr_dept = df_problems_contr_filtered[['repo_name','time_period','departed_actor_id','treatment_period']].drop_duplicates()\n",
    "    df_contr_dept_comm = pd.merge(df_contr_comm, df_contr_dept)\n",
    "    df_contr_dept_comm['dept_ov_comm'] = df_contr_dept_comm.apply(\n",
    "        lambda x: x['communication_log'].get(int(x['departed_actor_id'])), axis = 1)\n",
    "\n",
    "    cooccur_counts = BuildCooccurrenceCounts(df_problems_contr_filtered)\n",
    "    \n",
    "    df_contr_dept_comm['problem_count'] = df_contr_dept_comm.apply(\n",
    "        lambda row: cooccur_counts.get(\n",
    "            (row.repo_name, row.time_period, int(float(row.actor_id)), int(float(row.departed_actor_id))), 0), axis=1)\n",
    "    df_contr_dept_comm.loc[df_contr_dept_comm['problem_count'] ==0, 'dept_ov_comm']= np.nan\n",
    "    return df_contr_dept_comm\n",
    "\n",
    "def BuildCooccurrenceCounts(df_probs):\n",
    "    \"\"\"Returns dict mapping\n",
    "       (repo_name, time_period, actor, departed_actor) → count\n",
    "       for every unordered pair in each row’s all_actors list.\"\"\"\n",
    "    counts = defaultdict(int)\n",
    "    for (_, group) in df_probs.groupby(['repo_name','time_period']):\n",
    "        repo = group.repo_name.iloc[0]\n",
    "        period = group.time_period.iloc[0]\n",
    "        for actors in group.all_actors:\n",
    "            # turn actors-list into a set to avoid dupes\n",
    "            s = set([int(float(a)) for a in actors])\n",
    "            # for every *ordered* pair of distinct actors\n",
    "            for a, b in combinations(s, 2):\n",
    "                counts[(repo, period, a, b)] += 1\n",
    "                counts[(repo, period, b, a)] += 1\n",
    "    return counts\n",
    "\n",
    "df_contributors = pd.read_parquet('drive/output/derived/graph_structure/contributor_characteristics.parquet', filters = [('repo_name',\"in\",repo_list)])\n",
    "df_contr_dept_comm = GetCommunicationLogs(df_contributors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0310ab74-9df3-45ea-b568-26589b8afc10",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_pre_contr = df_contr_dept_comm[df_contr_dept_comm['time_period'] < df_contr_dept_comm['treatment_period']]\n",
    "df_pre_repo = (\n",
    "    df_pre_contr[['repo_name','time_period']].drop_duplicates()\n",
    "    .sort_values(['repo_name','time_period'], ascending=[True,False])\n",
    "    .groupby(['repo_name'], as_index=False)\n",
    "    .head(5)\n",
    ")\n",
    "df_pre_contr = pd.merge(df_pre_contr, df_pre_repo)\n",
    "\n",
    "actor_metrics = (\n",
    "    df_pre_contr\n",
    "    .groupby(['repo_name','actor_id'], as_index=False)\n",
    "    .agg(\n",
    "        agg_dept_ov_comm   = ('dept_ov_comm', 'sum'),\n",
    "        agg_problem_count     = ('problem_count',    'sum'),\n",
    "    )\n",
    ")\n",
    "actor_metrics['agg_dept_ov_comm'] = actor_metrics['agg_dept_ov_comm'].replace(0, np.nan)\n",
    "actor_metrics['agg_dept_ov_comm_per_problem'] = actor_metrics['agg_dept_ov_comm'] / actor_metrics['agg_problem_count']\n",
    "\n",
    "repo_metrics = (\n",
    "    actor_metrics.groupby('repo_name', as_index=False)\n",
    "    .agg(avg_dept_ov_comm_repo_avg = ('agg_dept_ov_comm', 'mean'),\n",
    "         avg_dept_ov_comm_per_problem_repo_avg = ('agg_dept_ov_comm_per_problem','mean'),\n",
    "    )\n",
    ")\n",
    "\n",
    "df_actor_summary = actor_metrics.merge(repo_metrics, on='repo_name')\n",
    "df_actor_summary['actor_id'] = pd.to_numeric(df_actor_summary['actor_id'], errors='coerce')\n",
    "\n",
    "for metric in ['dept_ov_comm', 'dept_ov_comm_per_problem']:\n",
    "    agg_col  = f'agg_{metric}'\n",
    "    avg_col  = f'avg_{metric}_repo_avg'\n",
    "\n",
    "        \n",
    "    df_actor_summary[f'{metric}_2bin']   = (\n",
    "        df_actor_summary[agg_col] > df_actor_summary[avg_col]\n",
    "    ).astype(int)\n",
    "    df_actor_summary[f'{metric}_05avg_2bin']   = (\n",
    "        df_actor_summary[agg_col] > .5*df_actor_summary[avg_col]\n",
    "    ).astype(int)\n",
    "    df_actor_summary[f'{metric}_2avg_2bin']   = (\n",
    "        df_actor_summary[agg_col] > 2*df_actor_summary[avg_col]\n",
    "    ).astype(int)\n",
    "    df_actor_summary[f'{metric}_3avg_2bin']   = (\n",
    "        df_actor_summary[agg_col] > 3*df_actor_summary[avg_col]\n",
    "    ).astype(int)\n",
    "    \n",
    "    df_actor_summary.loc[df_actor_summary[agg_col].isna(), f'{metric}_2bin'] = np.nan\n",
    "    df_actor_summary.loc[df_actor_summary[agg_col].isna(), f'{metric}_05avg_2bin'] = np.nan\n",
    "    df_actor_summary.loc[df_actor_summary[agg_col].isna(), f'{metric}_2avg_2bin'] = np.nan\n",
    "    df_actor_summary.loc[df_actor_summary[agg_col].isna(), f'{metric}_3avg_2bin'] = np.nan\n",
    "\n",
    "    if metric == 'dept_ov_comm_per_problem':\n",
    "        df_actor_summary[[f'{metric}_min_2bin',f'{metric}_min_05avg_2bin',f'{metric}_min_2avg_2bin',f'{metric}_min_3avg_2bin']] = df_actor_summary[\n",
    "        [f'{metric}_2bin',f'{metric}_05avg_2bin',f'{metric}_2avg_2bin',f'{metric}_3avg_2bin']]\n",
    "        \n",
    "        df_actor_summary.loc[df_actor_summary['agg_problem_count']<5, f'{metric}_min_2bin'] = np.nan\n",
    "        df_actor_summary.loc[df_actor_summary['agg_problem_count']<5, f'{metric}_min_05avg_2bin'] = np.nan\n",
    "        df_actor_summary.loc[df_actor_summary['agg_problem_count']<5, f'{metric}_min_2avg_2bin'] = np.nan\n",
    "        df_actor_summary.loc[df_actor_summary['agg_problem_count']<5, f'{metric}_min_3avg_2bin'] = np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7fc5db4a-236c-4b9e-9ada-e1da868ffa3d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_contr_all = df_problems_contr_filtered[['repo_name','time_period','all_actors_period']].drop_duplicates(\n",
    "    ['repo_name','time_period']).explode('all_actors_period').rename(columns={'all_actors_period':'actor_id'}).sort_values(\n",
    "    'time_period').drop_duplicates(['repo_name','actor_id'])\n",
    "df_contr_all['actor_id'] = pd.to_numeric(df_contr_all['actor_id'])\n",
    "\n",
    "df_actor_summary = pd.merge(df_contr_all, df_actor_summary, how = 'outer')\n",
    "df_actor_summary['agg_problem_count'] = df_actor_summary['agg_problem_count'].fillna(0)\n",
    "df_actor_summary['avg_dept_ov_comm_repo_avg'] = (df_actor_summary.groupby('repo_name')['avg_dept_ov_comm_repo_avg']\n",
    "                                           .transform(lambda x: x.ffill().bfill()))\n",
    "df_actor_summary['avg_dept_ov_comm_per_problem_repo_avg'] = (df_actor_summary.groupby('repo_name')['avg_dept_ov_comm_per_problem_repo_avg']\n",
    "                                          .transform(lambda x: x.ffill().bfill()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "07e65652-108c-4286-a36f-81b17f3e2e3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_actor_summary = pd.merge(df_actor_summary, df_contr_dept_comm[['repo_name','treatment_period']].drop_duplicates())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a1ced33f-be4e-4624-8035-d9636ca7e113",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from functools import reduce\n",
    "overview_metrics = [\n",
    "    'dept_ov_comm_2bin',\n",
    "    'dept_ov_comm_05avg_2bin',\n",
    "    'dept_ov_comm_2avg_2bin',\n",
    "    'dept_ov_comm_3avg_2bin',\n",
    "    'dept_ov_comm_per_problem_2bin',\n",
    "    'dept_ov_comm_per_problem_05avg_2bin',\n",
    "    'dept_ov_comm_per_problem_2avg_2bin',\n",
    "    'dept_ov_comm_per_problem_3avg_2bin',\n",
    "    'dept_ov_comm_per_problem_min_2bin',\n",
    "    'dept_ov_comm_per_problem_min_05avg_2bin',\n",
    "    'dept_ov_comm_per_problem_min_2avg_2bin',\n",
    "    'dept_ov_comm_per_problem_min_3avg_2bin'\n",
    "]\n",
    "status_funcs = {\n",
    "    'high': lambda s: s > 0,\n",
    "    'low':  lambda s: s == 0,\n",
    "    'never_communicated': lambda s: s.isna(),\n",
    "    'communicated':       lambda s: s.notna(),\n",
    "    'never_communicated_predep': \"\",\n",
    "}\n",
    "\n",
    "actor_frames = []\n",
    "for metric in overview_metrics:\n",
    "    for status, cond in status_funcs.items():\n",
    "        col_name = f'{status}_{metric}_actors'\n",
    "        if status == \"never_communicated_predep\":\n",
    "            df_temp = (\n",
    "                df_actor_summary[df_actor_summary[metric].isna()]\n",
    "                .query('time_period < treatment_period')\n",
    "                .groupby('repo_name')['actor_id']\n",
    "                .agg(list)\n",
    "                .reset_index()\n",
    "                .rename(columns={'actor_id': col_name})\n",
    "            )\n",
    "        else:   \n",
    "            df_temp = (\n",
    "                df_actor_summary[cond(df_actor_summary[metric])]\n",
    "                .groupby('repo_name')['actor_id']\n",
    "                .agg(list)\n",
    "                .reset_index()\n",
    "                .rename(columns={'actor_id': col_name})\n",
    "            )\n",
    "        actor_frames.append(df_temp)\n",
    "\n",
    "df_actor_lists = reduce(\n",
    "    lambda left, right: pd.merge(left, right, on='repo_name', how='outer'),\n",
    "    actor_frames\n",
    ")\n",
    "\n",
    "for metric in overview_metrics:\n",
    "    for status in status_funcs:\n",
    "        col = f'{status}_{metric}_actors'\n",
    "        df_actor_lists[col] = df_actor_lists[col].apply(lambda x: x if isinstance(x, list) else [])\n",
    "\n",
    "df_pr_issues      = df_problems_contr_filtered[\n",
    "    df_problems_contr_filtered['type'].isin(['linked', 'unlinked pr'])\n",
    "]\n",
    "df_pr_with_actors = pd.merge(df_pr_issues, df_actor_lists, on='repo_name', how='left')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ea8fb80e-3c51-4e59-b6a8-15613426bb6f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "status_map = {'high': 'above',\n",
    " 'low': 'below',\n",
    " 'never_communicated': 'never_comm',\n",
    " 'communicated': 'comm',\n",
    "             'never_communicated_predep':'never_comm_predep'}\n",
    "metric_map = {\n",
    "    'dept_ov_comm_2bin': 'dept_comm_avg',\n",
    "    'dept_ov_comm_05avg_2bin': 'dept_comm_05avg',\n",
    "    'dept_ov_comm_2avg_2bin': 'dept_comm_2avg',\n",
    "    'dept_ov_comm_3avg_2bin': 'dept_comm_3avg',\n",
    "    'dept_ov_comm_per_problem_2bin': 'dept_comm_per_problem_avg',\n",
    "    'dept_ov_comm_per_problem_05avg_2bin': 'dept_comm_per_problem_05avg',\n",
    "    'dept_ov_comm_per_problem_2avg_2bin': 'dept_comm_per_problem_2avg',\n",
    "    'dept_ov_comm_per_problem_3avg_2bin': 'dept_comm_per_problem_3avg',\n",
    "    'dept_ov_comm_per_problem_min_2bin': 'dept_comm_per_problem_min_avg',\n",
    "    'dept_ov_comm_per_problem_min_05avg_2bin': 'dept_comm_per_problem_min_05avg',\n",
    "    'dept_ov_comm_per_problem_min_2avg_2bin': 'dept_comm_per_problem_min_2avg',\n",
    "    'dept_ov_comm_per_problem_min_3avg_2bin': 'dept_comm_per_problem_min_3avg',\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d0c5e384-750e-461f-90b6-7e457c7c0da6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dept_ov_comm_2bin high\n",
      "0.18037486218302096 27210\n",
      "dept_ov_comm_2bin low\n",
      "0.5871979734996103 10264\n",
      "dept_ov_comm_05avg_2bin high\n",
      "0.1317367220593027 30690\n",
      "dept_ov_comm_05avg_2bin low\n",
      "0.6871863005609684 6774\n",
      "dept_ov_comm_2avg_2bin high\n",
      "0.27903991068936645 17915\n",
      "dept_ov_comm_2avg_2bin low\n",
      "0.4288124329022034 19561\n",
      "dept_ov_comm_3avg_2bin high\n",
      "0.3280273630204565 15203\n",
      "dept_ov_comm_3avg_2bin low\n",
      "0.3609015804597701 22272\n",
      "dept_ov_comm_per_problem_2bin high\n",
      "0.4028039681888989 12197\n",
      "dept_ov_comm_per_problem_2bin low\n",
      "0.22485160269093787 25270\n",
      "dept_ov_comm_per_problem_05avg_2bin high\n",
      "0.11769876633021127 32991\n",
      "dept_ov_comm_per_problem_05avg_2bin low\n",
      "0.6234318996415771 4464\n",
      "dept_ov_comm_per_problem_2avg_2bin high\n",
      "0.5977337110481586 1059\n",
      "dept_ov_comm_per_problem_2avg_2bin low\n",
      "0.029242819843342035 36385\n",
      "dept_ov_comm_per_problem_3avg_2bin high\n",
      "0.7982456140350878 114\n",
      "dept_ov_comm_per_problem_3avg_2bin low\n",
      "0.0004554832141039038 37323\n",
      "dept_ov_comm_per_problem_min_2bin high\n",
      "0.36607704049564516 11137\n",
      "dept_ov_comm_per_problem_min_2bin low\n",
      "0.20314498176478946 21113\n",
      "dept_ov_comm_per_problem_min_05avg_2bin high\n",
      "0.07771482214101318 29827\n",
      "dept_ov_comm_per_problem_min_05avg_2bin low\n",
      "0.5639004149377593 2410\n",
      "dept_ov_comm_per_problem_min_2avg_2bin high\n",
      "0.532608695652174 736\n",
      "dept_ov_comm_per_problem_min_2avg_2bin low\n",
      "0.02133739759954277 31494\n",
      "dept_ov_comm_per_problem_min_3avg_2bin high\n",
      "0.6 5\n",
      "dept_ov_comm_per_problem_min_3avg_2bin low\n",
      "0.0 32224\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"df_comm_wide_ov = PivotAndFlattenCommSummary(\\n    df_pr_with_actors, match_col='all_actors', overview_metrics=overview_metrics, base_status=['high', 'low'], \\n    special_status=['never_communicated', 'communicated'], metric_map=metric_map, status_map=status_map)\""
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# --- your existing actor-list build (unchanged) ---\n",
    "def SummarizeMetricStatus(df, metric, status, match_col    ):\n",
    "    actor_col = f'{status}_{metric}_actors'\n",
    "    df[actor_col] = df[actor_col].apply(lambda x: x if isinstance(x, list) else [])\n",
    "    mask = df.apply(\n",
    "        lambda row: (\n",
    "            row['departed_actor_id'] not in row['all_actors']\n",
    "            and bool(set(row[match_col]) & set(row[actor_col]))\n",
    "        ),\n",
    "        axis=1\n",
    "    )\n",
    "    df_filtered = df[mask].drop_duplicates(\n",
    "        ['repo_name','problem_id']).query('type != \"unlinked issue\"')\n",
    "    \n",
    "    summary = (\n",
    "        df_filtered\n",
    "        .groupby(['repo_name', 'time_period'])\n",
    "        .apply(lambda grp: pd.Series({\n",
    "            'prs_opened_count': len(grp),\n",
    "            'contributor_count': len(set().union(*grp[actor_col].tolist(), *grp['all_actors_period'].tolist()))\n",
    "        }))\n",
    "        .reset_index()\n",
    "    )\n",
    "    if status in ['low','high']:\n",
    "        opp_status = 'high' if status == 'low' else 'low'\n",
    "        opp_actor_col = f'{opp_status}_{metric}_actors'\n",
    "        opp_mask = df_filtered.apply(lambda row: (row['departed_actor_id'] not in row['all_actors']\n",
    "                                         and bool(set(row['all_actors']) & set(row[opp_actor_col]))),\n",
    "                            axis=1)\n",
    "        subset_opp_involved = df_filtered[opp_mask]\n",
    "        print(metric, status)\n",
    "        print(subset_opp_involved.shape[0]/df_filtered.shape[0], df_filtered.shape[0])\n",
    "        \n",
    "    summary['overview_metric'] = metric\n",
    "    summary['status'] = status\n",
    "    return summary\n",
    "\n",
    "\n",
    "def PivotAndFlattenCommSummary(df_pr_with_actors, match_col, overview_metrics, base_status, \n",
    "                               special_status, metric_map, status_map):\n",
    "    summaries = [\n",
    "        SummarizeMetricStatus(df_pr_with_actors, m, s, match_col)\n",
    "        for m in overview_metrics for s in base_status\n",
    "    ] + [\n",
    "        SummarizeMetricStatus(df_pr_with_actors, overview_metrics[0], s, match_col)\n",
    "        for s in special_status\n",
    "    ]\n",
    "    df_comm_summary = pd.concat(summaries, ignore_index=True)\n",
    "\n",
    "    df_wide = (\n",
    "        df_comm_summary\n",
    "        .pivot_table(\n",
    "            index=['repo_name', 'time_period'],\n",
    "            columns=['overview_metric', 'status'],\n",
    "            values=['prs_opened_count', 'contributor_count'],\n",
    "            fill_value=0\n",
    "        )\n",
    "        .reset_index()\n",
    "    )\n",
    "\n",
    "    flat_columns = []\n",
    "    for col in df_wide.columns:\n",
    "        if isinstance(col, tuple):\n",
    "            if all(col) and len(col) == 3:\n",
    "                metric_type, metric_name, status = col\n",
    "                flat_prefix = 'prs_opened' if metric_type == 'prs_opened_count' else 'contributors'\n",
    "                mname = metric_map[metric_name]\n",
    "                sname = status_map[status]\n",
    "                flat_columns.append(f'{flat_prefix}_{mname}_{sname}')\n",
    "            else:\n",
    "                flat_columns.append(col[0])\n",
    "        else:\n",
    "            flat_columns.append(col)\n",
    "\n",
    "    df_wide.columns = flat_columns\n",
    "\n",
    "    return df_wide.rename(columns={\n",
    "        'prs_opened_dept_comm_avg_comm':           'prs_opened_dept_comm',\n",
    "        'prs_opened_dept_comm_avg_never_comm':     'prs_opened_dept_never_comm',\n",
    "        'prs_opened_dept_comm_avg_never_comm_predep':     'prs_opened_dept_never_comm_predep',\n",
    "        'contributors_dept_comm_avg_comm':         'contributors_dept_comm',\n",
    "        'contributors_dept_comm_avg_never_comm':   'contributors_dept_never_comm',\n",
    "        'contributors_dept_comm_avg_never_comm_predep':   'contributors_dept_never_comm_predep',\n",
    "    })\n",
    "\n",
    "df_comm_wide = PivotAndFlattenCommSummary(\n",
    "    df_pr_with_actors, match_col='pr_opener', overview_metrics=overview_metrics, base_status=['high', 'low'], \n",
    "    special_status=['never_communicated', 'communicated', 'never_communicated_predep'], metric_map=metric_map, status_map=status_map)\n",
    "\"\"\"df_comm_wide_ov = PivotAndFlattenCommSummary(\n",
    "    df_pr_with_actors, match_col='all_actors', overview_metrics=overview_metrics, base_status=['high', 'low'], \n",
    "    special_status=['never_communicated', 'communicated'], metric_map=metric_map, status_map=status_map)\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0669239b-7819-4a3d-ade8-3bf080fc1a3c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.2246462730218955"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "0.4024080596281432\n",
    "0.2246462730218955"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "57f160a5-7bed-4b57-8cda-9622ec9683ae",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"# people are involved int wice as much activity as they're opening \\nfor avg in ['avg','2avg','3avg']:\\n    sel_cols = [f'prs_opened_dept_comm_{avg}_above',f'prs_opened_dept_comm_{avg}_below']\\n    print(df_comm_wide_ov[sel_cols].sum().sum()/df_comm_wide[sel_cols].sum().sum())\""
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"# people are involved int wice as much activity as they're opening \n",
    "for avg in ['avg','2avg','3avg']:\n",
    "    sel_cols = [f'prs_opened_dept_comm_{avg}_above',f'prs_opened_dept_comm_{avg}_below']\n",
    "    print(df_comm_wide_ov[sel_cols].sum().sum()/df_comm_wide[sel_cols].sum().sum())\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f59bfc96-72a1-4c5e-b0fd-5d9a5b90e357",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_problems_contr_filtered['unimp_actors'] = df_problems_contr_filtered.apply(\n",
    "    lambda row: [actor for actor in row['all_actors'] if actor not in row['important_actors_rolling']], axis=1)\n",
    "df_problems_contr_filtered['problem_unimp_contr_count'] = df_problems_contr_filtered['unimp_actors'].apply(len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73a21a90-d9ad-4808-b8c2-4e7e8d7d3735",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_contributors = df_problems_contr_filtered[['repo_name','time_period','treatment_period','all_actors_period','departed_actor_id']].explode('all_actors_period').sort_values(['repo_name','time_period'])\n",
    "df_project_predeparture_contributors = df_contributors.query('time_period < treatment_period & departed_actor_id != all_actors_period').drop_duplicates(['repo_name','all_actors_period'])\n",
    "df_project_predeparture_contributors = df_project_predeparture_contributors.groupby(['repo_name'])['all_actors_period'].agg(list).reset_index().rename(columns={'all_actors_period':'all_actors_pre_departure'})\n",
    "df_project_nondeparture_contributors = df_contributors.query('departed_actor_id != all_actors_period').drop_duplicates(['repo_name','all_actors_period'])\n",
    "df_project_nondeparture_contributors = df_project_nondeparture_contributors.groupby(['repo_name'])['all_actors_period'].agg(list).reset_index().rename(columns={'all_actors_period':'all_actors_non_departure'})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdda3090-2502-45f6-9442-e8cb6e803fcd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_problems_contr_filtered_predep = pd.merge(df_problems_contr_filtered, df_project_predeparture_contributors)\n",
    "df_problems_contr_filtered_predep = df_problems_contr_filtered_predep.loc[\n",
    "    df_problems_contr_filtered_predep.apply(lambda row: row['all_actors'].size == np.intersect1d(row['all_actors'], row['all_actors_pre_departure']).size, axis=1)\n",
    "]\n",
    "df_problems_contr_filtered_nondep = pd.merge(df_problems_contr_filtered, df_project_nondeparture_contributors)\n",
    "df_problems_contr_filtered_nondep = df_problems_contr_filtered_nondep.loc[\n",
    "    df_problems_contr_filtered_nondep.apply(lambda row: row['all_actors'].size == np.intersect1d(row['all_actors'], row['all_actors_non_departure']).size, axis=1)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78d6ba81-750d-45ef-a179-faf5268e91b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_problems_contr_filtered['contributions_dict'] = df_problems_contr_filtered['contributions_dict'].apply(ast.literal_eval)\n",
    "df_problems_contr_filtered['contributions_dict'] = (df_problems_contr_filtered['contributions_dict'].apply(lambda d: {float(k): v for k, v in d.items()}))\n",
    "df_problems_contr_filtered['total_contributions'] = (df_problems_contr_filtered['contributions_dict'].apply(\n",
    "    lambda contributions_dict: sum(item['contributions'] for key, item in contributions_dict.items()))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0eeec488-fd49-4eb5-81c0-ed47a184f4ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_agg_predep = df_problems_contr_filtered_predep.sort_values(['repo_name','problem_id_num','time_period']).drop_duplicates(\n",
    "    ['repo_name','problem_id']).query('type != \"unlinked issue\"').groupby(\n",
    "    ['repo_name','time_period'])['problem_id'].count().reset_index().rename(columns={'problem_id':'prs_opened_predep'})\n",
    "\n",
    "df_agg_nondep = df_problems_contr_filtered_nondep.sort_values(['repo_name','problem_id_num','time_period']).drop_duplicates(\n",
    "    ['repo_name','problem_id']).query('type != \"unlinked issue\"').groupby(\n",
    "    ['repo_name','time_period'])['problem_id'].count().reset_index().rename(columns={'problem_id':'prs_opened_nondep'})\n",
    "\n",
    "df_agg_prob = df_problems_contr_filtered.sort_values(['repo_name','problem_id_num','time_period']).drop_duplicates(\n",
    "    ['repo_name','problem_id']).query('type != \"unlinked issue\"').groupby(\n",
    "    ['repo_name','time_period'])['problem_id'].count().reset_index().rename(columns={'problem_id':'prs_opened_prob'})\n",
    "\n",
    "df_agg_prs = pd.merge(df_agg_predep, df_agg_nondep, how = 'outer').merge(df_comm_wide, how = 'outer').fillna(0).merge(df_agg_prob, how = 'outer').fillna(0)\n",
    "\n",
    "\n",
    "df_problem_contr_count = df_problems_contr_filtered.query('type != \"unlinked issue\"').groupby(\n",
    "    ['repo_name','time_period'])[['problem_contr_count','problem_unimp_contr_count']].mean().reset_index().rename(\n",
    "    columns={'problem_contr_count':'problem_avg_contr_count', 'problem_unimp_contr_count':'problem_avg_unimp_contr_count'})\n",
    "df_problem_close_time_contr = df_problems_contr_filtered.sort_values(['repo_name','problem_id_num','time_period']).drop_duplicates(\n",
    "    ['repo_name','problem_id']).query('type != \"unlinked issue\"').query('type != \"unlinked issue\"').groupby(\n",
    "    ['repo_name','time_period'])[['close_time','total_contributions']].mean().reset_index()\n",
    "df_agg_prs = pd.merge(df_agg_prs, df_problem_contr_count, how = 'outer').fillna(0).merge(df_problem_close_time_contr, how = 'outer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59a294a2-dc2a-4646-a962-b12dc5619e9a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_problems_contr_filtered['important_contributions_share'] = (df_problems_contr_filtered.apply(\n",
    "    lambda d: sum(v['contributions'] for k, v in d['contributions_dict'].items() if k in d['important_actors_rolling']), axis = 1) / \n",
    "                                                 df_problems_contr_filtered['total_contributions'])\n",
    "df_imp_share = df_problems_contr_filtered.query('type != \"unlinked issue\"').groupby(\n",
    "    ['repo_name','time_period'])[['important_contributions_share']].mean().reset_index()\n",
    "df_agg_prs = pd.merge(df_agg_prs, df_imp_share, how = 'outer').fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d84582b-7d70-49aa-a13f-e866b4a075eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_comment_grouped = (\n",
    "    df_problems_contr_filtered\n",
    "    .sort_values(['repo_name', 'problem_id_num', 'time_period'])\n",
    "    .drop_duplicates(['repo_name', 'problem_id'])\n",
    "    .query('type != \"unlinked issue\"')\n",
    "    .groupby(['repo_name', 'time_period'])\n",
    "    .agg(\n",
    "        review_count=('review_count', 'mean'),\n",
    "        review_comment_count=('review_comment_count', 'mean'),\n",
    "        prop_review_count_na=('review_count', lambda x: x.isna().mean()),\n",
    "        prop_review_comment_count_na=('review_comment_count', lambda x: x.isna().mean()),\n",
    "    )\n",
    "    .reset_index()\n",
    ")\n",
    "\n",
    "df_agg_prs = pd.merge(df_agg_prs, df_comment_grouped, how = 'outer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca1c2e5f-3ae2-4794-90e3-f4c70dd18795",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_agg_prs[['repo_name','time_period','prs_opened_prob','prs_opened_predep','prs_opened_nondep',\n",
    "            'prs_opened_dept_comm','prs_opened_dept_never_comm','prs_opened_dept_never_comm_predep']].head(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1348b4d5-f100-4839-bef4-1912bccff440",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "preperiod_recent = df_project_filtered_group.query('time_period < treatment_period').groupby('repo_name').tail(5)\n",
    "preperiod_recent['other_involved_count'] = preperiod_recent['departed_involved_count'] - preperiod_recent['problem_count']\n",
    "preperiod_recent['uniform_weight'] = 1\n",
    "\n",
    "count_dict = {\n",
    "    'ind_collab': 'problem_count',\n",
    "    'ind_key_collab': 'departed_involved_count',\n",
    "    'ind_other_collab': 'other_involved_count',\n",
    "    'departed_involved': 'problem_count',\n",
    "    'departed_involved_count': 'uniform_weight',\n",
    "    'key_contributor_count': 'uniform_weight',\n",
    "    'total_contributor_count': 'uniform_weight',\n",
    "    'problem_count': 'uniform_weight',\n",
    "    'departed_opened': 'departed_opened_count',\n",
    "    'departed_authored': 'departed_authored_count'\n",
    "}\n",
    "\n",
    "for collab_type, count_col in count_dict.items():\n",
    "    avg_collab = WeightedMean(preperiod_recent[collab_type], preperiod_recent[count_col])\n",
    "    base_wm = preperiod_recent.groupby('repo_name').apply(\n",
    "        lambda df: WeightedMean(df[collab_type], df[count_col], zero_weight_return = 0)\n",
    "    )\n",
    "\n",
    "    above_set = set(base_wm[base_wm > avg_collab].index)\n",
    "    df_project_filtered_group[f\"{collab_type}_2bin\"] = df_project_filtered_group['repo_name'].apply(lambda x: int(x in above_set))\n",
    "\n",
    "    # 3-bin: weighted quantiles\n",
    "    q33, q67 = WeightedQuantile(preperiod_recent[collab_type], preperiod_recent[count_col], [0.33, 0.67])\n",
    "\n",
    "    df_project_filtered_group[f\"{collab_type}_3bin\"] = df_project_filtered_group['repo_name'].apply(Assign3Bin)\n",
    "\n",
    "df_project_filtered_group = df_project_filtered_group.merge(df_agg_prs, how = 'left')\n",
    "df_project_filtered_group[['prs_opened_predep','prs_opened_nondep']] = df_project_filtered_group[['prs_opened_predep','prs_opened_nondep']].fillna(0)\n",
    "df_project_filtered_group.to_parquet('issue/project_collaboration.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "409f150e-8d5f-4959-aed2-c4a0cda273dc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
