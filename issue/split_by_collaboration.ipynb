{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6e7353ca-7a0a-49bf-b138-d7ea242d1972",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir('../')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 333,
   "id": "252286ad-d984-4fb0-8e31-b0da5354bee8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import warnings\n",
    "from source.lib.helpers import *\n",
    "from pandarallel import pandarallel\n",
    "import ast\n",
    "from itertools import combinations\n",
    "from collections import defaultdict\n",
    "from functools import reduce\n",
    "\n",
    "pd.set_option('display.max_columns', None)\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "7ac22f5f-f1a9-425a-b0af-6c29cf722dad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def WeightedQuantile(values, weights, quantiles):\n",
    "    values = np.array(values)\n",
    "    weights = np.array(weights)\n",
    "    sorter = np.argsort(values)\n",
    "    values = values[sorter]\n",
    "    weights = weights[sorter]\n",
    "    cumulative_weight = np.cumsum(weights)\n",
    "    total_weight = cumulative_weight[-1]\n",
    "    return np.interp(np.array(quantiles) * total_weight, cumulative_weight, values)\n",
    "\n",
    "def Assign3Bin(repo):\n",
    "    val = base_wm[repo]\n",
    "    if val <= q33:\n",
    "        return 0\n",
    "    elif val <= q67:\n",
    "        return 1\n",
    "    else:\n",
    "        return 2\n",
    "\n",
    "def ConvertLogKeysToInt(log_dict):\n",
    "    return {int(float(key)): value for key, value in log_dict.items()}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "97954a0d-146a-419d-9ade-65fcd5762c4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_problems_contr_filtered = pd.read_parquet('issue/filtered_problem_data.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "id": "b6a906a8-d4db-4d96-a691-2dd728a562bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ProjectLevelStats(df_problems_contr_filtered):\n",
    "    # controls that I can add down the road\n",
    "    # % of problems that are unlinked prs/linked\n",
    "    df_problems_contr_filtered['departed_involved'] = df_problems_contr_filtered.apply(lambda x: x['departed_actor_id'] in x['all_actors'], axis = 1)\n",
    "    df_problems_contr_filtered['key_contributor_count'] = df_problems_contr_filtered['important_actors_rolling'].apply(len)\n",
    "    df_problems_contr_filtered['total_contributor_count'] = df_problems_contr_filtered['all_actors_period'].apply(len)\n",
    "    df_problems_contr_filtered['departed_opener'] = pd.to_numeric(df_problems_contr_filtered.apply(lambda x: x['departed_actor_id'] in x['pr_opener'] if x['departed_involved'] else np.nan, axis = 1))\n",
    "    df_problems_contr_filtered['departed_author'] = pd.to_numeric(df_problems_contr_filtered.apply(lambda x: x['departed_actor_id'] in x['pr_authors'] if x['departed_involved'] else np.nan, axis = 1))\n",
    "    \n",
    "    df_project_filtered_group = df_problems_contr_filtered.groupby(\n",
    "        ['repo_name', 'time_period', 'treatment_period', 'key_contributor_count', 'total_contributor_count']\n",
    "    ).agg(\n",
    "        problem_count=('problem_id', 'count'),\n",
    "        ind_collab=('ind_collab_roll', 'mean'),\n",
    "        ind_key_collab=('ind_key_collab_roll', 'mean'),\n",
    "        ind_other_collab=('ind_other_collab_roll', 'mean'),\n",
    "        departed_involved_count=('departed_involved','sum'),\n",
    "        departed_involved=('departed_involved','mean'),\n",
    "        departed_opened_count=('departed_opener','sum'),\n",
    "        departed_opened=('departed_opener','mean'), # conditional on involvement\n",
    "        departed_authored_count=('departed_author','sum'),\n",
    "        departed_authored=('departed_author','mean'), # conditional on involvement\n",
    "    ).reset_index()\n",
    "    return df_project_filtered_group\n",
    "\n",
    "df_project_filtered_group = ProjectLevelStats(df_problems_contr_filtered)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 544,
   "id": "9ab1264e-b79d-43b2-99e8-db440f027aaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "repo_list = df_problems_contr_filtered['repo_name'].unique().tolist()\n",
    "def GetCommunicationLogs(df_contributors):\n",
    "    df_contributors = pd.read_parquet('drive/output/derived/graph_structure/contributor_characteristics.parquet', filters = [('repo_name',\"in\",repo_list)])\n",
    "    df_contributors['communication_log'] = df_contributors['communication_log'].apply(ast.literal_eval)\n",
    "    df_contributors['communication_log'] = df_contributors['communication_log'].apply(ConvertLogKeysToInt)\n",
    "    \n",
    "    df_contr_comm = df_contributors[['repo_name','time_period','actor_id','communication_log']]\n",
    "    df_contr_dept = df_problems_contr_filtered[['repo_name','time_period','departed_actor_id','treatment_period']].drop_duplicates()\n",
    "    df_contr_dept_comm = pd.merge(df_contr_comm, df_contr_dept)\n",
    "    df_contr_dept_comm['dept_ov_comm'] = df_contr_dept_comm.apply(\n",
    "        lambda x: x['communication_log'].get(int(x['departed_actor_id'])), axis = 1)\n",
    "\n",
    "    cooccur_counts = BuildCooccurrenceCounts(df_problems_contr_filtered)\n",
    "    \n",
    "    df_contr_dept_comm['problem_count'] = df_contr_dept_comm.apply(\n",
    "        lambda row: cooccur_counts.get(\n",
    "            (row.repo_name, row.time_period, int(float(row.actor_id)), int(float(row.departed_actor_id))), 0), axis=1)\n",
    "    df_contr_dept_comm.loc[df_contr_dept_comm['problem_count'] ==0, 'dept_ov_comm']= np.nan\n",
    "    return df_contr_dept_comm\n",
    "\n",
    "def BuildCooccurrenceCounts(df_probs):\n",
    "    \"\"\"Returns dict mapping\n",
    "       (repo_name, time_period, actor, departed_actor) → count\n",
    "       for every unordered pair in each row’s all_actors list.\"\"\"\n",
    "    counts = defaultdict(int)\n",
    "    for (_, group) in df_probs.groupby(['repo_name','time_period']):\n",
    "        repo = group.repo_name.iloc[0]\n",
    "        period = group.time_period.iloc[0]\n",
    "        for actors in group.all_actors:\n",
    "            # turn actors-list into a set to avoid dupes\n",
    "            s = set([int(float(a)) for a in actors])\n",
    "            # for every *ordered* pair of distinct actors\n",
    "            for a, b in combinations(s, 2):\n",
    "                counts[(repo, period, a, b)] += 1\n",
    "                counts[(repo, period, b, a)] += 1\n",
    "    return counts\n",
    "\n",
    "\n",
    "df_contr_dept_comm = GetCommunicationLogs(df_contributors)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 581,
   "id": "0310ab74-9df3-45ea-b568-26589b8afc10",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_pre_contr = df_contr_dept_comm[df_contr_dept_comm['time_period'] < df_contr_dept_comm['treatment_period']]\n",
    "df_pre_repo = (\n",
    "    df_pre[['repo_name','time_period']].drop_duplicates()\n",
    "    .sort_values(['repo_name','time_period'], ascending=[True,False])\n",
    "    .groupby(['repo_name'], as_index=False)\n",
    "    .head(5)\n",
    ")\n",
    "df_pre_contr = pd.merge(df_pre_contr, df_pre_repo)\n",
    "\n",
    "actor_metrics = (\n",
    "    df_pre_contr\n",
    "    .groupby(['repo_name','actor_id'], as_index=False)\n",
    "    .agg(\n",
    "        agg_dept_ov_comm   = ('dept_ov_comm', 'sum'),\n",
    "        agg_problem_count     = ('problem_count',    'sum'),\n",
    "    )\n",
    ")\n",
    "actor_metrics['agg_dept_ov_comm'] = actor_metrics['agg_dept_ov_comm'].replace(0, np.nan)\n",
    "actor_metrics['agg_dept_ov_comm_per_problem'] = actor_metrics['agg_dept_ov_comm'] / actor_metrics['agg_problem_count']\n",
    "\n",
    "repo_metrics = (\n",
    "    actor_metrics.groupby('repo_name', as_index=False)\n",
    "    .agg(avg_dept_ov_comm_repo_avg = ('agg_dept_ov_comm', 'mean'),\n",
    "         avg_dept_ov_comm_per_problem_repo_avg = ('agg_dept_ov_comm_per_problem','mean'),\n",
    "    )\n",
    ")\n",
    "\n",
    "df_actor_summary = actor_metrics.merge(repo_metrics, on='repo_name')\n",
    "df_actor_summary['actor_id'] = pd.to_numeric(df_actor_summary['actor_id'], errors='coerce')\n",
    "\n",
    "for metric in ['dept_ov_comm', 'dept_ov_comm_per_problem']:\n",
    "    agg_col  = f'agg_{metric}'\n",
    "    avg_col  = f'avg_{metric}_repo_avg'\n",
    "\n",
    "    df_actor_summary[f'{metric}_2bin']   = (\n",
    "        df_actor_summary[agg_col] > df_actor_summary[avg_col]\n",
    "    ).astype(int)\n",
    "    df_actor_summary.loc[df_actor_summary[agg_col].isna(), f'{metric}_2bin'] = np.nan\n",
    "    \n",
    "    if metric == 'dept_ov_comm':\n",
    "        df_actor_summary[f'{metric}_2avg_2bin']   = (\n",
    "            df_actor_summary[agg_col] > 2*df_actor_summary[avg_col]\n",
    "        ).astype(int)\n",
    "        df_actor_summary[f'{metric}_3avg_2bin']   = (\n",
    "            df_actor_summary[agg_col] > 3*df_actor_summary[avg_col]\n",
    "        ).astype(int)\n",
    "        df_actor_summary.loc[df_actor_summary[agg_col].isna(), f'{metric}_2avg_2bin'] = np.nan\n",
    "        df_actor_summary.loc[df_actor_summary[agg_col].isna(), f'{metric}_3avg_2bin'] = np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 582,
   "id": "7fc5db4a-236c-4b9e-9ada-e1da868ffa3d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_contr_all = df_problems_contr_filtered[['repo_name','time_period','all_actors_period']].drop_duplicates(\n",
    "    ['repo_name','time_period']).explode('all_actors_period').rename(columns={'all_actors_period':'actor_id'})\n",
    "df_contr_all['actor_id'] = pd.to_numeric(df_contr_all['actor_id'])\n",
    "\n",
    "df_actor_summary = pd.merge(df_contr_all, df_actor_summary, how = 'left')\n",
    "df_actor_summary['agg_problem_count'] = df_actor_summary['agg_problem_count'].fillna(0)\n",
    "df_actor_summary['avg_dept_ov_comm_repo_avg'] = (df_actor_summary.groupby('repo_name')['avg_dept_ov_comm_repo_avg']\n",
    "                                           .transform(lambda x: x.ffill().bfill()))\n",
    "df_actor_summary['avg_dept_ov_comm_per_problem_repo_avg'] = (df_actor_summary.groupby('repo_name')['avg_dept_ov_comm_per_problem_repo_avg']\n",
    "                                          .transform(lambda x: x.ffill().bfill()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 584,
   "id": "a1ced33f-be4e-4624-8035-d9636ca7e113",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from functools import reduce\n",
    "\n",
    "# --- your existing actor-list build (unchanged) ---\n",
    "def SummarizeMetricStatus(df, metric, status, match_col    ):\n",
    "    actor_col = f'{status}_{metric}_actors'\n",
    "    df[actor_col] = df[actor_col].apply(lambda x: x if isinstance(x, list) else [])\n",
    "    mask = df.apply(\n",
    "        lambda row: (\n",
    "            row['departed_actor_id'] not in row['all_actors']\n",
    "            and bool(set(row[match_col]) & set(row[actor_col]))\n",
    "        ),\n",
    "        axis=1\n",
    "    )\n",
    "    df_filtered = df[mask]\n",
    "    summary = (\n",
    "        df_filtered\n",
    "        .groupby(['repo_name', 'time_period'])\n",
    "        .apply(lambda grp: pd.Series({\n",
    "            'prs_opened_count': len(grp),\n",
    "            'contributor_count': len(set().union(*grp[actor_col].tolist(), *grp['all_actors_period'].tolist()))\n",
    "        }))\n",
    "        .reset_index()\n",
    "    )\n",
    "    if status in ['low','high']:\n",
    "        opp_status = 'high' if status == 'low' else 'low'\n",
    "        opp_actor_col = f'{opp_status}_{metric}_actors'\n",
    "        opp_mask = df_filtered.apply(lambda row: (row['departed_actor_id'] not in row['all_actors']\n",
    "                                         and bool(set(row['all_actors']) & set(row[opp_actor_col]))),\n",
    "                            axis=1)\n",
    "        subset_opp_involved = df_filtered[opp_mask]\n",
    "        print(metric, status)\n",
    "        print(subset_opp_involved.shape[0]/df_filtered.shape[0], df_filtered.shape[0])\n",
    "        \n",
    "    summary['overview_metric'] = metric\n",
    "    summary['status'] = status\n",
    "    return summary\n",
    "\n",
    "overview_metrics = [\n",
    "    'dept_ov_comm_2bin',\n",
    "    'dept_ov_comm_2avg_2bin',\n",
    "    'dept_ov_comm_3avg_2bin',\n",
    "]\n",
    "status_funcs = {\n",
    "    'high': lambda s: s > 0,\n",
    "    'low':  lambda s: s == 0,\n",
    "    'never_communicated': lambda s: s.isna(),\n",
    "    'communicated':       lambda s: s.notna(),\n",
    "}\n",
    "\n",
    "actor_frames = []\n",
    "for metric in overview_metrics:\n",
    "    for status, cond in status_funcs.items():\n",
    "        col_name = f'{status}_{metric}_actors'\n",
    "        df_temp = (\n",
    "            df_actor_summary[cond(df_actor_summary[metric])]\n",
    "            .groupby('repo_name')['actor_id']\n",
    "            .agg(list)\n",
    "            .reset_index()\n",
    "            .rename(columns={'actor_id': col_name})\n",
    "        )\n",
    "        actor_frames.append(df_temp)\n",
    "\n",
    "df_actor_lists = reduce(\n",
    "    lambda left, right: pd.merge(left, right, on='repo_name', how='outer'),\n",
    "    actor_frames\n",
    ")\n",
    "\n",
    "for metric in overview_metrics:\n",
    "    for status in status_funcs:\n",
    "        col = f'{status}_{metric}_actors'\n",
    "        df_actor_lists[col] = df_actor_lists[col].apply(lambda x: x if isinstance(x, list) else [])\n",
    "\n",
    "df_pr_issues      = df_problems_contr_filtered[\n",
    "    df_problems_contr_filtered['type'].isin(['linked', 'unlinked pr'])\n",
    "]\n",
    "df_pr_with_actors = pd.merge(df_pr_issues, df_actor_lists, on='repo_name', how='left')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 585,
   "id": "d0c5e384-750e-461f-90b6-7e457c7c0da6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dept_ov_comm_2bin high\n",
      "0.1449542185237008 40737\n",
      "dept_ov_comm_2bin low\n",
      "0.5313829007185804 13638\n",
      "dept_ov_comm_2avg_2bin high\n",
      "0.21941321021669521 26858\n",
      "dept_ov_comm_2avg_2bin low\n",
      "0.3633357558139535 27520\n",
      "dept_ov_comm_3avg_2bin high\n",
      "0.25706102665785413 22695\n",
      "dept_ov_comm_3avg_2bin low\n",
      "0.30542929292929294 31680\n"
     ]
    }
   ],
   "source": [
    "def PivotAndFlattenCommSummary(df_pr_with_actors, match_col, overview_metrics, base_status, \n",
    "                               special_status, metric_map, status_map):\n",
    "    summaries = [\n",
    "        SummarizeMetricStatus(df_pr_with_actors, m, s, match_col)\n",
    "        for m in overview_metrics for s in base_status\n",
    "    ] + [\n",
    "        SummarizeMetricStatus(df_pr_with_actors, overview_metrics[0], s, match_col)\n",
    "        for s in special_status\n",
    "    ]\n",
    "    df_comm_summary = pd.concat(summaries, ignore_index=True)\n",
    "\n",
    "    df_wide = (\n",
    "        df_comm_summary\n",
    "        .pivot_table(\n",
    "            index=['repo_name', 'time_period'],\n",
    "            columns=['overview_metric', 'status'],\n",
    "            values=['prs_opened_count', 'contributor_count'],\n",
    "            fill_value=0\n",
    "        )\n",
    "        .reset_index()\n",
    "    )\n",
    "\n",
    "    flat_columns = []\n",
    "    for col in df_wide.columns:\n",
    "        if isinstance(col, tuple):\n",
    "            if all(col) and len(col) == 3:\n",
    "                metric_type, metric_name, status = col\n",
    "                flat_prefix = 'prs_opened' if metric_type == 'prs_opened_count' else 'contributors'\n",
    "                mname = metric_map[metric_name]\n",
    "                sname = status_map[status]\n",
    "                flat_columns.append(f'{flat_prefix}_{mname}_{sname}')\n",
    "            else:\n",
    "                flat_columns.append(\n",
    "        else:\n",
    "            flat_columns.append(col)\n",
    "\n",
    "    df_wide.columns = flat_columns\n",
    "\n",
    "    return df_wide.rename(columns={\n",
    "        'prs_opened_dept_comm_avg_comm':           'prs_opened_dept_comm',\n",
    "        'prs_opened_dept_comm_avg_never_comm':     'prs_opened_dept_never_comm',\n",
    "        'contributors_dept_comm_avg_comm':         'contributors_dept_comm',\n",
    "        'contributors_dept_comm_avg_never_comm':   'contributors_dept_never_comm',\n",
    "    })\n",
    "\n",
    "df_comm_wide = PivotAndFlattenCommSummary(\n",
    "    df_pr_with_actors, match_col='pr_opener', overview_metrics=overview_metrics, base_status=['high', 'low'], \n",
    "    special_status=['never_communicated', 'communicated'], metric_map=metric_map, status_map=status_map)\n",
    "df_comm_wide_ov = PivotAndFlattenCommSummary(\n",
    "    df_pr_with_actors, match_col='all_actors', overview_metrics=overview_metrics, base_status=['high', 'low'], \n",
    "    special_status=['never_communicated', 'communicated'], metric_map303=metric_map, status_map=status_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 590,
   "id": "57f160a5-7bed-4b57-8cda-9622ec9683ae",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.046271264367816\n",
      "2.1028540954062303\n",
      "2.092137931034483\n"
     ]
    }
   ],
   "source": [
    "# people are involved int wice as much activity as they're opening \n",
    "for avg in ['avg','2avg','3avg']:\n",
    "    sel_cols = [f'prs_opened_dept_comm_{avg}_above',f'prs_opened_dept_comm_{avg}_below']\n",
    "    print(df_comm_wide_ov[sel_cols].sum().sum()/df_comm_wide[sel_cols].sum().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 597,
   "id": "73a21a90-d9ad-4808-b8c2-4e7e8d7d3735",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_contributors = df_problems_contr_filtered[['repo_name','time_period','treatment_period','all_actors_period','departed_actor_id']].explode('all_actors_period').sort_values(['repo_name','time_period'])\n",
    "df_project_predeparture_contributors = df_contributors.query('time_period < treatment_period & departed_actor_id != all_actors_period').drop_duplicates(['repo_name','all_actors_period'])\n",
    "df_project_predeparture_contributors = df_project_predeparture_contributors.groupby(['repo_name'])['all_actors_period'].agg(list).reset_index().rename(columns={'all_actors_period':'all_actors_pre_departure'})\n",
    "df_project_nondeparture_contributors = df_contributors.query('departed_actor_id != all_actors_period').drop_duplicates(['repo_name','all_actors_period'])\n",
    "df_project_nondeparture_contributors = df_project_nondeparture_contributors.groupby(['repo_name'])['all_actors_period'].agg(list).reset_index().rename(columns={'all_actors_period':'all_actors_non_departure'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 598,
   "id": "bdda3090-2502-45f6-9442-e8cb6e803fcd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_problems_contr_filtered_predep = pd.merge(df_problems_contr_filtered, df_project_predeparture_contributors)\n",
    "df_problems_contr_filtered_predep = df_problems_contr_filtered_predep.loc[\n",
    "    df_problems_contr_filtered_predep.apply(lambda row: row['all_actors'].size == np.intersect1d(row['all_actors'], row['all_actors_pre_departure']).size, axis=1)\n",
    "]\n",
    "df_problems_contr_filtered_nondep = pd.merge(df_problems_contr_filtered, df_project_nondeparture_contributors)\n",
    "df_problems_contr_filtered_nondep = df_problems_contr_filtered_nondep.loc[\n",
    "    df_problems_contr_filtered_nondep.apply(lambda row: row['all_actors'].size == np.intersect1d(row['all_actors'], row['all_actors_non_departure']).size, axis=1)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 599,
   "id": "0eeec488-fd49-4eb5-81c0-ed47a184f4ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_agg_predep = df_problems_contr_filtered_predep.drop_duplicates(['repo_name','problem_id']).query('type != \"unlinked issue\"').groupby(['repo_name','time_period'])['problem_id'].count().reset_index().rename(columns={'problem_id':'prs_opened_predep'})\n",
    "df_agg_nondep = df_problems_contr_filtered_nondep.drop_duplicates(['repo_name','problem_id']).query('type != \"unlinked issue\"').groupby(['repo_name','time_period'])['problem_id'].count().reset_index().rename(columns={'problem_id':'prs_opened_nondep'})\n",
    "df_agg_prs = pd.merge(df_agg_predep, df_agg_nondep, how = 'outer').merge(df_comm_wide, how = 'outer').fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 600,
   "id": "1348b4d5-f100-4839-bef4-1912bccff440",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "preperiod_recent = df_project_filtered_group.query('time_period < treatment_period').groupby('repo_name').tail(5)\n",
    "preperiod_recent['other_involved_count'] = preperiod_recent['departed_involved_count'] - preperiod_recent['problem_count']\n",
    "preperiod_recent['uniform_weight'] = 1\n",
    "\n",
    "count_dict = {\n",
    "    'ind_collab': 'problem_count',\n",
    "    'ind_key_collab': 'departed_involved_count',\n",
    "    'ind_other_collab': 'other_involved_count',\n",
    "    'departed_involved': 'problem_count',\n",
    "    'departed_involved_count': 'uniform_weight',\n",
    "    'key_contributor_count': 'uniform_weight',\n",
    "    'total_contributor_count': 'uniform_weight',\n",
    "    'problem_count': 'uniform_weight',\n",
    "    'departed_opened': 'departed_opened_count',\n",
    "    'departed_authored': 'departed_authored_count'\n",
    "}\n",
    "\n",
    "for collab_type, count_col in count_dict.items():\n",
    "    avg_collab = WeightedMean(preperiod_recent[collab_type], preperiod_recent[count_col])\n",
    "    base_wm = preperiod_recent.groupby('repo_name').apply(\n",
    "        lambda df: WeightedMean(df[collab_type], df[count_col], zero_weight_return = 0)\n",
    "    )\n",
    "\n",
    "    above_set = set(base_wm[base_wm > avg_collab].index)\n",
    "    df_project_filtered_group[f\"{collab_type}_2bin\"] = df_project_filtered_group['repo_name'].apply(lambda x: int(x in above_set))\n",
    "\n",
    "    # 3-bin: weighted quantiles\n",
    "    q33, q67 = WeightedQuantile(preperiod_recent[collab_type], preperiod_recent[count_col], [0.33, 0.67])\n",
    "\n",
    "    df_project_filtered_group[f\"{collab_type}_3bin\"] = df_project_filtered_group['repo_name'].apply(Assign3Bin)\n",
    "\n",
    "df_project_filtered_group = df_project_filtered_group.merge(df_agg_prs, how = 'left')\n",
    "df_project_filtered_group[['prs_opened_predep','prs_opened_nondep']] = df_project_filtered_group[['prs_opened_predep','prs_opened_nondep']].fillna(0)\n",
    "df_project_filtered_group.to_parquet('issue/project_collaboration.parquet')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
