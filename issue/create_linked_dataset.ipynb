{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d3af636e-28bd-4559-bbfe-77d40404901e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir('../')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bb830391-aa8e-4b4b-bfee-5b82ad5059f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from ast import literal_eval\n",
    "from pandarallel import pandarallel\n",
    "from collections import defaultdict\n",
    "#from source.lib.helpers import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9ed7f21d-5a5a-4383-b039-515dfe6b66d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: Pandarallel will run on 8 workers.\n",
      "INFO: Pandarallel will use standard multiprocessing data transfer (pipe) to transfer data between the main process and workers.\n"
     ]
    }
   ],
   "source": [
    "pandarallel.initialize(progress_bar = True, nb_workers = 8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "712840f1-11ca-4c50-add6-f8a7894a0375",
   "metadata": {},
   "outputs": [],
   "source": [
    "repo_list = []\n",
    "repo_filter = [(\"repo_name\", \"in\", repo_list)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c1dbfbd4-3b51-40e9-acdf-1f5d18a16500",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3c5fde7a-9f85-4b90-9bd0-5eb01e8f4903",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "% linked pr, 13.343554\n",
      "% same repo not linked pr, 11.876934\n",
      "% other repo discussion, 2.837078\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e6d1f181-7508-4378-aefc-b3b19c37ca63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "% linked issue, 11.107339\n",
      "% same repo not linked issue, 46.632518\n",
      "% other repo discussion, 7.808827\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "daba3c4f-dcfb-4934-aa1b-85647da538fb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a6124faf-5d90-4cd0-8bbe-fe8b56dcd349",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "93762e38-ca16-477d-96ea-f5f8d7cd839a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8fa62fff-cb94-44cf-b18e-e0f675897688",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "91aff870-c952-4724-a3b4-1bcb030d345a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "076d372a-9927-4262-951f-1bd38d3d5ee3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "181983 total issues\n",
      "24283 linked to a PR (0.13)\n",
      "55322 reference another discussion in the same repo (0.30)\n",
      "\n",
      "234119 total PRs\n",
      "25848 linked to an issue (0.11)\n",
      "119409 reference another discussion in the same repo (0.51)\n"
     ]
    }
   ],
   "source": [
    "total_issues = df_issue_full_links.shape[0]\n",
    "linked_issues = df_issue_full_links[df_issue_full_links['linked_pr'].apply(len)>0].shape[0]\n",
    "referencing_issues = df_issue_full_links[df_issue_full_links['same_repo'].apply(len)>0].shape[0]\n",
    "\n",
    "print(f\"{total_issues} total issues\")\n",
    "print(f\"{linked_issues} linked to a PR ({linked_issues/total_issues:.2f})\")\n",
    "print(f\"{referencing_issues} reference another discussion in the same repo ({referencing_issues/total_issues:.2f})\")\n",
    "\n",
    "total_prs = df_pr_full_links.shape[0]\n",
    "linked_prs = df_pr_full_links[df_pr_full_links['linked_issue'].apply(len)>0].shape[0]\n",
    "referencing_prs = df_pr_full_links[df_pr_full_links['same_repo'].apply(len)>0].shape[0]\n",
    "\n",
    "print(f\"\\n{total_prs} total PRs\")\n",
    "print(f\"{linked_prs} linked to an issue ({linked_prs/total_prs:.2f})\")\n",
    "print(f\"{referencing_prs} reference another discussion in the same repo ({referencing_prs/total_prs:.2f})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "eec25bf2-0fc8-4ce4-9369-34160adde7e9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def BuildStrictLinks(df_issue_full_links, df_pr_full_links):\n",
    "    def FlattenColumnFast(series_of_lists):\n",
    "        return sorted({x for lst in series_of_lists.dropna() for x in lst if pd.notna(x)})\n",
    "\n",
    "    def AggregateRefs(row_refs, base_refs):\n",
    "        if not row_refs:\n",
    "            return base_refs\n",
    "        flat = FlattenColumnFast(pd.Series(row_refs))\n",
    "        return sorted(set(base_refs).union(flat))\n",
    "\n",
    "    def MakeNumeric(list):\n",
    "        return [int(ele) for ele in list]\n",
    "        \n",
    "    pr_lookup = df_pr_full_links.set_index(['repo_name', 'pr_number'])\n",
    "    issue_lookup = df_issue_full_links.set_index(['repo_name', 'issue_number'])\n",
    "    reverse_issue_map = (\n",
    "        df_pr_full_links\n",
    "        .explode('linked_issue')\n",
    "        .dropna(subset=['linked_issue'])\n",
    "        .assign(linked_issue=lambda df: df['linked_issue'].astype(int))\n",
    "        .groupby(['repo_name', 'linked_issue'])\n",
    "        .apply(lambda g: g.to_dict(orient='records'))\n",
    "        .to_dict()\n",
    "    )\n",
    "    \n",
    "    records = []\n",
    "\n",
    "    # --- Process Issues ---\n",
    "    for _, row in df_issue_full_links.sort_values(['repo_name', 'issue_number']).iterrows():\n",
    "        repo = row['repo_name']\n",
    "        issue = MakeNumeric([row['issue_number']])[0]\n",
    "        linked_prs = MakeNumeric(row['linked_pr'])\n",
    "\n",
    "        record = {\n",
    "            'repo_name': repo,\n",
    "            'issues': [issue],\n",
    "            'prs': linked_prs if linked_prs else [],\n",
    "            'same_repo': row['same_repo'],\n",
    "            'other_repo': row['other_repo']\n",
    "        }\n",
    "\n",
    "        # Expand from directly linked PRs\n",
    "        if linked_prs:\n",
    "            pr_rows = [pr_lookup.loc[(repo, pr)] for pr in linked_prs if (repo, pr) in pr_lookup.index]\n",
    "            if pr_rows:\n",
    "                record['same_repo'] = AggregateRefs([r['same_repo'] for r in pr_rows], record['same_repo'])\n",
    "                record['other_repo'] = AggregateRefs([r['other_repo'] for r in pr_rows], record['other_repo'])\n",
    "\n",
    "        # Also check reverse linkage: if any PR links to this issue\n",
    "        reverse_prs_data = reverse_issue_map.get((repo, issue), [])\n",
    "        if reverse_prs_data:\n",
    "            reverse_prs = MakeNumeric([r['pr_number'] for r in reverse_prs_data])\n",
    "            record['prs'] = AggregateRefs([reverse_prs], record['prs'])\n",
    "            record['same_repo'] = AggregateRefs([r['same_repo'] for r in reverse_prs_data], record['same_repo'])\n",
    "            record['other_repo'] = AggregateRefs([r['other_repo'] for r in reverse_prs_data], record['other_repo'])\n",
    "            \n",
    "        records.append(record)\n",
    "\n",
    "    # Track known linked issue sets to avoid duplicate linking\n",
    "    seen_linked_issues = {(tuple(r['issues']), r['repo_name']) for r in records if r['prs']}\n",
    "\n",
    "    # --- Process PRs ---\n",
    "    for _, row in df_pr_full_links.sort_values(['repo_name', 'pr_number']).iterrows():\n",
    "        repo = row['repo_name']\n",
    "        pr = MakeNumeric([row['pr_number']])\n",
    "        linked_issues = MakeNumeric(row['linked_issue'])\n",
    "\n",
    "        if linked_issues:\n",
    "            key = (tuple(linked_issues), repo)\n",
    "            if key in seen_linked_issues:\n",
    "                continue\n",
    "\n",
    "        record = {\n",
    "            'repo_name': repo,\n",
    "            'prs': pr,\n",
    "            'issues': linked_issues if linked_issues else [],\n",
    "            'same_repo': row['same_repo'],\n",
    "            'other_repo': row['other_repo']\n",
    "        }\n",
    "\n",
    "        if linked_issues:\n",
    "            issue_rows = [issue_lookup.loc[(repo, iss)] for iss in linked_issues if (repo, iss) in issue_lookup.index]\n",
    "            if issue_rows:\n",
    "                record['same_repo'] = AggregateRefs([r['same_repo'] for r in issue_rows], record['same_repo'])\n",
    "                record['other_repo'] = AggregateRefs([r['other_repo'] for r in issue_rows], record['other_repo'])\n",
    "            seen_linked_issues.add(key)\n",
    "\n",
    "        records.append(record)\n",
    "\n",
    "    df_strict_links = pd.DataFrame.from_records(records)\n",
    "    df_strict_links['problem_id'] = df_strict_links.apply(lambda x: f\"{x['repo_name']}/{int(min(x['issues']+x['prs']))}\", axis = 1)\n",
    "    df_strict_links['problem_id_num'] = df_strict_links.apply(lambda x: int(min(x['issues']+x['prs'])), axis = 1)\n",
    "    \n",
    "    df_strict_links = df_strict_links.groupby(['repo_name','problem_id','problem_id_num'])[['issues', 'prs', 'same_repo','other_repo']].agg(RemoveDuplicatesFlattened).reset_index()\n",
    "    df_strict_links['type'] = df_strict_links.apply(lambda x: 'linked' if len(x['issues'])>0 and len(x['prs'])>0 else 'unlinked pr' if len(x['prs'])>0 else 'unlinked issue', axis = 1)\n",
    "    df_strict_links['same_repo'] = df_strict_links.apply(lambda x: [ref for ref in x['same_repo'] if ref not in x['issues'] and ref not in x['prs']], axis = 1)\n",
    "    \n",
    "    return df_strict_links.sort_values(['repo_name','problem_id_num']).reset_index(drop = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a0a9cf82-d7ff-42b4-adb9-95766613d481",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/5s/dvrxt95949x1pm_sjxm85lj00000gn/T/ipykernel_63886/1812803801.py:22: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  .apply(lambda g: g.to_dict(orient='records'))\n"
     ]
    }
   ],
   "source": [
    "df_strict_links = BuildStrictLinks(df_issue_full_links, df_pr_full_links)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7cb205ae-9587-44c8-97a5-1dff00ff613f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def PrepareMatchingInputs(df_strict_links):\n",
    "    df_matcheable = df_strict_links.query('type != \"linked\"').copy()\n",
    "\n",
    "    def ComputeSameRepoSubset(row):\n",
    "        val = row['issues'][0] if row['type'] == 'unlinked issue' else row['prs'][0]\n",
    "        return [x for x in row['same_repo'] if x > val] if row['type'] == 'unlinked issue' else [x for x in row['same_repo'] if x < val]\n",
    "\n",
    "    df_matcheable['same_repo_subset'] = df_matcheable.apply(ComputeSameRepoSubset, axis=1)\n",
    "    df_matcheable = df_matcheable[df_matcheable['same_repo_subset'].apply(bool)]\n",
    "\n",
    "    # index: (repo_name, type, val) → same_repo\n",
    "    index = {\n",
    "        (row['repo_name'], row['type'], v): row['same_repo']\n",
    "        for _, row in df_matcheable.iterrows()\n",
    "        for v in (row['issues'] if row['type'] == 'unlinked issue' else row['prs'])\n",
    "    }\n",
    "\n",
    "    return df_matcheable.reset_index(drop=True), index\n",
    "\n",
    "def RunMutualLinking(df_matcheable, index, mutual_same_repo):\n",
    "    df_final_match = []\n",
    "    seen_indices = set()\n",
    "\n",
    "    # Precompute: (repo, type) → {val → index}\n",
    "    valToIndex = defaultdict(dict)\n",
    "    for idx, row in df_matcheable.iterrows():\n",
    "        val_list = row['prs'] if row['type'] == 'unlinked pr' else row['issues']\n",
    "        for val in val_list:\n",
    "            valToIndex[(row['repo_name'], row['type'])][val] = idx\n",
    "\n",
    "    for idx, row in df_matcheable.iterrows():\n",
    "        if idx in seen_indices:\n",
    "            continue\n",
    "\n",
    "        repo = row['repo_name']\n",
    "        typ = row['type']\n",
    "        is_issue = typ == 'unlinked issue'\n",
    "        val = row['issues'][0] if is_issue else row['prs'][0]\n",
    "        opposite_type = 'unlinked pr' if is_issue else 'unlinked issue'\n",
    "\n",
    "        # Filter same_repo_subset for values with mutual link\n",
    "        if mutual_same_repo:\n",
    "            subset = [x for x in row['same_repo_subset'] if val in index.get((repo, opposite_type, x), []) and \n",
    "                      x in valToIndex[(repo, opposite_type)]]\n",
    "        else:\n",
    "            subset = [x for x in row['same_repo_subset'] if x in valToIndex[(repo, opposite_type)]]\n",
    "\n",
    "        if subset:\n",
    "            linked_val = min(subset, key=lambda x: abs(x - val))\n",
    "            linked_idx = valToIndex[(repo, opposite_type)][linked_val]\n",
    "            if linked_idx in seen_indices:\n",
    "                continue\n",
    "\n",
    "            linked_row = df_matcheable.loc[linked_idx]\n",
    "            seen_indices.update({idx, linked_idx})\n",
    "\n",
    "            combined_issues = list(set(row['issues'] + linked_row['issues']))\n",
    "            combined_prs = list(set(row['prs'] + linked_row['prs']))\n",
    "            combined_same_repo = list(set(row['same_repo'] + linked_row['same_repo']) - set(combined_issues + combined_prs))\n",
    "            combined_other_repo = list(set(row['other_repo'] + linked_row['other_repo']))\n",
    "\n",
    "            df_final_match.append({\n",
    "                'repo_name': repo,\n",
    "                'issues': combined_issues,\n",
    "                'prs': combined_prs,\n",
    "                'same_repo': combined_same_repo,\n",
    "                'other_repo': combined_other_repo,\n",
    "                'type': 'linked'\n",
    "            })\n",
    "        else:\n",
    "            seen_indices.add(idx)\n",
    "            row_dict = row.drop('same_repo_subset').to_dict()\n",
    "            df_final_match.append(row_dict)\n",
    "    \n",
    "    df_final_match = pd.DataFrame(df_final_match)\n",
    "    df_final_match['problem_id'] = df_final_match.apply(lambda x: f\"{x['repo_name']}/{int(min(x['issues']+x['prs']))}\", axis = 1)\n",
    "    df_final_match['problem_id_num'] = df_final_match.apply(lambda x: int(min(x['issues']+x['prs'])), axis = 1)\n",
    "\n",
    "\n",
    "    \n",
    "    # Build a set of problem_ids that were already considered for matching\n",
    "    matcheable_problem_ids = set(df_matcheable['problem_id'])\n",
    "    \n",
    "    # Keep only rows that were NOT in either\n",
    "    df_remaining = df_strict_links[~df_strict_links['problem_id'].isin(matcheable_problem_ids)]\n",
    "    df_final_match_links = pd.concat([df_final_match, df_remaining], ignore_index=True)\n",
    "\n",
    "    return df_final_match_links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5975e29f-bd94-4092-812a-065830f26395",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 15.8 s, sys: 196 ms, total: 16 s\n",
      "Wall time: 16.4 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "df_matcheable, index = PrepareMatchingInputs(df_strict_links)\n",
    "df_both_match_links = RunMutualLinking(df_matcheable, index, mutual_same_repo = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "588ed3ed-4a69-46a1-b558-2fc4f2cd3209",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 15.9 s, sys: 262 ms, total: 16.1 s\n",
      "Wall time: 16.6 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "df_matcheable, index = PrepareMatchingInputs(df_strict_links)\n",
    "df_single_match_links = RunMutualLinking(df_matcheable, index, mutual_same_repo = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "1b19732a-de69-49e6-965c-c02c01a8f653",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 count  proportion\n",
      "type                              \n",
      "unlinked pr     208116        0.53\n",
      "unlinked issue  155045        0.40\n",
      "linked           27397        0.07\n",
      "                 count  proportion\n",
      "type                              \n",
      "unlinked pr     194840        0.52\n",
      "unlinked issue  142851        0.38\n",
      "linked           38423        0.10\n",
      "                 count  proportion\n",
      "type                              \n",
      "unlinked pr     188507        0.51\n",
      "unlinked issue  142654        0.39\n",
      "linked           38712        0.10\n"
     ]
    }
   ],
   "source": [
    "print(pd.concat([df_strict_links[['type']].value_counts(), df_strict_links[['type']].value_counts(normalize = True)], axis = 1).round(2))\n",
    "print(pd.concat([df_both_match_links[['type']].value_counts(), df_both_match_links[['type']].value_counts(normalize = True)], axis = 1).round(2))\n",
    "print(pd.concat([df_single_match_links[['type']].value_counts(), df_single_match_links[['type']].value_counts(normalize = True)], axis = 1).round(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f9dfd9d4-9a75-494f-84e5-0d9dffc2b42d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_strict_links.to_parquet('issue/matched_problems_strict.parquet')\n",
    "df_both_match_links.to_parquet('issue/matched_problems_both.parquet')\n",
    "df_single_match_links.to_parquet('issue/matched_problems_single.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "2fc4d8cb-8d8a-46ce-8670-3235a23886b7",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "'break' outside loop (668683560.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[26], line 1\u001b[0;36m\u001b[0m\n\u001b[0;31m    break\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m 'break' outside loop\n"
     ]
    }
   ],
   "source": [
    "break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86acda1d-89e7-4f3d-b765-6517ae690e81",
   "metadata": {},
   "source": [
    "# Total set of problems consists of\n",
    "- Unlinked issues (can't trace or doesn't exist)\n",
    "- Unlinked prs (can't trace or doesn't exist)\n",
    "- Linked issues-pull requests\n",
    "\n",
    "# Starting dataset\n",
    "- Ordered by columns `repo_name`, `problem_number` (problem number is a custom new column cr\n",
    "- Contains column `type in (unlinked issues, unlinked prs, linked)`\n",
    "- Has columns `issues`, `prs`\n",
    "- Has columns `same_repo`, `same_repo`, `other_repo`, `other_repo` \n",
    "- We know that the linked issues-pull requests are 100% correct\n",
    "\n",
    "Now, we want to improve on the set of `Linked issues-pull requests`. This will necessarily decrease the number of unlinked issues and PRs. I don't expect the set of `Linked issues-pull requests` to ever be the whole set of problems because there are PRs that aren't linked to issues, and there are issues that don't require a PR. \n",
    "# Improvement 1 (strict, requires same_repo of issue and pr to both reference each other\n",
    "- If, for an issue, a value is listed in `same_repo` and that values meets the following criteria\n",
    "    1) exceeds `issue_number`\n",
    "    2) for that same `repo_name`, in df_pr_full_links it is listed as a `pr_number`\n",
    "    3) In `df_pr_full_links`, for that `pr_number`, `issue_number` is also mentioned in `same_repo` and that `pr_number` does not have a `linked_issue`\n",
    "    4) If there are multiple numbers that meet thet criteria, pick the one that is closest\n",
    "# Improvement 2 (less strict, does not require same_repo of issue and pr to both reference each other)\n",
    "- If, for an issue, a value is listed in `same_repo` and that values meets the following criteria\n",
    "    1) exceeds `issue_number`\n",
    "    2) for that same `repo_name`, in df_pr_full_links it is listed as a `pr_number`\n",
    "    3) In `df_pr_full_links`, that `pr_number` does not have a `linked_issue`\n",
    "- OR if for a PR, a value is listed in `same_repo` and that values meets the following criteria\n",
    "    1) is less than `pr_number`\n",
    "    2) for that same `repo_name`, in df_pr_full_links it is listed as a `pr_number`\n",
    "    3) In `df_pr_full_links`, that `pr_number` does not have a `linked_issue`\n",
    "- FOR BOTH\n",
    "    4) If there are multiple numbers that meet thet criteria, pick the one that is closest\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcbfc008-bf83-4b35-a576-fe9ef937737e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
