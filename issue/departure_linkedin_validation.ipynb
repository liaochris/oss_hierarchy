{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0cc0cfbb-9a8a-495c-9c3d-0535a54abea1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir('../')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 584,
   "id": "95cfb1c7-57cb-4b08-8b68-490de2d0348c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from ast import literal_eval\n",
    "import numpy as np\n",
    "import wrds\n",
    "pd.set_option('display.max_columns', None)\n",
    "from rapidfuzz import fuzz\n",
    "import re\n",
    "from source.scrape.get_linkedin_profiles.docs.get_linkedin_profiles import *\n",
    "from joblib import Parallel, delayed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 477,
   "id": "91e1fecc-d4a1-4ba0-9517-a1d66f5d9716",
   "metadata": {},
   "outputs": [],
   "source": [
    "indir_locations = Path(\"drive/output/scrape/get_standardized_locations\")\n",
    "indir_profiles = Path('drive/output/scrape/get_linkedin_profiles')\n",
    "indir_departures = Path('drive/output/derived/contributor_stats/departed_contributors')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 478,
   "id": "376c4044-197a-4357-9c1d-54afe540b2f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all_departures = AggregateAllDepartures(indir_departures)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "813ed001-21ee-46e1-a537-6c04d838d76f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "github_profile_locations = pd.read_csv(indir_locations / \"standardized_locations_github.csv\", index_col = 0)\n",
    "linkedin_profile_locations = pd.read_csv(indir_locations / \"standardized_locations_linkedin.csv\", index_col = 0)\n",
    "\n",
    "departed_committers = pd.read_csv(indir_profiles / 'departed_github_profiles.csv', index_col = 0)\n",
    "departed_linkedin_profiles = pd.read_parquet(indir_profiles / 'departed_linkedin_profiles.parquet')\n",
    "departed_linkedin_positions = pd.read_parquet(indir_profiles / 'departed_linkedin_positions.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "570420b0-d36b-4a08-ac4a-9a4a1bd46e55",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/user/20506/ipykernel_391209/1377185294.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  departed_committers_clean['name_clean'] = departed_committers_clean['name_clean'].replace('[nan]',np.nan)\n",
      "/tmp/user/20506/ipykernel_391209/1377185294.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  departed_committers_clean['name_clean'] = departed_committers_clean['name_clean'].apply(lambda x: literal_eval(x) if not pd.isnull(x) else x)\n"
     ]
    }
   ],
   "source": [
    "departed_committers_clean = departed_committers[['repo_name','actor_id','name_clean','linkedin_url','company','location']]\n",
    "departed_committers_clean['name_clean'] = departed_committers_clean['name_clean'].replace('[nan]',np.nan)\n",
    "departed_committers_clean['name_clean'] = departed_committers_clean['name_clean'].apply(lambda x: literal_eval(x) if not pd.isnull(x) else x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "7b883bba-6faf-4c27-9848-450a63fbae46",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for col in ['standardized_location_address', 'standardized_location_raw']:\n",
    "    github_profile_locations[col] = github_profile_locations[col].apply(lambda x: literal_eval(x) if not pd.isnull(x) else x)\n",
    "    linkedin_profile_locations[col] = linkedin_profile_locations[col].apply(lambda x: literal_eval(x) if not pd.isnull(x) else x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "36e88862-e847-4724-b6fc-bf0f4c5cdc7d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "departed_committers_location = pd.merge(departed_committers_clean, github_profile_locations.drop('standardized_location', axis = 1))\n",
    "departed_committers_location.reset_index(inplace = True)\n",
    "departed_committers_location['fullname'] = departed_committers_location['name_clean']\n",
    "departed_committers_location = departed_committers_location.explode('fullname')\n",
    "departed_profiles_location = pd.merge(departed_linkedin_profiles, linkedin_profile_locations.drop('standardized_location', axis = 1), how = 'left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 412,
   "id": "34478921-ef5b-4f3c-94d0-454ba0ee6a1c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "departed_profiles_location_merge = departed_profiles_location.rename({'standardized_location_address':'standardized_linkedin_address', 'standardized_location_raw':'raw_linkedin_address'}, axis = 1)\\\n",
    "    [['user_id','fullname','standardized_linkedin_address','raw_linkedin_address','profile_linkedin_url']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 413,
   "id": "de6b5e87-1317-47f5-a056-0dc23b49f449",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13502 unique committers to start with\n",
      "7115 unique committers can be matched to linkedin profiles\n",
      "5742 unique committers have GH and linkedin locations, or a GH company name\n"
     ]
    }
   ],
   "source": [
    "# first, see if there's an exact location match\n",
    "print(\"{} unique committers to start with\".format(departed_committers_location['index'].unique().shape[0]))\n",
    "merged_committers = pd.merge(departed_committers_location, departed_profiles_location_merge, on = 'fullname')\n",
    "print(\"{} unique committers can be matched to linkedin profiles\".format(merged_committers['index'].unique().shape[0]))\n",
    "merged_committers['location_intersection'] = merged_committers.apply(lambda x: set(x['standardized_location_address']).intersection(x['standardized_linkedin_address']) if type(x['standardized_location_address']) == list\n",
    "                                                                     and type(x['standardized_linkedin_address']) == list else [], axis = 1)\n",
    "merged_committers = merged_committers[merged_committers.apply(lambda x: type(x['standardized_location_address']) == list and type(x['standardized_linkedin_address']) == list or\n",
    "                                                              not pd.isnull(x['company']), axis = 1)]\n",
    "print(\"{} unique committers have GH and linkedin locations, or a GH company name\".format(merged_committers['index'].unique().shape[0]))\n",
    "merged_committers['exact_match'] = merged_committers.assign(location_intersection_length = merged_committers['location_intersection'].apply(lambda x: len(x)>0)).groupby(['repo_name','actor_id'])['location_intersection_length']\\\n",
    "    .transform('any')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 414,
   "id": "c8783ad7-de02-418b-875e-e97dc779eb6e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4396 unique committers have a potential location match or a GH company name\n"
     ]
    }
   ],
   "source": [
    "# next, filter out different countries and states\n",
    "merged_committers['github_country'] = merged_committers['standardized_location_raw'].apply(\n",
    "    lambda x: set(list([country['address_components'][-1]['long_name'] for country in x])) if type(x) != float else set())\n",
    "merged_committers['github_state'] = merged_committers['standardized_location_raw'].apply(\n",
    "    lambda x: set(list([country['address_components'][-2]['long_name'] for country in x if len(country['address_components'])>1])) if type(x) != float else set())\n",
    "merged_committers['linkedin_country'] = merged_committers['raw_linkedin_address'].apply(\n",
    "    lambda x: set(list([country['address_components'][-1]['long_name'] for country in x])) if type(x) != float else set())\n",
    "merged_committers['linkedin_state'] = merged_committers['raw_linkedin_address'].apply(\n",
    "    lambda x: set(list([country['address_components'][-2]['long_name'] for country in x if len(country['address_components'])>1])) if type(x) != float else set())\n",
    "for region in ['country','state']:\n",
    "    merged_committers[f'{region}_overlap'] = merged_committers.apply(lambda x: x[f'github_{region}'].intersection(x[f'linkedin_{region}']), axis = 1)\n",
    "merged_committers = merged_committers[merged_committers.apply(lambda x: (len(x['country_overlap'])>0 and len(x['state_overlap'])>0) or x['exact_match'] or\n",
    "                                                              (not pd.isnull(x['company']) and type(x['standardized_location_address']) == float or type(x['standardized_linkedin_address']) == float), axis = 1)]\n",
    "print(\"{} unique committers have a potential location match or a GH company name\".format(merged_committers['index'].unique().shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 415,
   "id": "ab6b3aaa-602a-4b06-9376-33a3e9ce9135",
   "metadata": {},
   "outputs": [],
   "source": [
    "# if a perfect match exists, filter out non perfect matches\n",
    "merged_committers = merged_committers[merged_committers.apply(lambda x: not (x['exact_match'] and len(x['location_intersection'])==0), axis = 1)]\n",
    "departed_positions = departed_linkedin_positions[departed_linkedin_positions['user_id'].isin(merged_committers['user_id'].tolist())]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 416,
   "id": "364fd6e7-aee7-41cc-ba1b-65c15ebcee4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading library list...\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "params = dict()\n",
    "company_list = tuple(departed_positions['rcid'].dropna().drop_duplicates().tolist())\n",
    "params['company_list'] = company_list\n",
    "\n",
    "db = wrds.Connection(wrds_username=\"chrisliao\")\n",
    "company_mappings = db.raw_sql(\"\"\"SELECT * FROM revelio.company_mapping WHERE rcid in %(company_list)s\"\"\", params = params)\n",
    "db.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 417,
   "id": "3034a40a-91c1-436f-8445-0a378bb885dc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3154 unique committers have a potential location match or a potential GH company match\n"
     ]
    }
   ],
   "source": [
    "departed_company_mappings = pd.merge(departed_positions, company_mappings[['rcid','company','ultimate_parent_company_name']])[['user_id','company','ultimate_parent_company_name']].drop_duplicates()\\\n",
    "    .rename({'company':'linkedin_company_name'}, axis = 1)\n",
    "company_list = departed_company_mappings.groupby('user_id').agg({'linkedin_company_name':list, 'ultimate_parent_company_name':list}).reset_index()\n",
    "merged_committers['merge_count'] = merged_committers.groupby(['repo_name','actor_id'])['name_clean'].transform('count')\n",
    "merged_committers = pd.merge(merged_committers, company_list, how = 'left')\n",
    "# filter out those we can't match by company (linkedin, github)\n",
    "merged_committers = merged_committers[merged_committers.apply(lambda x:  not (x['merge_count']>1 and type(x['linkedin_company_name']) == float or pd.isnull(x['company'])), axis = 1)]\n",
    "print(\"{} unique committers have a potential location match or a potential GH company match\".format(merged_committers['index'].unique().shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 418,
   "id": "a6f4aaeb-585a-412f-b230-b0640ff5d54e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "merged_committers['linkedin_company_matches'] = merged_committers.apply(\n",
    "    lambda x: [(comp_name, round(fuzz.ratio(x['company'].lower(), comp_name.lower()), 2)) for comp_name in x['linkedin_company_name']] \n",
    "    if type(x['linkedin_company_name']) == list and not pd.isnull(x['company']) else np.nan, axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 419,
   "id": "e5fae3c5-93ae-42f5-bc2f-fe415ae0177d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1975 unique committers have a potential location match & a potential GH company match\n"
     ]
    }
   ],
   "source": [
    "merged_committers['company_match'] = merged_committers['linkedin_company_matches'].apply(lambda x: any([score[1]>45 for score in x]) if type(x) == list else False)\n",
    "merged_committers['max_score'] = merged_committers['linkedin_company_matches'].apply(lambda x: max([score[1] for score in x]) if type(x) == list else np.nan)\n",
    "merged_committers = merged_committers.query('company_match | max_score.isna()')\n",
    "merged_committers['merge_count'] = merged_committers.groupby(['repo_name','actor_id'])['name_clean'].transform('count')\n",
    "merged_committers = pd.merge(merged_committers, departed_linkedin_profiles[['user_id','updated_dt']])\n",
    "print(\"{} unique committers have a potential location match & a potential GH company match\".format(merged_committers['index'].unique().shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 420,
   "id": "abf3e9e4-ac54-4a2d-a240-a5d96d06b6ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_committers = merged_committers.sort_values(['max_score','updated_dt'], ascending = False).drop_duplicates(['repo_name','actor_id'])\n",
    "merged_committers = merged_committers.drop(['name_clean','linkedin_url','index'], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 428,
   "id": "6d6bcae2-0257-40ee-af9f-6a739f86264c",
   "metadata": {},
   "outputs": [],
   "source": [
    "departed_committers_linkedin = pd.merge(departed_committers_clean, merged_committers, how = 'left')[['repo_name','actor_id','linkedin_url','profile_linkedin_url','user_id']]\n",
    "departed_committers_linkedin['linkedin_url'] = departed_committers_linkedin.apply(\n",
    "    lambda x: x['linkedin_url'] if not pd.isnull(x['linkedin_url']) else x['profile_linkedin_url'], axis = 1)\n",
    "departed_committers_linkedin.drop('profile_linkedin_url', axis = 1, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 443,
   "id": "81352a45-ece6-4710-bc00-80e085b0a4eb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pattern = r'^(?:https?:\\/\\/)?(?:www\\.)?linkedin\\.com/(.*)$'\n",
    "departed_committers_linkedin['linkedin_url'] = departed_committers_linkedin['linkedin_url'].apply(lambda x: re.sub(pattern, r'linkedin.com/\\1', x) if not pd.isnull(x) else x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 448,
   "id": "53c506d9-8838-4e55-bf58-6d5894e96a14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading library list...\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "params = dict()\n",
    "linkedin_profiles = tuple(departed_committers_linkedin.query('~linkedin_url.isna() & user_id.isna()')['linkedin_url'].tolist())\n",
    "params['linkedin_profiles'] = linkedin_profiles\n",
    "\n",
    "db = wrds.Connection(wrds_username=\"chrisliao\")\n",
    "github_linkedin_profiles = db.raw_sql(\"\"\"SELECT * FROM revelio.individual_user WHERE profile_linkedin_url in %(linkedin_profiles)s\"\"\", params = params)\n",
    "db.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 461,
   "id": "d8ca4f2e-1ec3-444f-b7fc-383777018407",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "profile_user_dict = github_linkedin_profiles[['profile_linkedin_url','user_id']].set_index('profile_linkedin_url').to_dict()['user_id']\n",
    "departed_committers_linkedin['user_id'] = departed_committers_linkedin.apply(lambda x: profile_user_dict.get(x['linkedin_url'], x['user_id']), axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 475,
   "id": "e11cf6f7-1dbe-4f2e-ac99-c349e8b991ce",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading library list...\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "db = wrds.Connection(wrds_username=\"chrisliao\")\n",
    "departed_linkedin_profiles_all = QueryForLinkedinPositions(departed_committers_linkedin.dropna(), db)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 531,
   "id": "4a5a6daf-5a6e-44ba-87a9-78b2097ada68",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 1. get positions for relevant users\n",
    "# 2. see if in the hypothesized departure period, they changed jobs\n",
    "#departed_linkedin_profiles_all.groupby('user_id')['\n",
    "for datecol in ['startdate','enddate']:\n",
    "    departed_linkedin_profiles_all[datecol] = pd.to_datetime(departed_linkedin_profiles_all[datecol])\n",
    "departed_job_change_dates = departed_linkedin_profiles_all.groupby('user_id').agg({'startdate':list, 'enddate':list}).reset_index()\n",
    "for datecol in ['startdate','enddate']:\n",
    "    departed_job_change_dates[datecol] = departed_job_change_dates[datecol].apply(lambda x: list(set([ele.date() for ele in x if not pd.isnull(ele)])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 535,
   "id": "6ed4b8b5-bb2f-49b4-84b7-94ac996369b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "departed_full = pd.merge(departed_committers_linkedin, departed_job_change_dates, how = 'left')\n",
    "for datecol in ['startdate','enddate']:\n",
    "    departed_full[datecol] = departed_full[datecol].apply(lambda x: np.nan if type(x) == list and len(x) == 0 else x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 607,
   "id": "7524c68b-edd6-4e55-b9b8-650917848375",
   "metadata": {},
   "outputs": [],
   "source": [
    "def SummarizeSpecification(df_specification, idx, time_period, rolling_window, departed_full, indir_departures):\n",
    "    criteria_pct = df_specifications.loc[idx,'criteria_pct']\n",
    "    consecutive_periods = df_specifications.loc[idx,'consecutive_periods']\n",
    "    post_period_length = df_specifications.loc[idx,'post_period_length']\n",
    "    decline_type = df_specifications.loc[idx,'decline_type']\n",
    "    decline_stat = df_specifications.loc[idx,'decline_stat']\n",
    "    if decline_stat == 0 or decline_type == \"threshold_gap_qty\":\n",
    "        decline_stat = int(decline_stat)\n",
    "    \n",
    "    df_departed = pd.read_parquet(indir_departures / f'departed_contributors_major_months{time_period}_window{rolling_window}D_criteria_commits_{criteria_pct}pct_consecutive{consecutive_periods}_post_period{post_period_length}_{decline_type}_{decline_stat}.parquet')\n",
    "    df_departure_range = CleanDepartures(df_departed, decline_type)\n",
    "    df_departure_range = pd.merge(df_departure_range, departed_full, how = 'left')\n",
    "    df_departure_range['startdate_dep'] = df_departure_range.apply(\n",
    "        lambda x: any([x['time_range'][0] <= ele <= x['time_range'][1] for ele in x['startdate']]) if type(x['startdate']) == list else False, axis = 1)\n",
    "    df_departure_range['enddate_dep'] = df_departure_range.apply(\n",
    "        lambda x: any([x['time_range'][0] <= ele <= x['time_range'][1] for ele in x['enddate']]) if type(x['enddate']) == list else False, axis = 1)\n",
    "    df_departure_range['corresponding_departure'] = df_departure_range.apply(\n",
    "        lambda x: x['startdate_dep'] or x['enddate_dep'], axis = 1)\n",
    "    departure_count = df_departure_range.shape[0]\n",
    "    linkedin_match_pct = 1-df_departure_range['user_id'].isna().mean()\n",
    "    departure_occurred_pct = df_departure_range.query('~user_id.isna()')['corresponding_departure'].mean()\n",
    "    \n",
    "    return [time_period, rolling_window, 'commits', criteria_pct, consecutive_periods, post_period_length, decline_type, decline_stat,\\\n",
    "            departure_count, linkedin_match_pct, departure_occurred_pct]\n",
    "\n",
    "def CleanDepartures(df_departed, decline_type):\n",
    "    if decline_type == \"threshold_gap_qty\":\n",
    "        df_departed = df_departed.query('below_qty_mean_gap0 == 1 | below_qty_mean_gap1 == 1')\n",
    "\n",
    "    df_departed = pd.merge(df_departed, df_departed.query('time_period == final_period')\\\n",
    "                           [['repo_name','actor_id','grouped_index']].rename({'grouped_index':'final_index'}, axis = 1))\n",
    "    if decline_type == \"threshold_gap_qty\":\n",
    "        df_departed['final_index'] = df_departed.apply(\n",
    "            lambda x: x['final_index'] if x['below_qty_mean_gap0'] == 1 else x['final_index']+1, axis = 1)\n",
    "    df_departed = pd.merge(df_departed.drop('final_period', axis = 1), df_departed.query('grouped_index == final_index')[['actor_id','repo_name','time_period']]\\\n",
    "                          .rename({'time_period':'final_period'}, axis = 1))\n",
    "    df_departed['first_post_period_index'] = df_departed['final_index'] + 1\n",
    "    df_departed['relative_time'] = (df_departed['grouped_index'] - df_departed['final_index'])-1\n",
    "\n",
    "        \n",
    "    df_departed['time_period'] = pd.to_datetime(df_departed['time_period'])\n",
    "    df_departure_range = df_departed.query('relative_time == -1 | relative_time == 0')\\\n",
    "        .groupby(['repo_name','actor_id']).agg({'time_period':list}).reset_index()\n",
    "    df_departure_range['time_range'] = df_departure_range['time_period'].apply(lambda x: [x[0].date(), x[1].date()])\n",
    "\n",
    "    return df_departure_range.drop('time_period', axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df47110f-7eca-4f64-a94e-e9283a7d96f0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 630,
   "id": "cd6928a8-f88c-4453-b610-16341b971af6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>repo_name</th>\n",
       "      <th>abandoned_date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>007gzs/django_restframework_apiview</td>\n",
       "      <td>2021-07-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0compute/yanc</td>\n",
       "      <td>2015-07-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>1e0ng/simhash</td>\n",
       "      <td>2022-07-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>4Catalyzer/flask-resty-tenants</td>\n",
       "      <td>2020-07-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <td>ARMmbed/mbed-cloud-sdk-python</td>\n",
       "      <td>2021-01-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23556</th>\n",
       "      <td>zerok/celery-prometheus-exporter</td>\n",
       "      <td>2020-01-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23573</th>\n",
       "      <td>zertrin/clikraken</td>\n",
       "      <td>2021-07-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23588</th>\n",
       "      <td>zopefoundation/Products.ZCTextIndex</td>\n",
       "      <td>2020-07-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23596</th>\n",
       "      <td>zyga/guacamole</td>\n",
       "      <td>2016-01-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23614</th>\n",
       "      <td>zzzsochi/trans</td>\n",
       "      <td>2017-01-01</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1662 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                 repo_name abandoned_date\n",
       "9      007gzs/django_restframework_apiview     2021-07-01\n",
       "14                           0compute/yanc     2015-07-01\n",
       "34                           1e0ng/simhash     2022-07-01\n",
       "45          4Catalyzer/flask-resty-tenants     2020-07-01\n",
       "58           ARMmbed/mbed-cloud-sdk-python     2021-01-01\n",
       "...                                    ...            ...\n",
       "23556     zerok/celery-prometheus-exporter     2020-01-01\n",
       "23573                    zertrin/clikraken     2021-07-01\n",
       "23588  zopefoundation/Products.ZCTextIndex     2020-07-01\n",
       "23596                       zyga/guacamole     2016-01-01\n",
       "23614                       zzzsochi/trans     2017-01-01\n",
       "\n",
       "[1662 rows x 2 columns]"
      ]
     },
     "execution_count": 630,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.read_parquet('drive/output/derived/project_outcomes/abandoned_projects/abandoned_projects_consecutive_req2_permanentTrue.parquet')\n",
    "df_departed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 609,
   "id": "7c8c2331-1fe7-4a29-84cc-ac305ede8906",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 732\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<timed exec>:10: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 1828\n",
      "3 732\n",
      "3 1828\n",
      "6 732\n",
      "6 1828\n",
      "CPU times: user 1min 2s, sys: 3.47 s, total: 1min 5s\n",
      "Wall time: 57.8 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "departure_validation_cols = ['time_period','rolling_window','criteria_col','criteria_pct','consecutive_periods',\n",
    "                             'post_period_length','decline_type','decline_stat', 'departure_count', 'linkedin_match_pct', 'departure_occurred_pct']\n",
    "df_departure_linkedin_validation = pd.DataFrame(columns = departure_validation_cols)\n",
    "for time_period in [2,3,6]:\n",
    "    for rolling_window in [732, 1828]:\n",
    "        print(time_period, rolling_window)\n",
    "        df_specifications = pd.read_csv(indir_departures / f'departed_contributors_specification_summary_major_months{time_period}_window{rolling_window}D.csv').query('criteria_col == \"commits\"')\n",
    "        results = [SummarizeSpecification(df_specifications, idx, time_period, rolling_window, departed_full, indir_departures) for idx in df_specifications.index]\n",
    "        tmp_df = pd.DataFrame(results, columns=departure_validation_cols)\n",
    "        df_departure_linkedin_validation = pd.concat(\n",
    "            [df_departure_linkedin_validation, tmp_df],\n",
    "            ignore_index=True\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 620,
   "id": "c6af3de4-8846-4274-a47e-ebf5b909502c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>time_period</th>\n",
       "      <th>rolling_window</th>\n",
       "      <th>criteria_col</th>\n",
       "      <th>criteria_pct</th>\n",
       "      <th>consecutive_periods</th>\n",
       "      <th>post_period_length</th>\n",
       "      <th>decline_type</th>\n",
       "      <th>decline_stat</th>\n",
       "      <th>departure_count</th>\n",
       "      <th>linkedin_match_pct</th>\n",
       "      <th>departure_occurred_pct</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>732</td>\n",
       "      <td>commits</td>\n",
       "      <td>75</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>threshold_mean</td>\n",
       "      <td>0.2</td>\n",
       "      <td>2523</td>\n",
       "      <td>0.195402</td>\n",
       "      <td>0.176471</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>732</td>\n",
       "      <td>commits</td>\n",
       "      <td>75</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>threshold_mean</td>\n",
       "      <td>0.1</td>\n",
       "      <td>1545</td>\n",
       "      <td>0.194175</td>\n",
       "      <td>0.206667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>732</td>\n",
       "      <td>commits</td>\n",
       "      <td>75</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>threshold_mean</td>\n",
       "      <td>0.0</td>\n",
       "      <td>458</td>\n",
       "      <td>0.194323</td>\n",
       "      <td>0.179775</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>732</td>\n",
       "      <td>commits</td>\n",
       "      <td>75</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>threshold_gap_qty</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1536</td>\n",
       "      <td>0.147786</td>\n",
       "      <td>0.215859</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2</td>\n",
       "      <td>732</td>\n",
       "      <td>commits</td>\n",
       "      <td>75</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>threshold_gap_qty</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2458</td>\n",
       "      <td>0.164768</td>\n",
       "      <td>0.187654</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>395</th>\n",
       "      <td>6</td>\n",
       "      <td>1828</td>\n",
       "      <td>commits</td>\n",
       "      <td>90</td>\n",
       "      <td>6</td>\n",
       "      <td>4</td>\n",
       "      <td>threshold_mean</td>\n",
       "      <td>0.2</td>\n",
       "      <td>300</td>\n",
       "      <td>0.203333</td>\n",
       "      <td>0.360656</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>396</th>\n",
       "      <td>6</td>\n",
       "      <td>1828</td>\n",
       "      <td>commits</td>\n",
       "      <td>90</td>\n",
       "      <td>6</td>\n",
       "      <td>4</td>\n",
       "      <td>threshold_mean</td>\n",
       "      <td>0.1</td>\n",
       "      <td>206</td>\n",
       "      <td>0.228155</td>\n",
       "      <td>0.382979</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>397</th>\n",
       "      <td>6</td>\n",
       "      <td>1828</td>\n",
       "      <td>commits</td>\n",
       "      <td>90</td>\n",
       "      <td>6</td>\n",
       "      <td>4</td>\n",
       "      <td>threshold_mean</td>\n",
       "      <td>0.0</td>\n",
       "      <td>28</td>\n",
       "      <td>0.107143</td>\n",
       "      <td>0.666667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>398</th>\n",
       "      <td>6</td>\n",
       "      <td>1828</td>\n",
       "      <td>commits</td>\n",
       "      <td>90</td>\n",
       "      <td>6</td>\n",
       "      <td>4</td>\n",
       "      <td>threshold_gap_qty</td>\n",
       "      <td>0.0</td>\n",
       "      <td>87</td>\n",
       "      <td>0.172414</td>\n",
       "      <td>0.333333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>399</th>\n",
       "      <td>6</td>\n",
       "      <td>1828</td>\n",
       "      <td>commits</td>\n",
       "      <td>90</td>\n",
       "      <td>6</td>\n",
       "      <td>4</td>\n",
       "      <td>threshold_gap_qty</td>\n",
       "      <td>5.0</td>\n",
       "      <td>146</td>\n",
       "      <td>0.212329</td>\n",
       "      <td>0.451613</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>400 rows × 11 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    time_period rolling_window criteria_col criteria_pct consecutive_periods  \\\n",
       "0             2            732      commits           75                   3   \n",
       "1             2            732      commits           75                   3   \n",
       "2             2            732      commits           75                   3   \n",
       "3             2            732      commits           75                   3   \n",
       "4             2            732      commits           75                   3   \n",
       "..          ...            ...          ...          ...                 ...   \n",
       "395           6           1828      commits           90                   6   \n",
       "396           6           1828      commits           90                   6   \n",
       "397           6           1828      commits           90                   6   \n",
       "398           6           1828      commits           90                   6   \n",
       "399           6           1828      commits           90                   6   \n",
       "\n",
       "    post_period_length       decline_type  decline_stat departure_count  \\\n",
       "0                    3     threshold_mean           0.2            2523   \n",
       "1                    3     threshold_mean           0.1            1545   \n",
       "2                    3     threshold_mean           0.0             458   \n",
       "3                    3  threshold_gap_qty           0.0            1536   \n",
       "4                    3  threshold_gap_qty           5.0            2458   \n",
       "..                 ...                ...           ...             ...   \n",
       "395                  4     threshold_mean           0.2             300   \n",
       "396                  4     threshold_mean           0.1             206   \n",
       "397                  4     threshold_mean           0.0              28   \n",
       "398                  4  threshold_gap_qty           0.0              87   \n",
       "399                  4  threshold_gap_qty           5.0             146   \n",
       "\n",
       "     linkedin_match_pct  departure_occurred_pct  \n",
       "0              0.195402                0.176471  \n",
       "1              0.194175                0.206667  \n",
       "2              0.194323                0.179775  \n",
       "3              0.147786                0.215859  \n",
       "4              0.164768                0.187654  \n",
       "..                  ...                     ...  \n",
       "395            0.203333                0.360656  \n",
       "396            0.228155                0.382979  \n",
       "397            0.107143                0.666667  \n",
       "398            0.172414                0.333333  \n",
       "399            0.212329                0.451613  \n",
       "\n",
       "[400 rows x 11 columns]"
      ]
     },
     "execution_count": 620,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# have to exclude abandoned projects\n",
    "df_departure_linkedin_validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 621,
   "id": "e3435d44-2c71-4c07-8265-c4087e75275e",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'asd' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[621], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43masd\u001b[49m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'asd' is not defined"
     ]
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
