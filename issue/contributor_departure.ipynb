{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "78d92bc4-d550-4cf9-b10e-68269158b29f",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir('../')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1115164b-ccc5-44ee-9ca5-ace225bffc2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: Pandarallel will run on 8 workers.\n",
      "INFO: Pandarallel will use Memory file system to transfer data between the main process and workers.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import sys\n",
    "import glob\n",
    "import warnings\n",
    "import random\n",
    "from pandarallel import pandarallel\n",
    "from source.lib.JMSLab import autofill\n",
    "from source.lib.helpers import *\n",
    "from ast import literal_eval\n",
    "from concurrent.futures import ThreadPoolExecutor, ProcessPoolExecutor\n",
    "from glob import glob \n",
    "import datetime\n",
    "import itertools\n",
    "import time\n",
    "from multiprocessing import pool\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "pd.set_option('display.max_columns', None)\n",
    "pandarallel.initialize(progress_bar = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "2472dd52-b405-453e-94a2-7bb5d33f5912",
   "metadata": {},
   "outputs": [],
   "source": [
    "def CleanCommittersInfo(indir_committers_info):\n",
    "    # TODO: edit file so it can handle pushes\n",
    "    df_committers_info = pd.read_csv(indir_committers_info / 'committers_info_pr.csv', index_col = 0).dropna()\n",
    "    df_committers_info['committer_info'] = df_committers_info['committer_info'].apply(literal_eval)\n",
    "    # TODO: handle cleaning so that it can handle the other cases\n",
    "    df_committers_info = df_committers_info[df_committers_info['committer_info'].apply(lambda x: len(x)==4)]\n",
    "    df_committers_info['actor_name'] = df_committers_info['committer_info'].apply(lambda x: x[0])\n",
    "    df_committers_info['actor_id'] = df_committers_info['committer_info'].apply(lambda x: x[1])\n",
    "\n",
    "    committers_match = df_committers_info[['name','email','user_type','actor_name','actor_id']].drop_duplicates()\n",
    "    committers_match.rename({'actor_id':'commit_author_id'}, axis = 1, inplace = True)\n",
    "\n",
    "    return committers_match\n",
    "\n",
    "def LinkPRCommits(df_pr_selected, df_pr_commits_selected, committers_match, commit_cols):\n",
    "    # TODO: what % of commits were dropped because nobody could be found\n",
    "    matched_commits = pd.merge(df_pr_commits_selected, committers_match,\n",
    "                               how = 'inner', left_on = ['commit author name','commit author email'],\n",
    "                               right_on = ['name','email'])\n",
    "    matched_commits = matched_commits.assign(commits=1)\n",
    "    \n",
    "    matched_commits_total = matched_commits.groupby(['repo_name','pr_number'])\\\n",
    "        [commit_cols].sum()\n",
    "    matched_commits_total.columns = [col + ' total' for col in commit_cols]\n",
    "    matched_commits_share = pd.merge(\n",
    "        matched_commits,\n",
    "        matched_commits_total.reset_index(), on = ['repo_name','pr_number'])\n",
    "    \n",
    "    for col in commit_cols:\n",
    "        matched_commits_share[f\"{col} share\"] = matched_commits_share[col]/matched_commits_share[f\"{col} total\"]\n",
    "\n",
    "    final_agg_cols = commit_cols + [f\"{col} share\" for col in commit_cols]\n",
    "    commit_stats = matched_commits_share\\\n",
    "        .assign(commits=1)\\\n",
    "        .groupby(['repo_name','pr_number','commit_author_id'])\\\n",
    "        [final_agg_cols].sum().reset_index()\n",
    "    \n",
    "    merged_commits = df_pr_selected.query('pr_action == \"closed\" & ~pr_merged_by_id.isna()')\n",
    "    # TODO: what % of commits had truncated information bc 250 max - also, is that push or PR? \n",
    "    # TODO: what % of commits could we not get information for\n",
    "    df_commit_stats = pd.merge(merged_commits, commit_stats, on = ['repo_name','pr_number'])\n",
    "\n",
    "    return df_commit_stats\n",
    "\n",
    "def LinkIssuePR(df_issue_selected, df_linked_issues):\n",
    "    df_linked_issues = df_linked_issues.query('linked_pull_request != \"list index out of range\"')\n",
    "    df_linked_issues['linked_pr_number'] = df_linked_issues['linked_pull_request'].apply(lambda x: x.split(\"/\")[-1])\n",
    "    df_issue_pr = df_linked_issues[['repo_name','issue_number', 'linked_pr_number']].drop_duplicates()\n",
    "\n",
    "    df_issue_selected = pd.merge(df_issue_selected, df_issue_pr, how = 'left', on = ['repo_name','issue_number'])\n",
    "\n",
    "    return df_issue_selected\n",
    "\n",
    "def FilterDuplicateIssues(df, query):\n",
    "    df_sel = df.query(query)\\\n",
    "        .sort_values(['repo_name','issue_number','created_at'])\\\n",
    "        [['repo_name','actor_id', 'issue_user_id','issue_number', \n",
    "          'issue_comment_id', 'created_at', 'linked_pr_number']]\\\n",
    "        .dropna(subset = ['issue_number'])\\\n",
    "        .dropna(axis=1, how='all')\\\n",
    "        .drop_duplicates()\n",
    "\n",
    "    return df_sel\n",
    "\n",
    "def ImputeTimePeriod(df, time_period_months):\n",
    "    t = time_period_months\n",
    "    df['created_at'] = pd.to_datetime(df['created_at'])\n",
    "    df['year'] = df['created_at'].apply(lambda x: x.year)\n",
    "    \n",
    "    df['period'] = df['created_at'].apply(lambda x: int(x.month>6))\n",
    "    df['time_period'] = df['created_at'].apply(lambda x: datetime.date(x.year, int(t*(x.month/t if x.month%t == 0 else np.ceil(x.month/t))-(t-1)), 1))\n",
    "    df['time_period'] = pd.to_datetime(df['time_period'])\n",
    "    \n",
    "    df_period_index = df[['year','period']].drop_duplicates()\\\n",
    "        .sort_values(['year','period'], ascending = True)\\\n",
    "        .reset_index(drop = True)\n",
    "    df_period_index['index'] = df_period_index.index\n",
    "    df = pd.merge(df, df_period_index).drop(['year','period'], axis = 1)\\\n",
    "        .rename({'index': 'time_period_index'}, axis = 1)\n",
    "    \n",
    "    return df \n",
    "\n",
    "def AssignPRAuthorship(df_pr_commit_stats, author_thresh, commit_cols):\n",
    "    commit_cols_share = [f\"{col} share\" for col in commit_cols]\n",
    "    commit_author_bool = df_pr_commit_stats.apply(lambda x: any([x[col]>author_thresh for col in commit_cols_share]), axis = 1)\n",
    "    df_pr_commit_author_stats = df_pr_commit_stats[commit_author_bool]\n",
    "    return df_pr_commit_author_stats\n",
    "\n",
    "def CalculateIssueCommentStats(issue_comments):        \n",
    "    ts_issue_comments = issue_comments.assign(issue_comments=1)\n",
    "    ts_issue_comments['linked_pr_issue_comments'] = 1 - ts_issue_comments['linked_pr_number'].isna().astype(int)\n",
    "    ts_issue_comments['linked_pr_issue_number'] = ts_issue_comments.apply(lambda x: x['issue_number'] if not pd.isnull(x['linked_pr_number']) else np.nan, axis = 1)\n",
    "    ts_issue_comments = ts_issue_comments.groupby(['time_period','time_period_index', 'repo_name','actor_id'])\\\n",
    "        .agg({'issue_comments': 'sum','linked_pr_issue_comments': 'sum',\n",
    "              'issue_number': 'nunique', 'linked_pr_issue_number':'nunique'}).reset_index()\n",
    "\n",
    "    return ts_issue_comments\n",
    "\n",
    "\n",
    "def CalculateColumnPercentile(df, repo, rolling_window, major_col_list, major_pct_list):   \n",
    "    df_repo = df[df['repo_name'] == repo]\n",
    "    df_quantile = df_repo.set_index('time_period')\\\n",
    "        [major_col_list].resample(\"1d\")\\\n",
    "        .quantile(major_pct_list).reset_index(level = 1)\n",
    "    \n",
    "    df_quantile_wide = pd.DataFrame(index = df_quantile.index.unique())\n",
    "    for pct in major_pct_list:\n",
    "        df_quantile_subset = df_quantile[df_quantile['level_1'] == pct].drop('level_1', axis = 1)\n",
    "        df_quantile_subset.columns = [f'{major_col}_{int(pct*100)}th_pct' for major_col in major_col_list]\n",
    "        df_quantile_wide = pd.concat([df_quantile_wide, df_quantile_subset], axis = 1)\n",
    "    \n",
    "    df_all_pct = df_quantile_wide.rolling(window = rolling_window, min_periods = 1)\\\n",
    "        .mean()\\\n",
    "        .reset_index()\n",
    "    \n",
    "    df_repo = pd.merge(df_repo, df_all_pct, on = ['time_period'])\n",
    "\n",
    "    return df_repo\n",
    "\n",
    "\n",
    "def CalculateColumnPercentileDF(df, rolling_window, major_col_list, major_pct_list, general_pct_list): \n",
    "    repo_list = df['repo_name'].unique().tolist()\n",
    "    \n",
    "    with ThreadPoolExecutor(8) as pool:\n",
    "        df = pd.concat(pool.map(CalculateColumnPercentile,df), repo_list,rolling_window),major_col_list),major_pct_list)))\n",
    "\n",
    "    df_quantile = df.set_index('time_period')\\\n",
    "        [major_col_list].resample(\"1d\")\\\n",
    "        .quantile(general_pct_list).reset_index(level = 1)\n",
    "    \n",
    "    df_quantile_wide = pd.DataFrame(index = df_quantile.index.unique())\n",
    "    for pct in general_pct_list:\n",
    "        df_quantile_subset = df_quantile[df_quantile['level_1'] == pct].drop('level_1', axis = 1)\n",
    "        df_quantile_subset.columns = [f'general_{major_col}_{int(pct*100)}th_pct' for major_col in major_col_list]\n",
    "        df_quantile_wide = pd.concat([df_quantile_wide, df_quantile_subset], axis = 1)\n",
    "    \n",
    "    df_all_pct = df_quantile_wide.rolling(window = rolling_window, min_periods = 1)\\\n",
    "        .mean()\\\n",
    "        .reset_index()\n",
    "\n",
    "    df = pd.merge(df, df_all_pct, on = ['time_period'])\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def GetMajorContributorPostPercentile(ts_data, rolling_window, major_col_list, major_pct_list, general_pct_list):\n",
    "    ts_data = ts_data.reset_index()\\\n",
    "        .sort_values(['repo_name','time_period_index','actor_id'])\n",
    "\n",
    "    major_pct_quantile_list = [f'{major_col}_{int(major_pct*100)}th_pct' for major_col, major_pct in itertools.product(major_col_list, major_pct_list)]\n",
    "    general_pct_quantile_list = [f'general_{major_col}_{int(general_pct*100)}th_pct'  for major_col, general_pct in itertools.product(major_col_list, general_pct_list)]\n",
    "    \n",
    "    major_cols = ['time_period','time_period_index', 'repo_name','actor_id']\n",
    "    for lst in [major_col_list, major_pct_quantile_list, general_pct_quantile_list]:\n",
    "        major_cols.extend(lst)\n",
    "    \n",
    "    ts_data_pct = CalculateColumnPercentileDF(ts_data, rolling_window, major_col_list, major_pct_list, general_pct_list)\n",
    "    major_contributor_data = ts_data_pct[major_cols]\n",
    "\n",
    "    return major_contributor_data\n",
    "\n",
    "def GroupedFill(df, group, fill_cols):\n",
    "    df[fill_cols] = df.groupby(group)[fill_cols].ffill()\n",
    "    df[fill_cols] = df.groupby(group)[fill_cols].bfill()\n",
    "\n",
    "    return df\n",
    "\n",
    "def GenerateBalancedContributorsPanel(ic_major_contributor_data, pr_major_contributor_data):\n",
    "    major_contributors = pd.concat([ic_major_contributor_data[['repo_name','actor_id']].drop_duplicates(),\n",
    "                                    pr_major_contributor_data[['repo_name','actor_id']].drop_duplicates()]).drop_duplicates()\n",
    "    time_periods = sorted(ic_major_contributor_data['time_period'].unique().tolist())\n",
    "    major_contributors['time_period'] = [time_periods for i in range(major_contributors.shape[0])]\n",
    "    major_contributors_data = major_contributors.explode('time_period').reset_index(drop = True)\n",
    "    major_contributors_data = pd.merge(major_contributors_data, ic_major_contributor_data, how = 'left')\n",
    "    major_contributors_data = pd.merge(major_contributors_data, pr_major_contributor_data, how = 'left')\n",
    "\n",
    "    return major_contributors_data\n",
    "\n",
    "def RemovePeriodsPriorToJoining(major_contributors_data):\n",
    "    contributor_earliest = major_contributors_data.dropna().sort_values('time_period')\\\n",
    "        [['repo_name','actor_id','time_period']]\\\n",
    "        .drop_duplicates(['repo_name','actor_id'])\\\n",
    "        .rename({'time_period':'earliest_appearance'}, axis = 1)\n",
    "    major_contributors_data = pd.merge(major_contributors_data, contributor_earliest, how = 'inner', on = ['repo_name','actor_id'])\n",
    "    major_contributors_data = major_contributors_data.query('time_period>=earliest_appearance')\n",
    "\n",
    "    return major_contributors_data\n",
    "\n",
    "\n",
    "def OutputMajorContributors(committers_match, df_pr_commit_stats, df_issue_selected, issue_comments, \n",
    "                            major_pct_list, general_pct_list, time_period, author_thresh, commit_cols, \n",
    "                            rolling_window):\n",
    "    major_pr_col_list = ['pr'] + commit_cols + [f\"{col} share\" for col in commit_cols]\n",
    "    df_pr_commit_stats = ImputeTimePeriod(df_pr_commit_stats, time_period)\n",
    "    df_pr_commit_author_stats = AssignPRAuthorship(df_pr_commit_stats, author_thresh, commit_cols)\n",
    "    ts_pr_authorship = df_pr_commit_author_stats.assign(pr = 1)\\\n",
    "        .groupby(['time_period', 'time_period_index', 'repo_name','actor_id'])\\\n",
    "        [major_pr_col_list].sum()\n",
    "    \n",
    "    pr_major_contributor_data = GetMajorContributorPostPercentile(ts_pr_authorship, rolling_window, major_pr_col_list, major_pct_list, general_pct_list)\n",
    "    print(\"percentile for PRs obtained\")\n",
    "    issue_comments = ImputeTimePeriod(issue_comments, time_period)\n",
    "    ts_issue_comments = CalculateIssueCommentStats(issue_comments)\n",
    "    \n",
    "    major_ic_col_list  = ['issue_comments','linked_pr_issue_comments', 'issue_number', 'linked_pr_issue_number']\n",
    "    ic_major_contributor_data = GetMajorContributorPostPercentile(ts_issue_comments, rolling_window, major_ic_col_list, major_pct_list, general_pct_list)\n",
    "    print(\"percentile for issues obtained\")\n",
    "    major_contributors_data = GenerateBalancedContributorsPanel(ic_major_contributor_data, pr_major_contributor_data)\n",
    "\n",
    "    major_contributors_data = RemovePeriodsPriorToJoining(major_contributors_data)\n",
    "\n",
    "    pct_cols = [col for col in major_contributors_data.columns if 'pct' in col]\n",
    "    major_cols = major_pr_col_list\n",
    "    major_cols.extend(major_ic_col_list)\n",
    "    \n",
    "    major_contributors_data = GroupedFill(major_contributors_data, ['repo_name','time_period'], pct_cols)\n",
    "    major_contributors_data = GroupedFill(major_contributors_data, ['repo_name','time_period'], ['time_period_index'])\n",
    "    major_contributors_data[major_cols] = major_contributors_data[major_cols].fillna(0)\n",
    "\n",
    "    print(f\"Major PCT: {major_pct_list}, General PCT: {general_pct_list}, Time Period: {time_period} months\")\n",
    "    print(major_contributors_data[['repo_name','actor_id']].drop_duplicates().shape)\n",
    "    major_contributors_data = major_contributors_data.drop_duplicates()\n",
    "    major_contributors_data.to_parquet(f'issue/major_contributors_major_months{time_period}_window{rolling_window}_sample100.to_parquet')\n",
    "    print(\"major contributors exported\")\n",
    "\n",
    "    return major_contributors_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "403904f0-ebbf-431e-ae71-33e48ae5a400",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "drive/output/scrape/link_issue_pull_request/linked_issue/modin-project_modin_linked_issue_to_pull_request.csv\n",
      "drive/output/scrape/link_issue_pull_request/linked_issue/PyMySQL_PyMySQL_linked_issue_to_pull_request.csv\n",
      "drive/output/scrape/link_issue_pull_request/linked_issue/google_grr_linked_issue_to_pull_request.csv\n",
      "drive/output/scrape/link_issue_pull_request/linked_issue/celiao_tmdbsimple_linked_issue_to_pull_request.csv\n",
      "drive/output/scrape/link_issue_pull_request/linked_issue/aaugustin_websockets_linked_issue_to_pull_request.csv\n"
     ]
    }
   ],
   "source": [
    "indir_committers_info = Path('drive/output/scrape/link_committers_profile')\n",
    "indir_data = Path('drive/output/derived/data_export')\n",
    "\n",
    "commit_cols = ['commits','commit additions','commit deletions','commit changes total','commit files changed count']\n",
    "author_thresh = 1/3\n",
    "rolling_window = '1828D'\n",
    "\n",
    "df_issue = pd.read_parquet(indir_data / 'df_issue.parquet')\n",
    "df_pr = pd.read_parquet(indir_data / 'df_pr.parquet')\n",
    "df_pr_commits = pd.read_parquet(indir_data / 'df_pr_commits.parquet')\n",
    "df_linked_issues = ReadFileList(glob('drive/output/scrape/link_issue_pull_request/linked_issue/*.csv'))\n",
    "\n",
    "df_issue['created_at'] = pd.to_datetime(df_issue['created_at'])\n",
    "df_pr['created_at'] = pd.to_datetime(df_pr['created_at'])\n",
    "\n",
    "selected_repos = df_issue[['repo_name']].drop_duplicates().sample(100, random_state = 1235)['repo_name'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "a913e4c2-4715-4317-ac27-279f0a1168e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_issue_selected = df_issue[(df_issue['repo_name'].isin(selected_repos)) & (df_issue['created_at']>='2015-01-01')]\n",
    "df_pr_selected = df_pr[(df_pr['repo_name'].isin(selected_repos))  & (df_pr['created_at']>='2015-01-01')]\n",
    "df_pr_commits_selected = df_pr_commits[(df_pr_commits['repo_name'].isin(selected_repos))]\n",
    "df_linked_issues = df_linked_issues[(df_linked_issues['repo_name'].isin(selected_repos))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "e5f3547c-1c7d-499e-8326-6999d5816fd0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "committers_match = CleanCommittersInfo(indir_committers_info)\n",
    "df_pr_commit_stats = LinkPRCommits(df_pr_selected, df_pr_commits_selected, committers_match, commit_cols)\n",
    "df_issue_selected = LinkIssuePR(df_issue_selected, df_linked_issues)\n",
    "issue_comments = FilterDuplicateIssues(df_issue_selected, 'type == \"IssueCommentEvent\"')\n",
    "\n",
    "major_pct_list = [0.75, 0.9, 0.95]\n",
    "general_pct_list = [0.25, 0.5, 0.75]\n",
    "time_period_months = [2, 3, 6, 12]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "35a081b4-e7ec-4697-82ee-f7346b3b31ea",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "percentile for PRs obtained\n",
      "percentile for PRs obtained\n",
      "percentile for PRs obtained\n",
      "percentile for PRs obtained\n",
      "percentile for issues obtained\n",
      "percentile for issues obtained\n",
      "percentile for issues obtained\n",
      "percentile for issues obtained\n",
      "Major PCT: [0.75, 0.9, 0.95], General PCT: [0.25, 0.5, 0.75], Time Period: 12 months\n",
      "(227, 2)\n",
      "Major PCT: [0.75, 0.9, 0.95], General PCT: [0.25, 0.5, 0.75], Time Period: 6 months\n",
      "(229, 2)\n",
      "major contributors exported\n",
      "major contributors exported\n",
      "Major PCT: [0.75, 0.9, 0.95], General PCT: [0.25, 0.5, 0.75], Time Period: 3 months\n",
      "(204, 2)\n",
      "Major PCT: [0.75, 0.9, 0.95], General PCT: [0.25, 0.5, 0.75], Time Period: 2 months\n",
      "(204, 2)\n",
      "major contributors exported\n",
      "major contributors exported\n"
     ]
    }
   ],
   "source": [
    "time_period_months.reverse()\n",
    "with ThreadPoolExecutor(4) as pool:\n",
    "    pool.map(OutputMajorContributors, itertools.repeat(committers_match), itertools.repeat(df_pr_commit_stats),\n",
    "             itertools.repeat(df_issue_selected), itertools.repeat(issue_comments), itertools.repeat(major_pct_list), itertools.repeat(general_pct_list), time_period_months, itertools.repeat(author_thresh), itertools.repeat(commit_cols), itertools.repeat(rolling_window))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "oss_hierarchy",
   "language": "python",
   "name": "oss_hierarchy"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
