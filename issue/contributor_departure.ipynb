{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b4e815d6-c66d-4155-af31-572e47e5992e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir('../')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b6d3e464-9f3f-4bf5-9f49-1413e70b2c7c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: Pandarallel will run on 8 workers.\n",
      "INFO: Pandarallel will use Memory file system to transfer data between the main process and workers.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import sys\n",
    "import glob\n",
    "import warnings\n",
    "import random\n",
    "from pandarallel import pandarallel\n",
    "from source.lib.JMSLab import autofill\n",
    "from source.lib.helpers import *\n",
    "from ast import literal_eval\n",
    "from concurrent.futures import ThreadPoolExecutor, ProcessPoolExecutor\n",
    "from glob import glob \n",
    "import datetime\n",
    "import itertools\n",
    "import time\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "pd.set_option('display.max_columns', None)\n",
    "pandarallel.initialize(progress_bar = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b6926de3-7199-4182-83a5-79ca25b75d1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "indir_committers_info = Path('drive/output/scrape/link_committers_profile')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8928606a-3246-4b18-be6f-d79b69883872",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_issue = pd.read_parquet('issue/df_issue.parquet')\n",
    "df_pr = pd.read_parquet('issue/df_pr.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4264111b-6d4c-4ad7-990d-98ac1742a7b9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "drive/output/scrape/link_issue_pull_request/linked_issue/modin-project_modin_linked_issue_to_pull_request.csv\n",
      "drive/output/scrape/link_issue_pull_request/linked_issue/PyMySQL_PyMySQL_linked_issue_to_pull_request.csv\n",
      "drive/output/scrape/link_issue_pull_request/linked_issue/google_grr_linked_issue_to_pull_request.csv\n",
      "drive/output/scrape/link_issue_pull_request/linked_issue/celiao_tmdbsimple_linked_issue_to_pull_request.csv\n",
      "drive/output/scrape/link_issue_pull_request/linked_issue/aaugustin_websockets_linked_issue_to_pull_request.csv\n"
     ]
    }
   ],
   "source": [
    "df_pr_commits = pd.read_parquet('issue/df_pr_commits.parquet')\n",
    "df_linked_issues = ReadFileList(glob('drive/output/scrape/link_issue_pull_request/linked_issue/*.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d9176e4c-4f59-4f7f-b19f-100445055ea4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_issue['created_at'] = pd.to_datetime(df_issue['created_at'])\n",
    "df_pr['created_at'] = pd.to_datetime(df_pr['created_at'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "17fdbec8-095b-45b1-ba28-7e10ea751759",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_num = 100\n",
    "commit_cols = ['commits','commit additions','commit deletions','commit changes total','commit files changed count']\n",
    "author_thresh = 1/3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "795361e4-3eb3-4bb3-9742-96edb8748052",
   "metadata": {},
   "outputs": [],
   "source": [
    "def CleanCommittersInfo(indir_committers_info):\n",
    "    # TODO: edit file so it can handle pushes\n",
    "    df_committers_info = pd.read_csv(indir_committers_info / 'committers_info_pr.csv', index_col = 0).dropna()\n",
    "    df_committers_info['committer_info'] = df_committers_info['committer_info'].apply(literal_eval)\n",
    "    # TODO: handle cleaning so that it can handle the other cases\n",
    "    df_committers_info = df_committers_info[df_committers_info['committer_info'].apply(lambda x: len(x)==4)]\n",
    "    df_committers_info['actor_name'] = df_committers_info['committer_info'].apply(lambda x: x[0])\n",
    "    df_committers_info['actor_id'] = df_committers_info['committer_info'].apply(lambda x: x[1])\n",
    "\n",
    "    committers_match = df_committers_info[['name','email','user_type','actor_name','actor_id']].drop_duplicates()\n",
    "    committers_match.rename({'actor_id':'commit_author_id'}, axis = 1, inplace = True)\n",
    "\n",
    "    return committers_match"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4d626d84-9fa2-49a9-9b08-d132e76d4cd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def LinkPRCommits(df_pr_selected, df_pr_commits_selected, committers_match, commit_cols):\n",
    "\n",
    "    # TODO: what % of commits were dropped because nobody could be found\n",
    "    matched_commits = pd.merge(df_pr_commits_selected, committers_match,\n",
    "                               how = 'inner', left_on = ['commit author name','commit author email'],\n",
    "                               right_on = ['name','email'])\n",
    "    matched_commits = matched_commits.assign(commits=1)\n",
    "    \n",
    "    matched_commits_total = matched_commits.groupby(['repo_name','pr_number'])\\\n",
    "        [commit_cols].sum()\n",
    "    matched_commits_total.columns = [col + ' total' for col in commit_cols]\n",
    "    matched_commits_share = pd.merge(\n",
    "        matched_commits,\n",
    "        matched_commits_total.reset_index(), on = ['repo_name','pr_number'])\n",
    "    \n",
    "    for col in commit_cols:\n",
    "        matched_commits_share[f\"{col} share\"] = matched_commits_share[col]/matched_commits_share[f\"{col} total\"]\n",
    "\n",
    "    final_agg_cols = commit_cols + [f\"{col} share\" for col in commit_cols]\n",
    "    commit_stats = matched_commits_share\\\n",
    "        .assign(commits=1)\\\n",
    "        .groupby(['repo_name','pr_number','commit_author_id'])\\\n",
    "        [final_agg_cols].sum().reset_index()\n",
    "    \n",
    "    merged_commits = df_pr_selected.query('pr_action == \"closed\" & ~pr_merged_by_id.isna()')\n",
    "    # TODO: what % of commits had truncated information bc 250 max - also, is that push or PR? \n",
    "    # TODO: what % of commits could we not get information for\n",
    "    df_commit_stats = pd.merge(merged_commits, commit_stats, on = ['repo_name','pr_number'])\n",
    "\n",
    "    return df_commit_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "84a1ff36-ad76-45a1-944a-f429abcf53ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def LinkIssuePR(df_issue_selected, df_linked_issues):\n",
    "    df_linked_issues = df_linked_issues.query('linked_pull_request != \"list index out of range\"')\n",
    "    df_linked_issues['linked_pr_number'] = df_linked_issues['linked_pull_request'].apply(lambda x: x.split(\"/\")[-1])\n",
    "    df_issue_pr = df_linked_issues[['repo_name','issue_number', 'linked_pr_number']].drop_duplicates()\n",
    "\n",
    "    df_issue_selected = pd.merge(df_issue_selected, df_issue_pr, how = 'left', on = ['repo_name','issue_number'])\n",
    "\n",
    "    return df_issue_selected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "aed90c74-26c2-45aa-9a09-43d163a14797",
   "metadata": {},
   "outputs": [],
   "source": [
    "def FilterDuplicateIssues(df, query):\n",
    "    df_sel = df.query(query)\\\n",
    "        .sort_values(['repo_name','issue_number','created_at'])\\\n",
    "        [['repo_name','actor_id', 'issue_user_id','issue_number', \n",
    "          'issue_comment_id', 'created_at', 'linked_pr_number']]\\\n",
    "        .dropna(subset = ['issue_number'])\\\n",
    "        .dropna(axis=1, how='all')\\\n",
    "        .drop_duplicates()\n",
    "\n",
    "    return df_sel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "650b2226-9867-401d-a0fc-6749ab9f3019",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ImputeTimePeriod(df, time_period_months):\n",
    "    df['created_at'] = pd.to_datetime(df['created_at'])\n",
    "    df['year'] = df['created_at'].apply(lambda x: x.year)\n",
    "    \n",
    "    df['period'] = df['created_at'].apply(lambda x: int(x.month>6))\n",
    "    df['time_period'] = df['created_at'].apply(lambda x: datetime.date(x.year, np.floor(x.month/time_period_months)+1, 1))\n",
    "    df['time_period'] = pd.to_datetime(df['time_period'])\n",
    "    \n",
    "    df_period_index = df[['year','period']].drop_duplicates()\\\n",
    "        .sort_values(['year','period'], ascending = True)\\\n",
    "        .reset_index(drop = True)\n",
    "    df_period_index['index'] = df_period_index.index\n",
    "    df = pd.merge(df, df_period_index).drop(['year','period'], axis = 1)\\\n",
    "        .rename({'index': 'time_period_index'}, axis = 1)\n",
    "    \n",
    "    return df "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3053eb2a-fa50-49db-818d-f25a802920c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def AssignPRAuthorship(df_pr_commit_stats, author_thresh, commit_cols):\n",
    "    commit_cols_share = [f\"{col} share\" for col in commit_cols]\n",
    "    commit_author_bool = df_pr_commit_stats.apply(lambda x: any([x[col]>author_thresh for col in commit_cols_share]), axis = 1)\n",
    "    df_pr_commit_author_stats = df_pr_commit_stats[commit_author_bool]\n",
    "    return df_pr_commit_author_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0956b327-8045-4309-85e8-d517307f0fce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def CalculateColumnPercentile(df, repo, window, col, pct): \n",
    "    df_repo = df.query(f'repo_name == \"{repo}\"')\n",
    "    df_pct = df_repo.set_index('time_period')\\\n",
    "        [col].resample(\"1d\")\\\n",
    "        .quantile(pct)\\\n",
    "        .rolling(window = window, min_periods = 1)\\\n",
    "        .mean()\\\n",
    "        .rename(f'{col}_{int(pct*100)}th_pct')\\\n",
    "        .reset_index()\n",
    "    df_repo = pd.merge(df_repo, df_pct, on = ['time_period'])\n",
    "\n",
    "    return df_repo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2480023a-37bb-4ba6-9b88-5eea44b3cacd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def CalculateColumnPercentileDF(df, window, col, pct, general_pct): \n",
    "    repo_list = df['repo_name'].unique().tolist()\n",
    "    \n",
    "    with ThreadPoolExecutor(8) as pool:\n",
    "        df = pd.concat(pool.map(CalculateColumnPercentile, itertools.repeat(df), repo_list, itertools.repeat(window), itertools.repeat(col), itertools.repeat(pct)))\n",
    "\n",
    "    df_all_pct = df.set_index('time_period')\\\n",
    "        [col].resample(\"1d\")\\\n",
    "        .quantile(general_pct)\\\n",
    "        .rolling(window = window, min_periods = 1)\\\n",
    "        .mean()\\\n",
    "        .rename(f'general_{col}_{int(general_pct*100)}th_pct')\\\n",
    "        .reset_index()\n",
    "    df = pd.merge(df, df_all_pct, on = ['time_period'])\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5feda96e-2f7d-435b-8ffa-cafadb224eee",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "selected_repos = df_issue[['repo_name']].drop_duplicates()['repo_name'].tolist()#.sample(sample_num, random_state = 123)['repo_name'].tolist()\n",
    "\n",
    "df_issue_selected = df_issue[(df_issue['repo_name'].isin(selected_repos)) & (df_issue['created_at']>='2015-01-01')]\n",
    "df_pr_selected = df_pr[(df_pr['repo_name'].isin(selected_repos))  & (df_pr['created_at']>='2015-01-01')]\n",
    "df_pr_commits_selected = df_pr_commits[(df_pr_commits['repo_name'].isin(selected_repos))]\n",
    "df_linked_issues = df_linked_issues[(df_linked_issues['repo_name'].isin(selected_repos))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "6efdcc85-7183-42a9-a017-763d8815d815",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "options_list = []\n",
    "for major_pct_options in [0.75, 0.9, 0.95]:\n",
    "    for general_pct_options in [0.25, 0.5, 0.75]:\n",
    "        for time_period_months_options in [2, 3, 6, 12]:\n",
    "            options_list.append([major_pct_options, general_pct_options, time_period_months_options])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d4ef0035-288b-48b5-b1d0-f414b094759a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "committers_match = CleanCommittersInfo(indir_committers_info)\n",
    "df_pr_commit_stats = LinkPRCommits(df_pr_selected, df_pr_commits_selected, committers_match, commit_cols)\n",
    "df_issue_selected = LinkIssuePR(df_issue_selected, df_linked_issues)\n",
    "issue_comments = FilterDuplicateIssues(df_issue_selected, 'type == \"IssueCommentEvent\"')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "9db84354-903b-41bc-9584-7361491f7191",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def GetMajorContributorPostpercentile(ts_data, rolling_window, major_col, major_pct, general_pct):\n",
    "    ts_data = ts_data.reset_index()\\\n",
    "        .sort_values(['repo_name','time_period_index','actor_id'])\n",
    "\n",
    "    \n",
    "    repo_pct_col = f'{major_col}_{int(major_pct*100)}th_pct'\n",
    "    general_pct_col = f'general_{major_col}_{int(general_pct*100)}th_pct'\n",
    "    major_cols = ['time_period','time_period_index', 'repo_name','actor_id', major_col,\n",
    "                  repo_pct_col, general_pct_col]\n",
    "    \n",
    "    ts_data_pct = CalculateColumnPercentileDF(ts_data, '1828D', major_col, major_pct, general_pct)\n",
    "    major_contributor_data = ts_data_pct[major_cols].query(f'{major_col}>{repo_pct_col} & {major_col}>{general_pct_col}')\n",
    "\n",
    "    return major_contributor_data\n",
    "\n",
    "def GroupedFill(df, group, fill_cols):\n",
    "    df[fill_cols] = major_contributors_data.groupby(group)[fill_cols].ffill()\n",
    "    df[fill_cols] = major_contributors_data.groupby(group)[fill_cols].bfill()\n",
    "\n",
    "    return df\n",
    "\n",
    "def GenerateBalancedContributorsPanel(ic_major_contributor_data, pr_major_contributor_data):\n",
    "    major_contributors = pd.concat([ic_major_contributor_data[['repo_name','actor_id']].drop_duplicates(),\n",
    "                                    pr_major_contributor_data[['repo_name','actor_id']].drop_duplicates()]).drop_duplicates()\n",
    "    time_periods = sorted(ic_major_contributor_data['time_period'].unique().tolist())\n",
    "    major_contributors['time_period'] = [time_periods for i in range(major_contributors.shape[0])]\n",
    "    major_contributors_data = major_contributors.explode('time_period').reset_index(drop = True)\n",
    "    major_contributors_data = pd.merge(major_contributors_data, ic_major_contributor_data, how = 'left')\n",
    "    major_contributors_data = pd.merge(major_contributors_data, pr_major_contributor_data, how = 'left')\n",
    "\n",
    "    return major_contributors_data\n",
    "\n",
    "def RemovePeriodsPriorToJoining(major_contributors_data):\n",
    "    contributor_earliest = major_contributors_data.dropna().sort_values('time_period')\\\n",
    "        [['repo_name','actor_id','time_period']]\\\n",
    "        .drop_duplicates(['repo_name','actor_id'])\\\n",
    "        .rename({'time_period':'earliest_appearance'}, axis = 1)\n",
    "    major_contributors_data = pd.merge(major_contributors_data, contributor_earliest, how = 'inner', on = ['repo_name','actor_id'])\n",
    "    major_contributors_data = major_contributors_data.query('time_period>=earliest_appearance')\n",
    "\n",
    "    return major_contributors_data\n",
    "\n",
    "\n",
    "def OutputMajorContributors(committers_match, df_pr_commit_stats, df_issue_selected, issue_comments, options):\n",
    "    major_pct = options[0]\n",
    "    general_pct = options[1]\n",
    "    time_period = options[2]\n",
    "\n",
    "    df_pr_commit_stats = ImputeTimePeriod(df_pr_commit_stats, time_period)\n",
    "    df_pr_commit_author_stats = AssignPRAuthorship(df_pr_commit_stats, author_thresh, commit_cols)\n",
    "    ts_pr_authorship = df_pr_commit_author_stats.assign(pr = 1)\\\n",
    "        .groupby(['time_period', 'time_period_index', 'repo_name','actor_id'])\\\n",
    "        [['pr'] + commit_cols + [f\"{col} share\" for col in commit_cols]].sum()\n",
    "    \n",
    "    major_pr_col = 'pr'\n",
    "    rolling_window = '1828D'\n",
    "    pr_major_contributor_data = GetMajorContributorPostpercentile(ts_pr_authorship, rolling_window, major_pr_col, major_pct, general_pct)\n",
    "\n",
    "    issue_comments = ImputeTimePeriod(issue_comments, time_period)\n",
    "    ts_issue_comments = issue_comments.assign(issue_comments=1)\\\n",
    "        .groupby(['time_period','time_period_index', 'repo_name','actor_id'])\\\n",
    "        ['issue_comments'].sum()\n",
    "\n",
    "    major_ic_col = 'issue_comments'\n",
    "    ic_major_contributor_data = GetMajorContributorPostpercentile(ts_issue_comments, rolling_window, major_ic_col, major_pct, general_pct)\n",
    "    \n",
    "    major_contributors_data = GenerateBalancedContributorsPanel(ic_major_contributor_data, pr_major_contributor_data)\n",
    "\n",
    "    major_contributors_data = RemovePeriodsPriorToJoining(major_contributors_data)\n",
    "\n",
    "    pct_cols = [repo_pct_pr_col, general_pct_pr_col, repo_pct_ic_col, general_pct_ic_col]\n",
    "    major_cols = [major_pr_col, major_ic_col]\n",
    "\n",
    "    major_contributors_data = GroupedFill(major_contributors_data, ['repo_name','time_period'], pct_cols)\n",
    "    major_contributors_data = GroupedFill(major_contributors_data, ['repo_name','time_period'], ['time_period_index'])\n",
    "    major_contributors_data[major_cols] = major_contributors_data[major_cols].fillna(0)\n",
    "\n",
    "    print(f\"Major PCT: {major_pct}, General PCT: {general_pct}, Time Period: {time_period} months\")\n",
    "    print(major_contributors_data[['repo_name','actor_id']].drop_duplicates().shape)\n",
    "    major_contributors_data.to_csv(f'issue/major_contributors_major{major_pct}_general{general_pct}_months{time_period}.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64e145dd-7288-484e-bbfa-e0dbe99aad1e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#pool.map(OutputMajorContributors, itertools.repeat(committers_match), itertools.repeat(df_pr_commit_stats), itertools.repeat(df_issue_selected), \n",
    "#         itertools.repeat(issue_comments), options_list)\n",
    "OutputMajorContributors(committers_match, df_pr_commit_stats, df_issue_selected, issue_comments, options_list[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ec45686-9b5a-4d26-acaf-116200db8428",
   "metadata": {},
   "outputs": [],
   "source": [
    "with multiprocessing.Pool(4) as pool:\n",
    "    for result in pool.imap(OutputMajorContributors, github_repos):\n",
    "        print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b07df3d3-e130-414a-832b-00ed3f911919",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4138, 2)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO: What % was dropped at each stage\n",
    "major_contributors_data[['repo_name','actor_id']].drop_duplicates().shape\n",
    "#major_contributors_data.to_csv(f'major_contributors_{sample_num}.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e691c45e-82b7-415d-be75-c7ff428dcc99",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.003611777045728788"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO: autofill about ignoring reopenings\n",
    "np.mean(df_issue['issue_action']=='reopened')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "oss_hierarchy",
   "language": "python",
   "name": "oss_hierarchy"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
