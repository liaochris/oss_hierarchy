{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b036b314-02d3-4c1c-ac78-289d112fd164",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir('../')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "108a703a-b75b-4afc-ad92-6a0191f457d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from source.lib.helpers import *\n",
    "import ast\n",
    "import glob\n",
    "import pyarrow as pa\n",
    "import pyarrow.parquet as pq\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "28f6b260-14d8-460f-82b0-ee8faaf47a3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "download_post_date = '2018-09-01'\n",
    "time_period = 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "71cd7e9c-d0e3-42fd-aea9-be6516fc1396",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pypi_github_mapping = pd.read_csv('output/derived/collect_github_repos/linked_pypi_github.csv')\n",
    "monthly_downloads = pd.read_parquet('drive/output/scrape/pypi_monthly_downloads/pypi_monthly_downloads.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "439227bb-dd50-4e07-9e32-d0aa2251cc29",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_github_downloads = pd.merge(df_pypi_github_mapping.rename(columns={'package':'project'}), monthly_downloads)\n",
    "df_github_downloads = df_github_downloads[df_github_downloads['github repository'] != 'Unavailable']\n",
    "df_github_downloads = df_github_downloads.rename(columns = {'github repository':'repo_name', 'month': 'created_at'})\n",
    "df_github_downloads = ImputeTimePeriod(df_github_downloads, time_period)\n",
    "df_github_downloads = df_github_downloads.groupby(['repo_name', 'time_period','project'])['num_downloads'].sum().reset_index()\n",
    "\n",
    "df_github = df_github_downloads.groupby(['repo_name', 'time_period']).agg(\n",
    "    total_downloads=('num_downloads', 'sum'),\n",
    "    total_downloads_one_project=('num_downloads', 'max'),\n",
    "    unique_projects=('project', lambda x: set(x)),\n",
    "    project_count=('project', 'nunique')\n",
    ").reset_index()\n",
    "df_github.to_parquet('issue/github_downloads.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf49a715",
   "metadata": {},
   "outputs": [],
   "source": [
    "def FlattenScorecard(row_str):\n",
    "    if pd.isnull(row_str):\n",
    "        return\n",
    "    row = ast.literal_eval(row_str)\n",
    "    data = {\"score\": row.get(\"score\"),\n",
    "            \"repo_name\": row.get(\"repo\").get(\"name\")}\n",
    "    for check in row.get(\"checks\", []):\n",
    "        name = check.get(\"name\", \"\").lower().replace(\"-\", \"_\").replace(\" \", \"_\")\n",
    "        if name:\n",
    "            data[f\"{name}_name\"] = check.get(\"name\")\n",
    "            data[f\"{name}_score\"] = check.get(\"score\")\n",
    "            data[f\"{name}_reason\"] = check.get(\"reason\")\n",
    "            data[f\"{name}_details\"] = check.get(\"details\")\n",
    "    return data\n",
    "\n",
    "dfs = []\n",
    "for file in glob.glob(\"drive/output/scrape/get_weekly_scorecard_data/scorecard/*.csv\"):\n",
    "    df = pd.read_csv(file).T\n",
    "    df.columns = df.iloc[0]\n",
    "    df = df[1:].reset_index(drop=True)\n",
    "    if 'scorecard_data' in df.columns:\n",
    "        flat = df[\"scorecard_data\"].apply(FlattenScorecard).apply(pd.Series)\n",
    "        dfs.append(pd.concat([df, flat], axis=1).assign(source_file=file))\n",
    "        print(file)\n",
    "df_scorecard = pd.concat(dfs, ignore_index=True)\n",
    "df_scorecard.to_parquet('issue/github_scorecard_full.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8eae56a-ec6b-41f8-b876-4d757e0d47c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# export detailed score data\n",
    "# and also export the score data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2c6b651",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ProcessScorecard(df_scorecard, time_period):\n",
    "    \"\"\"\n",
    "    Process the scorecard DataFrame by:\n",
    "      - Grouping by [time_period, repo_name] and averaging score columns\n",
    "      - Computing the percentage change in score from the previous time period\n",
    "      - Flagging a trend ('increase' or 'decrease') if the change is at least 10%,\n",
    "        otherwise 'stable'\n",
    "    \n",
    "    Parameters:\n",
    "      df_scorecard: DataFrame containing raw scorecard data.\n",
    "      time_period: Value (or column name) to be used by ImputeTimePeriod.\n",
    "      \n",
    "    Returns:\n",
    "      A DataFrame aggregated by time period and repo with mean scores and trend flags.\n",
    "    \"\"\"\n",
    "    df_scorecard['created_at'] = pd.to_datetime(df_scorecard['date'])\n",
    "    df_scorecard = ImputeTimePeriod(df_scorecard, time_period)\n",
    "    df_scorecard = df_scorecard.drop(columns=['time', 'commit_sha', 'date', 'week', 'year', 'scorecard_data'])\n",
    "    df_scorecard['overall_score'] = df_scorecard['score']\n",
    "    agg_cols = {'overall_score': 'mean'}\n",
    "    # If you have another score-like column (e.g., '_score') include it:\n",
    "    for col in df_scorecard.columns:\n",
    "        if '_score' in col:\n",
    "            agg_cols[col] = 'mean'\n",
    "        \n",
    "    df_scorecard_scores = df_scorecard.groupby(['time_period', 'repo_name']).agg(agg_cols).reset_index()\n",
    "    df_scorecard_scores = df_scorecard_scores.sort_values(by=['repo_name', 'time_period'])\n",
    "    df_scorecard_scores['overall_score_change'] = df_scorecard_scores.groupby('repo_name')['overall_score'].diff()\n",
    "    \n",
    "    def ClassifyTrend(change):\n",
    "        if pd.isna(change):\n",
    "            return None \n",
    "        if change >= 1:\n",
    "            return \"increase\"\n",
    "        elif change <= -1:\n",
    "            return \"decrease\"\n",
    "        else:\n",
    "            return \"stable\"\n",
    "    \n",
    "    df_scorecard_scores['overall_trend'] = df_scorecard_scores['overall_score_change'].apply(ClassifyTrend)\n",
    "    df_scorecard_scores['overall_increase'] = (df_scorecard_scores['overall_trend'] == 'increase').astype(int)\n",
    "    df_scorecard_scores['overall_decrease'] = (df_scorecard_scores['overall_trend'] == 'decrease').astype(int)\n",
    "    df_scorecard_scores['overall_stable'] = (df_scorecard_scores['overall_trend'] == 'stable').astype(int)\n",
    "    return df_scorecard_scores\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ceafc895",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_scorecard_scores = ProcessScorecard(df_scorecard, time_period)\n",
    "df_scorecard_scores.to_parquet('issue/github_scorecard_scores.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e54b098f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ReadParquetHandlingDbdate(file_path):\n",
    "    table = pq.read_table(file_path)\n",
    "    new_columns = []\n",
    "    for i, field in enumerate(table.schema):\n",
    "        # If the field's type string contains \"dbdate\", cast it to string.\n",
    "        if \"dbdate\" in str(field.type).lower():\n",
    "            new_columns.append(table.column(i).cast(pa.string()))\n",
    "        else:\n",
    "            new_columns.append(table.column(i))\n",
    "    new_table = pa.Table.from_arrays(new_columns, table.schema.names)\n",
    "    return new_table.to_pandas()\n",
    "\n",
    "def ProcessSoftwareDownloads(file_path, time_period):\n",
    "    \"\"\"\n",
    "    Reads in a software downloads file and processes it to:\n",
    "      1) Identify, for each time period, when a new version is released.\n",
    "         A new release is defined as the first appearance of a library_version in a given time period \n",
    "         (i.e. it does not appear in any previous time period).\n",
    "      2) Count the number of new releases overall and by release type \n",
    "         (major, minor, patch, and other).\n",
    "         \n",
    "    Parameters:\n",
    "      file_path: Path to the input file (parquet format).\n",
    "      time_period: Value (or column name) to be used by ImputeTimePeriod.\n",
    "      \n",
    "    Returns:\n",
    "      A summary DataFrame with one row per time period containing new release counts by release type.\n",
    "    \"\"\"\n",
    "    df = ReadParquetHandlingDbdate(file_path)\n",
    "    project = df['project'].tolist()[0]\n",
    "    if 'date' in df.columns:\n",
    "        df.rename(columns={'date': 'created_at'}, inplace=True)\n",
    "    df['created_at'] = pd.to_datetime(df['created_at'])\n",
    "    \n",
    "    def ClassifyReleaseType(release_version):\n",
    "        parts = release_version.split('.')\n",
    "        if not all(p.isdigit() for p in parts):\n",
    "            return \"other\"\n",
    "        if len(parts) == 2:\n",
    "            return \"major\" if parts[1] == \"0\" else \"minor\"\n",
    "        elif len(parts) >= 3:\n",
    "            if parts[1] == \"0\" and parts[2] == \"0\":\n",
    "                return \"major\"\n",
    "            elif parts[2] == \"0\":\n",
    "                return \"minor\"\n",
    "            else:\n",
    "                return \"patch\"\n",
    "        return \"other\"\n",
    "    \n",
    "    df['release_type'] = df['library_version'].apply(ClassifyReleaseType)\n",
    "    \n",
    "    df_first = df.sort_values('created_at').groupby('library_version', as_index=False).first()\n",
    "    df_first = ImputeTimePeriod(df_first, time_period)\n",
    "    time_periods = sorted(df_first['time_period'].unique())\n",
    "    summary_list = []\n",
    "    \n",
    "    for tp in time_periods[1:]:\n",
    "        df_tp = df_first[df_first['time_period'] == tp]\n",
    "        overall_count = len(df_tp)\n",
    "        major_count = (df_tp['release_type'] == 'major').sum()\n",
    "        minor_count = (df_tp['release_type'] == 'minor').sum()\n",
    "        patch_count = (df_tp['release_type'] == 'patch').sum()\n",
    "        other_count = (df_tp['release_type'] == 'other').sum()\n",
    "        major_minor_count = (df_tp['release_type'].isin(['major','minor'])).sum()\n",
    "        major_minor_patch_count = (df_tp['release_type'].isin(['major','minor','patch'])).sum()\n",
    "\n",
    "        df_sorted = df_tp.sort_values('created_at', ascending=False)\n",
    "        \n",
    "        df_valid = df_sorted[df_sorted['release_type'].isin(['major', 'minor', 'patch'])]\n",
    "        latest_overall_downloads = df_valid.iloc[0]['num_downloads'] if not df_valid.empty else 0\n",
    "        latest_dict = df_sorted.drop_duplicates(subset=['release_type']).set_index('release_type')['num_downloads'].to_dict()\n",
    "        latest_major_downloads = latest_dict.get('major', 0)\n",
    "        latest_minor_downloads = latest_dict.get('minor', 0)\n",
    "        \n",
    "        summary_list.append({\n",
    "            'time_period': tp,\n",
    "            'overall_new_release_count': overall_count,\n",
    "            'major_new_release_count': major_count,\n",
    "            'minor_new_release_count': minor_count,\n",
    "            'patch_new_release_count': patch_count,\n",
    "            'other_new_release_count': other_count,\n",
    "            'major_minor_release_count': major_minor_count,\n",
    "            'major_minor_patch_release_count': major_minor_patch_count,\n",
    "            'latest_major_downloads': latest_major_downloads,\n",
    "            'latest_minor_downloads': latest_minor_downloads,\n",
    "            'latest_mmp_downloads': latest_overall_downloads # excludes others\n",
    "        })\n",
    "    \n",
    "    summary_df = pd.DataFrame(summary_list)\n",
    "    summary_df['project'] = project\n",
    "    \n",
    "    return summary_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9bf9c5d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "indir_project_downloads = \"drive/output/scrape/pypi_package_downloads\"\n",
    "file_list = glob.glob(os.path.join(indir_project_downloads, \"*.parquet\"))\n",
    "summary_dfs = []\n",
    "for file_path in file_list:\n",
    "    summary_df = ProcessSoftwareDownloads(file_path, time_period)\n",
    "    summary_dfs.append(summary_df)\n",
    "\n",
    "combined_downloads_summary = pd.concat(summary_dfs, ignore_index=True)\n",
    "combined_downloads_summary = pd.merge(df_pypi_github_mapping.rename(columns={'package':'project'}), combined_downloads_summary)\n",
    "combined_downloads_summary = combined_downloads_summary[combined_downloads_summary['github repository'] != 'Unavailable']\n",
    "combined_downloads_summary.drop(['project','license'], axis=1, inplace=True)\n",
    "combined_downloads_summary = combined_downloads_summary.rename(columns = {'github repository':'repo_name'})\n",
    "combined_downloads_summary.to_parquet('issue/github_downloads_detailed.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b23f490a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "oss_hierarchy",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
