{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c1bc1d28-d1be-4522-92f7-760a436170d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir('../')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "73cdcb91-8512-4af5-a4cc-6735962866e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: Pandarallel will run on 16 workers.\n",
      "INFO: Pandarallel will use Memory file system to transfer data between the main process and workers.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import networkx as nx\n",
    "import numpy as np\n",
    "from source.derived.contributor_stats.calculate_contributions import *\n",
    "from source.lib.helpers import *\n",
    "from datetime import datetime\n",
    "from concurrent.futures import ProcessPoolExecutor, as_completed\n",
    "import concurrent.futures\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "393d39d5-ab8e-4e82-9093-61240b1dccdd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "02strich/pykerberos 2015-07-01 00:00:0023andMe/Yamale\n",
      " 2015-07-01 00:00:00\n",
      "0rpc/zerorpc-python2015-07-01 00:00:00 \n",
      "4Catalyzer/flask-resty 2015-07-01 00:00:00\n",
      "4teamwork/ftw.upgrade 2015-07-01 00:00:004teamwork/ftw.simplelayout\n",
      "\n",
      " 4teamwork/ftw.testbrowser2015-07-01 00:00:00\n",
      " 2015-07-01 00:00:005monkeys/django-enumfield\n",
      " 2015-07-01 00:00:00\n",
      "ARM-DOE/pyart 2015-07-01 00:00:00"
     ]
    }
   ],
   "source": [
    "def Main():\n",
    "    indir_data = Path('drive/output/derived/data_export')\n",
    "    indir_committers_info = Path('drive/output/scrape/link_committers_profile')\n",
    "    commit_cols = ['commits','commit additions','commit deletions','commit changes total','commit files changed count']\n",
    "    \n",
    "    time_period = 6\n",
    "    author_thresh = 1/3\n",
    "    \n",
    "    issue_cols = [\"repo_name\", \"created_at\", \"issue_number\", \"issue_action\", \"actor_id\", \"type\"]\n",
    "    pr_cols = [\"repo_name\", \"created_at\", \"pr_number\", \"actor_id\", \"actor_login\", \"pr_action\", \n",
    "               \"pr_merged_by_id\", \"pr_review_comment_path\", \"pr_review_comment_original_position\", \n",
    "               \"pr_review_comment_original_commit_id\",\"type\"]\n",
    "    pr_commits_cols = [\"repo_name\", \"commit time\", \"commit author name\", \"commit author email\", \n",
    "                       \"pr_number\", \"commit additions\", \"commit deletions\",\n",
    "                       \"commit changes total\", \"commit files changed count\"]\n",
    "    committers_match = CleanCommittersInfo(indir_committers_info)\n",
    "    \n",
    "    df_issue = pd.read_parquet(indir_data / 'df_issue.parquet', columns=issue_cols)\n",
    "    df_pr = pd.read_parquet(indir_data / 'df_pr.parquet', columns=pr_cols)\n",
    "    df_pr_commits = pd.read_parquet(indir_data / 'df_pr_commits.parquet', columns=pr_commits_cols)\n",
    "    \n",
    "    df_issue, df_pr, df_pr_commits = ProcessData(df_issue, df_pr, df_pr_commits)\n",
    "    \n",
    "    repo_list = sorted(set(df_issue.index).union(set(df_pr.index)).union(set(df_pr_commits.index)))\n",
    "    exported_graphs_log = []\n",
    "    df_issue['date'] = df_issue['created_at'].dt.to_period('M').dt.to_timestamp()\n",
    "    time_periods = sorted(ImputeTimePeriod(df_issue.drop_duplicates(['date']), time_period)['time_period'].unique())\n",
    "    \n",
    "    \n",
    "    def worker(args):\n",
    "        time_period_date, repo = args\n",
    "        return CreateGraph(repo, time_period_date, exported_graphs_log=[])\n",
    "    tasks = list(itertools.product(time_periods, repo_list))\n",
    "    all_logs = []\n",
    "    with concurrent.futures.ProcessPoolExecutor() as executor:\n",
    "        for log_entries in executor.map(worker, tasks):\n",
    "            if log_entries is not None:\n",
    "                all_logs.extend(log_entries)\n",
    "    \n",
    "    exported_graphs_log = all_logs\n",
    "    df_log = pd.DataFrame(exported_graphs_log)\n",
    "    df_log.to_csv(\"issue/exported_graphs_log.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f5596ec-9f08-42f3-88ca-e432f7301a90",
   "metadata": {},
   "outputs": [],
   "source": [
    "#1.  DTR I can explore closing is a response to the original issue opener\n",
    "\n",
    "#2.  we want to make pr reviewers interact with the authors\n",
    "# df_pr.query('type == \"PullRequestReviewEvent\"') - is the reviewer\n",
    "# df_pr_commit_author_stats[['repo_name','pr_number','actor_id','pr_opened_at']] - this is the PR author\n",
    "#\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "795a9bde-51ff-4f25-a321-7c390f8ef23a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ProcessData(df_issue, df_pr, df_pr_commits):\n",
    "    df_issue.set_index('repo_name', inplace=True)\n",
    "    df_pr.set_index('repo_name', inplace=True)\n",
    "    df_pr_commits.set_index('repo_name', inplace=True)\n",
    "    \n",
    "    df_issue['created_at'] = pd.to_datetime(df_issue['created_at'])\n",
    "    df_pr['created_at'] = pd.to_datetime(df_pr['created_at'])\n",
    "    df_pr_commits['created_at'] = pd.to_datetime(df_pr_commits['commit time'], unit='s')\n",
    "    \n",
    "    df_issue = df_issue[~df_issue['created_at'].isna()]\n",
    "    df_pr = df_pr[~df_pr['created_at'].isna()]\n",
    "    df_pr_commits = df_pr_commits[~df_pr_commits['created_at'].isna()]\n",
    "    \n",
    "    return df_issue, df_pr, df_pr_commits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4934326e-c5a9-4446-b235-3998ea3e01ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def BuildIssueInteractionGraph(df, method):\n",
    "    \"\"\"\n",
    "    Build an undirected graph from the dataframe using one of two methods.\n",
    "    Additionally, track:\n",
    "      1. The origins in which a node appears (stored as a node attribute \"origins\").\n",
    "      2. The number of interactions (edge weight) attributed to each origin (stored in an edge attribute \"origin_counts\").\n",
    "    Also creates subgraphs for each origin: 'pr', 'issue', and 'pr review'.\n",
    "\n",
    "    Parameters:\n",
    "      - df: A pandas DataFrame sorted by id_number and created_at.\n",
    "      - method: 'keep_consecutive' or 'drop_consecutive'\n",
    "         (Only 'keep_consecutive' is demonstrated here.)\n",
    "    \n",
    "    Returns:\n",
    "      - A dictionary with keys:\n",
    "          \"full\"     : The full interaction graph.\n",
    "          \"pr\"       : Graph containing only edges with at least one 'pr' interaction.\n",
    "          \"issue\"    : Graph containing only edges with at least one 'issue' interaction.\n",
    "          \"pr_review\": Graph containing only edges with at least one 'pr review' interaction.\n",
    "    \"\"\"\n",
    "\n",
    "    df = df.reset_index(drop=True)\n",
    "    df_sorted = df.sort_values(['id_number', 'created_at']).copy()\n",
    "    \n",
    "    # Vectorized function to compute previous different actor.\n",
    "    def compute_prev_diff_vectorized(s):\n",
    "        shifted = s.shift(1)\n",
    "        candidate = shifted.where(shifted != s)\n",
    "        candidate = candidate.ffill()\n",
    "        candidate.iloc[0] = None\n",
    "        return candidate\n",
    "    df_sorted['prev_diff'] = df_sorted.groupby('id_number')['actor_id'].transform(compute_prev_diff_vectorized)\n",
    "    \n",
    "    # Filter rows: exclude 'opened' and 'reopened', and rows without a previous different actor.\n",
    "    df_edges = df_sorted[\n",
    "        (df_sorted['issue_action'] != 'opened') &\n",
    "        (df_sorted['issue_action'] != 'reopened') &\n",
    "        (df_sorted['prev_diff'].notnull())\n",
    "    ]\n",
    "    \n",
    "    # Dictionaries to store node origins and edge attributes.\n",
    "    node_origins = {}  # Maps node -> set of origins\n",
    "    edge_dict = {}     # Maps edge (sorted tuple of nodes) -> {\"weight\": total, \"origin_counts\": {origin: count}}\n",
    "    \n",
    "    for _, row in df_edges.iterrows():\n",
    "        u = row['actor_id']\n",
    "        v = row['prev_diff']\n",
    "        origin = row['origin']\n",
    "        \n",
    "        # Record origin for each node.\n",
    "        node_origins.setdefault(u, set()).add(origin)\n",
    "        node_origins.setdefault(v, set()).add(origin)\n",
    "        \n",
    "        # Use a sorted tuple as the undirected edge key.\n",
    "        edge_key = tuple(sorted([u, v]))\n",
    "        if edge_key not in edge_dict:\n",
    "            edge_dict[edge_key] = {'weight': 0, 'origin_counts': {}}\n",
    "        edge_dict[edge_key]['weight'] += 1\n",
    "        edge_dict[edge_key]['origin_counts'][origin] = edge_dict[edge_key]['origin_counts'].get(origin, 0) + 1\n",
    "    \n",
    "    G = nx.Graph()\n",
    "    for edge_key, attr in edge_dict.items():\n",
    "        u, v = edge_key\n",
    "        G.add_edge(u, v, weight=attr['weight'], origin_counts=attr['origin_counts'])\n",
    "    \n",
    "    for node, origins in node_origins.items():\n",
    "        if node in G.nodes():\n",
    "            G.nodes[node]['origins'] = \" | \".join(list(origins))\n",
    "        else:\n",
    "            G.add_node(node, origins= \" | \".join(list(origins)))\n",
    "    \n",
    "    # Build subgraphs for each origin.\n",
    "    G_pr = nx.Graph()\n",
    "    G_issue = nx.Graph()\n",
    "    G_pr_review = nx.Graph()\n",
    "    \n",
    "    for u, v, data in G.edges(data=True):\n",
    "        if data['origin_counts'].get('pr', 0) > 0:\n",
    "            G_pr.add_node(u, **G.nodes[u])\n",
    "            G_pr.add_node(v, **G.nodes[v])\n",
    "            G_pr.add_edge(u, v,\n",
    "                          weight=data['origin_counts']['pr'],\n",
    "                          origin_counts={'pr': data['origin_counts']['pr']})\n",
    "        if data['origin_counts'].get('issue', 0) > 0:\n",
    "            G_issue.add_node(u, **G.nodes[u])\n",
    "            G_issue.add_node(v, **G.nodes[v])\n",
    "            G_issue.add_edge(u, v,\n",
    "                             weight=data['origin_counts']['issue'],\n",
    "                             origin_counts={'issue': data['origin_counts']['issue']})\n",
    "        if data['origin_counts'].get('pr review', 0) > 0:\n",
    "            G_pr_review.add_node(u, **G.nodes[u])\n",
    "            G_pr_review.add_node(v, **G.nodes[v])\n",
    "            G_pr_review.add_edge(u, v,\n",
    "                                 weight=data['origin_counts']['pr review'],\n",
    "                                 origin_counts={'pr review': data['origin_counts']['pr review']})\n",
    "    \n",
    "    return {\"full\": G, \"pr\": G_pr, \"issue\": G_issue, \"pr_review\": G_pr_review}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fb7d9784-c653-4ac1-9ac9-86dbf3747da2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ExportGraphs(repo, time_period_date, graphs):\n",
    "    \"\"\"\n",
    "    Exports graphs only if they contain nodes. Returns a dictionary log entry\n",
    "    indicating which graphs were exported and their file paths.\n",
    "    \n",
    "    Parameters:\n",
    "      - repo: Repository name.\n",
    "      - time_period_date: The time period (datetime object) used to create directories.\n",
    "      - graphs: A dictionary of graphs with keys \"full\", \"pr\", \"issue\", \"pr_review\".\n",
    "    \n",
    "    Returns:\n",
    "      A dictionary log entry.\n",
    "    \"\"\"\n",
    "    yearmonth = f\"{time_period_date.year}{str(time_period_date.month).zfill(2)}\"\n",
    "    output_dir = f\"issue/graphs/{yearmonth}\"\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    output_base = f\"{output_dir}/{repo.replace('/', '_')}\"\n",
    "    \n",
    "    log_entry = {\n",
    "         \"time_period_date\": str(time_period_date),\n",
    "         \"repo\": repo,\n",
    "         \"full_exported\": None,\n",
    "         \"pr_exported\": None,\n",
    "         \"issue_exported\": None,\n",
    "         \"pr_review_exported\": None,\n",
    "    }\n",
    "    \n",
    "    if graphs[\"full\"].number_of_nodes() != 0:\n",
    "         print(repo, time_period_date)\n",
    "         full_file = output_base + \".gexf\"\n",
    "         nx.write_gexf(graphs[\"full\"], full_file)\n",
    "         log_entry[\"full_exported\"] = full_file\n",
    "         \n",
    "    if graphs[\"pr\"].number_of_nodes() != 0:\n",
    "         pr_file = output_base + \"_pr.gexf\"\n",
    "         nx.write_gexf(graphs[\"pr\"], pr_file)\n",
    "         log_entry[\"pr_exported\"] = pr_file\n",
    "         \n",
    "    if graphs[\"issue\"].number_of_nodes() != 0:\n",
    "         issue_file = output_base + \"_issue.gexf\"\n",
    "         nx.write_gexf(graphs[\"issue\"], issue_file)\n",
    "         log_entry[\"issue_exported\"] = issue_file\n",
    "         \n",
    "    if graphs[\"pr_review\"].number_of_nodes() != 0:\n",
    "         pr_review_file = output_base + \"_pr_review.gexf\"\n",
    "         nx.write_gexf(graphs[\"pr_review\"], pr_review_file)\n",
    "         log_entry[\"pr_review_exported\"] = pr_review_file\n",
    "         \n",
    "    return log_entry\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ede7dc39-32a9-4c3a-b7d7-f5ed360718ce",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def SelectRepoData(repo, df_issue, df_pr, df_pr_commits):\n",
    "    \"\"\"\n",
    "    Select and return DataFrames for a given repo.\n",
    "    Returns a tuple: (df_issue_sel, df_pr_sel, df_pr_commits_sel).\n",
    "    If a DataFrame is missing for the repo, returns an empty DataFrame.\n",
    "    \"\"\"\n",
    "    df_issue_sel = df_issue.loc[[repo]].copy() if repo in df_issue.index else pd.DataFrame()\n",
    "    df_pr_sel = df_pr.loc[[repo]].copy() if repo in df_pr.index else pd.DataFrame()\n",
    "    df_pr_commits_sel = df_pr_commits.loc[[repo]].copy() if repo in df_pr_commits.index else pd.DataFrame()\n",
    "    return df_issue_sel, df_pr_sel, df_pr_commits_sel\n",
    "\n",
    "def ProcessReviewComments(df_pr_sel):\n",
    "    \"\"\"\n",
    "    Process Pull Request review comment events from df_pr_sel.\n",
    "    Returns a processed DataFrame with a standardized 'id_number' column.\n",
    "    \"\"\"\n",
    "    if df_pr_sel.shape[0]==0:\n",
    "        return pd.DataFrame()\n",
    "        \n",
    "    df_review = df_pr_sel[df_pr_sel['type'] == \"PullRequestReviewCommentEvent\"].copy()\n",
    "    df_review['combo'] = list(zip(\n",
    "        df_review['pr_review_comment_path'],\n",
    "        df_review['pr_review_comment_original_position'],\n",
    "        df_review['pr_review_comment_original_commit_id']\n",
    "    ))\n",
    "    df_review['id_number'] = df_review.groupby('pr_number')['combo']\\\n",
    "        .transform(lambda x: pd.factorize(x)[0])\n",
    "    valid_mask = df_review['pr_number'].notnull() & (df_review['pr_number'] != np.inf)\n",
    "    df_review = df_review[valid_mask]\n",
    "    df_review['id_number'] = (\n",
    "        \"pr_rc\" + \n",
    "        df_review['pr_number'].astype(int).astype(str) + \"_\" + \n",
    "        df_review['id_number'].astype(str)\n",
    "    )\n",
    "    df_review.drop(columns='combo', inplace=True)\n",
    "    return df_review\n",
    "\n",
    "def ImputeTimeEmptyRobust(df, time_period):\n",
    "    \"\"\"\n",
    "    Impute the time period into the DataFrame and reset the index.\n",
    "    Assumes ImputeTimePeriod is defined elsewhere.\n",
    "    \"\"\"\n",
    "    if df.empty:\n",
    "        return df\n",
    "    return ImputeTimePeriod(df, time_period).reset_index()\n",
    "\n",
    "\n",
    "def ProcessOtherComments(df_pr_sel, df_issue_sel):\n",
    "    \"\"\"\n",
    "    Process Pull Request events and Issue events.\n",
    "    Returns two DataFrames with a standardized 'id_number' column.\n",
    "    \"\"\"\n",
    "    if df_pr_sel.shape[0] != 0:\n",
    "        df_pr_comments = df_pr_sel[df_pr_sel['type'] == \"PullRequestEvent\"].copy()\n",
    "        df_pr_comments = df_pr_comments.rename(columns={'pr_number': 'id_number'})\n",
    "    else:\n",
    "        df_pr_comments = pd.DataFrame()\n",
    "\n",
    "    if df_issue_sel.shape[0] != 0:\n",
    "        df_issue_comments = df_issue_sel[df_issue_sel['issue_action'] != \"closed\"].copy()\n",
    "        df_issue_comments = df_issue_comments.rename(columns={'issue_number': 'id_number'})\n",
    "    else:\n",
    "        df_issue_comments = pd.DataFrame()\n",
    "    \n",
    "\n",
    "    return df_pr_comments, df_issue_comments\n",
    "\n",
    "def ConcatenateAndFilterDiscussions(df_pr_comments, df_issue_comments, df_review_comments, sel_cols, target_period):\n",
    "    \"\"\"\n",
    "    Concatenate all discussion interactions, sort them, and filter by time period.\n",
    "    \"\"\"\n",
    "    all_discussions = pd.concat(\n",
    "        [df_pr_comments, df_issue_comments, df_review_comments],\n",
    "        ignore_index=True\n",
    "    ).sort_values(['id_number', 'created_at'])\n",
    "    if 'issue_action' not in all_discussions.columns:\n",
    "        all_discussions['issue_action'] = np.nan\n",
    "\n",
    "    \n",
    "    return all_discussions[sel_cols]\n",
    "\n",
    "def CreateGraph(repo, time_period_date, exported_graphs_log):\n",
    "    \"\"\"\n",
    "    Process repository data to build an issue interaction graph.\n",
    "    This function:\n",
    "      1. Selects relevant DataFrames for the repo.\n",
    "      2. Imputes the time period and resets indices.\n",
    "      3. Filters each DataFrame to only include rows where the time period matches time_period_date.\n",
    "      4. Processes PR commits and PR authorship.\n",
    "      5. Processes review comments and other discussion events.\n",
    "      6. Checks if all DataFrames are empty; if so, logs the result and returns.\n",
    "      7. Concatenates interactions and filters by the target time period.\n",
    "      8. Builds the interaction graph.\n",
    "      9. Exports the graphs using ExportGraphs and logs the export paths.\n",
    "    \"\"\"\n",
    "    # Step 1: Data selection.\n",
    "    df_issue_sel, df_pr_sel, df_pr_commits_sel = SelectRepoData(repo, df_issue, df_pr, df_pr_commits)\n",
    "    if df_issue_sel.empty and df_pr_sel.empty and df_pr_commits_sel.empty:\n",
    "        return exported_graphs_log\n",
    "\n",
    "    # Step 2: Impute time period.\n",
    "    df_issue_sel = ImputeTimeEmptyRobust(df_issue_sel, time_period)\n",
    "    df_pr_sel = ImputeTimeEmptyRobust(df_pr_sel, time_period)\n",
    "    df_pr_commits_sel = ImputeTimeEmptyRobust(df_pr_commits_sel, time_period)\n",
    "\n",
    "    # Step 3: Filter to only include rows where the time period matches time_period_date.\n",
    "    if not df_issue_sel.empty:\n",
    "        df_issue_sel = df_issue_sel[df_issue_sel['time_period'] == time_period_date]\n",
    "    if not df_pr_sel.empty:\n",
    "        df_pr_sel = df_pr_sel[df_pr_sel['time_period'] == time_period_date]\n",
    "    if not df_pr_commits_sel.empty:\n",
    "        df_pr_commits_sel = df_pr_commits_sel[df_pr_commits_sel['time_period'] == time_period_date]\n",
    "\n",
    "    # Step 4: Process PR commits and authorship (for review counting).\n",
    "    if df_pr_sel.shape[0] > 0 and df_pr_commits_sel.shape[0] > 0:\n",
    "        df_pr_commit_stats = LinkCommits(df_pr_sel, df_pr_commits_sel, committers_match, commit_cols, 'pr')\n",
    "        df_pr_commit_author_stats = AssignPRAuthorship(df_pr_commit_stats, df_pr_sel, author_thresh, commit_cols)\n",
    "    else:\n",
    "        df_pr_commit_author_stats = pd.DataFrame()\n",
    "    \n",
    "    sel_cols = ['created_at', 'actor_id', 'id_number', 'type', 'issue_action', 'time_period', 'origin']\n",
    "    \n",
    "    # Step 5: Process discussion interactions.\n",
    "    df_pr_comments, df_issue_comments = ProcessOtherComments(df_pr_sel, df_issue_sel)\n",
    "    df_review_comments = ProcessReviewComments(df_pr_sel)\n",
    "    \n",
    "    # Step 6: Check if all DataFrames are empty.\n",
    "    if df_pr_comments.empty and df_issue_comments.empty and df_review_comments.empty:\n",
    "        log_entry = {\n",
    "             \"time_period_date\": str(time_period_date),\n",
    "             \"repo\": repo,\n",
    "             \"full_exported\": None,\n",
    "             \"pr_exported\": None,\n",
    "             \"issue_exported\": None,\n",
    "             \"pr_review_exported\": None,\n",
    "        }\n",
    "        exported_graphs_log.append(log_entry)    \n",
    "        return exported_graphs_log\n",
    "\n",
    "    # Step 7: Concatenate and filter discussions.\n",
    "    discussion_filtered = ConcatenateAndFilterDiscussions(\n",
    "        df_pr_comments.assign(origin='pr'), \n",
    "        df_issue_comments.assign(origin='issue'), \n",
    "        df_review_comments.assign(origin='pr review'), sel_cols, str(time_period_date.date())\n",
    "    )\n",
    "\n",
    "    # Step 8: Build the interaction graph.\n",
    "    graphs = BuildIssueInteractionGraph(discussion_filtered, 'keep_consecutive')\n",
    "\n",
    "    # Step 9: Export graphs and update the log.\n",
    "    log_entry = ExportGraphs(repo, time_period_date, graphs)\n",
    "    exported_graphs_log.append(log_entry)    \n",
    "    return exported_graphs_log\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccdebc33-cfa0-4007-8a28-3854fcce4bda",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d276430d-55f9-4901-84c3-f050e6ff9f8b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
