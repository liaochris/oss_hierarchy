{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "41942ffe-16c4-4335-8687-93083ffe441c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir('../')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fcc9a74b-b50a-4fee-9ea4-ba5afbfbc14a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: Pandarallel will run on 8 workers.\n",
      "INFO: Pandarallel will use Memory file system to transfer data between the main process and workers.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import sys\n",
    "import glob\n",
    "import warnings\n",
    "import random\n",
    "from pandarallel import pandarallel\n",
    "from source.lib.JMSLab import autofill\n",
    "from source.lib.helpers import ExportTable, AddToTableList\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "pd.set_option('display.max_columns', None)\n",
    "pandarallel.initialize(progress_bar = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "decac92b-83d9-4cd0-9925-40e97893905e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ReadPrIssueData(file_dirs, data_cols):\n",
    "    df_final = pd.DataFrame(columns = data_cols)\n",
    "    for file in file_dirs:\n",
    "        df_part = pd.read_csv(file, nrows = 1)\n",
    "        df_part_cols = [col for col in data_cols if col in df_part.columns]\n",
    "        df_part = pd.read_csv(file, usecols = df_part_cols)\n",
    "        df_final = pd.concat([df_final, df_part]).drop_duplicates()\n",
    "\n",
    "    df_final = AddDates(df_final)\n",
    "\n",
    "    return df_final\n",
    "\n",
    "def AddDates(df):\n",
    "    df['created_at'] = pd.to_datetime(df['created_at'])\n",
    "    df['date'] = df.parallel_apply(lambda x: f\"{x['created_at'].year}-{x['created_at'].month}\", axis = 1)\n",
    "\n",
    "    return df\n",
    "\n",
    "def ReturnMeanMedStd(pd_series):\n",
    "    return [pd_series.mean(), np.median(pd_series), np.std(pd_series)]\n",
    "\n",
    "def GetIssueStats(df_issue_selected, df_pr_selected, table_list_length, repo_col):\n",
    "    issue_stats = []\n",
    "    months_active = pd.concat([\n",
    "        df_issue_selected[[repo_col, 'date']].drop_duplicates(),\n",
    "        df_pr_selected[[repo_col, 'date']].drop_duplicates()\n",
    "    ]).drop_duplicates().groupby(repo_col)['date'].count()\n",
    "    proj_activity = [\"\"]\n",
    "    proj_activity.extend(returnMeanMedStd(months_active))\n",
    "\n",
    "    issue_stats = AddToTableList(issue_stats, proj_activity, table_list_length)\n",
    "\n",
    "    opened_activity = OpenCloseStats(df_issue_selected, 'issue_action == \"opened\"', [repo_col,'issue_number'])\n",
    "    closed_activity = OpenCloseStats(df_issue_selected, 'issue_action == \"closed\"', [repo_col,'issue_number'])\n",
    "    comment_activity = OpenCloseStats(df_issue_selected, 'type == \"IssueCommentEvent\"', [repo_col,'issue_number','issue_comment_id'])\n",
    "\n",
    "    issue_stats = AddToTableList(issue_stats, opened_activity, table_list_length)\n",
    "    issue_stats = AddToTableList(issue_stats, closed_activity, table_list_length)\n",
    "    issue_stats = AddToTableList(issue_stats, comment_activity, table_list_length)\n",
    "\n",
    "    opened_people = PeopleStats(df_issue_selected, 'issue_action == \"opened\"')\n",
    "    closed_people = PeopleStats(df_issue_selected, 'issue_action == \"closed\"')\n",
    "    comment_people = PeopleStats(df_issue_selected, 'type == \"IssueCommentEvent\"')\n",
    "\n",
    "    issue_stats = AddToTableList(issue_stats, opened_people, table_list_length)\n",
    "    issue_stats = AddToTableList(issue_stats, closed_people, table_list_length)\n",
    "    issue_stats = AddToTableList(issue_stats, comment_people, table_list_length)\n",
    "    \n",
    "    return issue_stats\n",
    "\n",
    "def OpenCloseStats(df, query_filter, dup_cols):\n",
    "    df_filtered = df.query(query_filter)\n",
    "    df_filtered_stats = df_filtered.drop_duplicates([dup_cols]).groupby(repo_col)['type'].count()\n",
    "    \n",
    "    df_filtered_month_stats = df_filtered.drop_duplicates([dup_cols]).groupby([repo_col, 'date'])['type'].count()\n",
    "    df_filtered_activity = [df_filtered.shape[0]]\n",
    "    df_filtered_activity.extend(returnMeanMedStd(df_filtered_stats))\n",
    "    df_filtered_activity.extend(returnMeanMedStd(df_filtered_month_stats))\n",
    "\n",
    "    return df_filtered_activity\n",
    "\n",
    "def PeopleStats(df, query_filter):\n",
    "    df_filtered = df.query(query_filter)\n",
    "    df_filtered_stats = df_filtered.groupby(repo_col)['actor_id'].nunique()\n",
    "    \n",
    "    df_filtered_month_stats = df_filtered.groupby([repo_col, 'date'])['actor_id'].nunique()\n",
    "    df_filtered_activity = [len(df_filtered['actor_id'].unique())]\n",
    "    df_filtered_activity.extend(returnMeanMedStd(df_filtered_stats))\n",
    "    df_filtered_activity.extend(returnMeanMedStd(df_filtered_month_stats))\n",
    "\n",
    "    return df_filtered_activity\n",
    "\n",
    "\n",
    "def GetIssueClosingStats(df_issue_selected, df_pr_selected, table_list_length, repo_col):\n",
    "    issue_closing_stats = []\n",
    "    selcols = ['created_at',repo_col,'issue_number']\n",
    "\n",
    "    opened_issues = df_issue_selected.query('issue_action == \"opened\"')[\n",
    "        selcols].dropna().drop_duplicates().rename({'created_at':'opened_date'}, axis = 1)\n",
    "    closed_issues = df_issue_selected.query('issue_action == \"closed\"')[\n",
    "        selcols].dropna().drop_duplicates().rename({'created_at':'closed_date'}, axis = 1)\n",
    "    df_merged_issues = pd.merge(opened_issues, closed_issues, how = 'left')\n",
    "    df_merged_issues['closed'] = df_merged_issues['closed_date'].notna()\n",
    "    num_issues = df_merged_issues[[repo_col,'issue_number']].drop_duplicates().shape[0]\n",
    "    closed_issues = df_merged_issues.query('~closed_date.isna()')[[repo_col,'issue_number']].drop_duplicates().shape[0]\n",
    "    closed_pct_activity = [num_issues, closed_issues]\n",
    "    closed_pct = df_merged_issues.groupby(repo_col)['closed'].mean()\n",
    "    closed_pct_activity.extend(returnMeanMedStd(closed_pct))\n",
    "\n",
    "    issue_closing_stats = AddToTableList(issue_closing_stats, closed_pct_activity, table_list_length)\n",
    "\n",
    "    df_merged_issues['closing_time'] = df_merged_issues.parallel_apply(\n",
    "        lambda x: (x['closed_date']-x['opened_date']).total_seconds(), axis = 1)\n",
    "    closing_time_days = df_merged_issues.groupby(repo_col)['closing_time'].mean().dropna()/86400\n",
    "    closing_time_activity = [num_issues, closed_issues]\n",
    "    closing_time_activity = returnMeanMedStd(closing_time_days)\n",
    "    \n",
    "    issue_closing_stats = AddToTableList(issue_closing_stats, closing_time_activity, table_list_length)\n",
    "\n",
    "    for days in [30, 60, 180]:\n",
    "        df_merged_issues[f'closed_{days}_days'] = df_merged_issues['closing_time'] <= days * 86400\n",
    "        \n",
    "    closed_cond = []\n",
    "    closed_uncond = []\n",
    "    for days in [30, 60, 180]:\n",
    "        closed_timeline = f'closed_{days}_days'\n",
    "        df_subset_cond_mean = df_merged_issues.dropna().groupby(repo_col)[closed_timeline].mean()\n",
    "        num_closed_cond = df_merged_issues.dropna()[closed_timeline].sum()\n",
    "        closed_cond.extend([num_closed_cond])\n",
    "        closed_cond.extend(returnMeanMedStd(df_subset_cond_mean))\n",
    "                          \n",
    "        df_subset_uncond_mean = df_merged_issues.groupby(repo_col)[closed_timeline].mean()\n",
    "        num_closed_uncond = df_merged_issues.dropna()[closed_timeline].sum()\n",
    "        closed_uncond.extend([num_closed_uncond])\n",
    "        closed_uncond.extend(returnMeanMedStd(df_subset_uncond_mean))\n",
    "\n",
    "    issue_closing_stats = AddToTableList(issue_closing_stats, closed_cond, table_list_length)\n",
    "    issue_closing_stats = AddToTableList(issue_closing_stats, closed_uncond, table_list_length)\n",
    "\n",
    "    return issue_closing_stats\n",
    "\n",
    "def GetIssueCommentStats(df_issue_selected, df_pr_selected, table_list_length, repo_col):\n",
    "    issue_comment_stats = []\n",
    "    \n",
    "    df_issue_comments = df_issue_selected.query('type == \"IssueCommentEvent\"')\n",
    "    df_issue_time = df_issue_selected.query('issue_action == \"opened\"')[[repo_col,'issue_number','created_at']].dropna()\n",
    "    df_issue_time.rename({'created_at': 'opened_date'}, axis = 1, inplace = True)\n",
    "    df_issue_comments_details = pd.merge(df_issue_time, df_issue_comments, how = 'left', on = [repo_col, 'issue_number'])\n",
    "\n",
    "    num_comments = df_issue_comments_details[[repo_col,'issue_comment_id']].dropna().drop_duplicates().shape[0]\n",
    "    num_issues = df_issue_comments_details[[repo_col,'issue_number']].drop_duplicates().shape[0]\n",
    "    \n",
    "    df_no_comments = df_issue_comments_details.query('issue_comment_id.isna()')[[repo_col,'issue_number']].assign(\n",
    "        issue_comment_id = 0).set_index([repo_col,'issue_number'])\n",
    "    df_has_comments = df_issue_comments_details.query('~issue_comment_id.isna()').groupby(\n",
    "               [repo_col, 'issue_number'])['issue_comment_id'].nunique()\n",
    "    issue_comment_counts = pd.concat([df_no_comments,df_has_comments])['issue_comment_id']\n",
    "    comment_activity = [num_comments]\n",
    "    comment_activity.extend(returnMeanMedStd(issue_comment_counts))\n",
    "    issue_comment_stats = AddToTableList(issue_comment_stats, comment_activity, table_list_length)\n",
    "    \n",
    "    df_issue_comments_details['comment_time'] = df_issue_comments_details.parallel_apply(\n",
    "        lambda x: (x['created_at']-x['opened_date']).total_seconds(), axis = 1)\n",
    "    comment_time_days = df_issue_comments_details.groupby(repo_col)['comment_time'].mean().dropna()/86400\n",
    "\n",
    "    comment_time_activity = [num_comments]\n",
    "    comment_time_activity.extend(returnMeanMedStd(comment_time_days))\n",
    "    issue_comment_stats = AddToTableList(issue_comment_stats, comment_time_activity, table_list_length)\n",
    "\n",
    "    for days in [30, 60, 180]:\n",
    "        df_issue_comments_details[f'comment_{days}_days'] = df_issue_comments_details['comment_time'] <= days * 86400\n",
    "\n",
    "    comment_days_prop = [num_issues]\n",
    "    comment_days_mean = [num_issues]\n",
    "    for days in [30, 60, 180]:\n",
    "        comment_timeline = f'comment_{days}_days'\n",
    "        df_closed_prop = df_issue_comments_details.groupby(repo_col)[comment_timeline].mean()\n",
    "        num_comments_days = df_issue_comments_details.query(f'{comment_timeline} == True')[[repo_col,'issue_comment_id']].drop_duplicates().shape[0]\n",
    "        \n",
    "        comment_days_prop.extend([num_comments_days])\n",
    "        comment_days_prop.extend(returnMeanMedStd(df_closed_prop))\n",
    "    \n",
    "        df_closed_mean = df_issue_comments_details.query(f'{comment_timeline} == True').groupby(repo_col)['type'].count()\n",
    "        comment_days_mean.extend([num_comments_days])\n",
    "        comment_days_mean.extend(returnMeanMedStd(df_closed_mean))\n",
    "\n",
    "    issue_comment_stats = AddToTableList(issue_comment_stats, comment_days_prop, table_list_length)\n",
    "    issue_comment_stats = AddToTableList(issue_comment_stats, comment_days_mean, table_list_length)\n",
    "\n",
    "    return issue_comment_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f7359494-bb34-4092-9c02-b69425784f03",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ndef GetPRStats(df_issue_selected, df_pr_selected, table_list_length, repo_col):\\n    pr_stats = []\\n    months_active = pd.concat([\\n        df_issue_selected[[repo_col, \\'date\\']].drop_duplicates(),\\n        df_pr_selected[[repo_col, \\'date\\']].drop_duplicates()\\n    ]).drop_duplicates().groupby(repo_col)[\\'date\\'].count()\\n    proj_activity = [\"\"]\\n    proj_activity.extend(returnMeanMedStd(months_active))\\n\\n    pr_stats = AddToTableList(pr_stats, proj_activity, table_list_length)\\n\\n    opened_activity = OpenCloseStats(df_pr_selected, \\'type == \"PullRequestEvent\" & pr_action == \"opened\"\\', [repo_col, \\'pr_number\\'])\\n    closed_activity = OpenCloseStats(df_pr_selected, \\'type == \"PullRequestEvent\" & pr_action == \"closed\" & pr_merged_by_id.isna()\\', [repo_col, \\'pr_number\\'])\\n    merged_activity = OpenCloseStats(df_pr_selected, \\'type == \"PullRequestEvent\" & pr_action == \"closed\" & ~pr_merged_by_id.isna()\\', [repo_col, \\'pr_number\\'])\\n   # separate categories for PR reviews and PR review comments\\n    \\n    comment_activity = OpenCloseStats(df_pr_selected, \\'type == \"IssueCommentEvent\"\\')\\n\\n    issue_stats = AddToTableList(issue_stats, opened_activity, table_list_length)\\n    issue_stats = AddToTableList(issue_stats, closed_activity, table_list_length)\\n    issue_stats = AddToTableList(issue_stats, comment_activity, table_list_length)\\n\\n    # also have people for each of these \\n    opened_people = PeopleStats(df_issue_selected, \\'issue_action == \"opened\"\\')\\n    closed_people = PeopleStats(df_issue_selected, \\'issue_action == \"closed\"\\')\\n    comment_people = PeopleStats(df_issue_selected, \\'type == \"IssueCommentEvent\"\\')\\n\\n    issue_stats = AddToTableList(issue_stats, opened_people, table_list_length)\\n    issue_stats = AddToTableList(issue_stats, closed_people, table_list_length)\\n    issue_stats = AddToTableList(issue_stats, comment_people, table_list_length)\\n    \\n    return issue_stats\\n\\n\\ndef GetIssueClosingStats(df_issue_selected, df_pr_selected, table_list_length, repo_col):\\n    issue_closing_stats = []\\n    selcols = [\\'created_at\\',repo_col,\\'issue_number\\']\\n\\n    opened_issues = df_issue_selected.query(\\'issue_action == \"opened\"\\')[\\n        selcols].dropna().drop_duplicates().rename({\\'created_at\\':\\'opened_date\\'}, axis = 1)\\n    closed_issues = df_issue_selected.query(\\'issue_action == \"closed\"\\')[\\n        selcols].dropna().drop_duplicates().rename({\\'created_at\\':\\'closed_date\\'}, axis = 1)\\n    df_merged_issues = pd.merge(opened_issues, closed_issues, how = \\'left\\')\\n    df_merged_issues[\\'closed\\'] = df_merged_issues[\\'closed_date\\'].notna()\\n    num_issues = df_merged_issues[[repo_col,\\'issue_number\\']].drop_duplicates().shape[0]\\n    closed_pct_activity = [num_issues]\\n    closed_pct = df_merged_issues.groupby(repo_col)[\\'closed\\'].mean()\\n    closed_pct_activity.extend(returnMeanMedStd(closed_pct))\\n\\n    issue_closing_stats = AddToTableList(issue_closing_stats, closed_pct_activity, table_list_length)\\n\\n    df_merged_issues[\\'closing_time\\'] = df_merged_issues.parallel_apply(\\n        lambda x: (x[\\'closed_date\\']-x[\\'opened_date\\']).total_seconds(), axis = 1)\\n    closing_time_days = df_merged_issues.groupby(repo_col)[\\'closing_time\\'].mean().dropna()/86400\\n    closing_time_activity = [num_issues]\\n    closing_time_activity = returnMeanMedStd(closing_time_days)\\n    \\n    issue_closing_stats = AddToTableList(issue_closing_stats, closing_time_activity, table_list_length)\\n\\n    for days in [30, 60, 180]:\\n        df_merged_issues[f\\'closed_{days}_days\\'] = df_merged_issues[\\'closing_time\\'] <= days * 86400\\n        \\n    closed_cond = []\\n    closed_uncond = []\\n    for days in [30, 60, 180]:\\n        closed_timeline = f\\'closed_{days}_days\\'\\n        df_subset_cond_mean = df_merged_issues.dropna().groupby(repo_col)[closed_timeline].mean()\\n        num_closed_cond = df_merged_issues.dropna()[closed_timeline].sum()\\n        closed_cond.extend([num_closed_cond])\\n        closed_cond.extend(returnMeanMedStd(df_subset_cond_mean))\\n                          \\n        df_subset_uncond_mean = df_merged_issues.groupby(repo_col)[closed_timeline].mean()\\n        num_closed_uncond = df_merged_issues.dropna()[closed_timeline].sum()\\n        closed_uncond.extend([num_closed_uncond])\\n        closed_uncond.extend(returnMeanMedStd(df_subset_uncond_mean))\\n\\n    issue_closing_stats = AddToTableList(issue_closing_stats, closed_cond, table_list_length)\\n    issue_closing_stats = AddToTableList(issue_closing_stats, closed_uncond, table_list_length)\\n\\n    return issue_closing_stats\\n\\ndef GetIssueCommentStats(df_issue_selected, df_pr_selected, table_list_length, repo_col):\\n    issue_comment_stats = []\\n    \\n    df_issue_comments = df_issue_selected.query(\\'type == \"IssueCommentEvent\"\\')\\n    df_issue_time = df_issue_selected.query(\\'issue_action == \"opened\"\\')[[repo_col,\\'issue_number\\',\\'created_at\\']].dropna()\\n    df_issue_time.rename({\\'created_at\\': \\'opened_date\\'}, axis = 1, inplace = True)\\n    df_issue_comments_details = pd.merge(df_issue_time, df_issue_comments, how = \\'left\\', on = [repo_col, \\'issue_number\\'])\\n\\n    num_comments = df_issue_comments_details[[repo_col,\\'issue_comment_id\\']].dropna().drop_duplicates().shape[0]\\n    num_issues = df_issue_comments_details[[repo_col,\\'issue_number\\']].drop_duplicates().shape[0]\\n    \\n    df_no_comments = df_issue_comments_details.query(\\'issue_comment_id.isna()\\')[[repo_col,\\'issue_number\\']].assign(\\n        issue_comment_id = 0).set_index([repo_col,\\'issue_number\\'])\\n    df_has_comments = df_issue_comments_details.query(\\'~issue_comment_id.isna()\\').groupby(\\n               [repo_col, \\'issue_number\\'])[\\'issue_comment_id\\'].nunique()\\n    issue_comment_counts = pd.concat([df_no_comments,df_has_comments])[\\'issue_comment_id\\']\\n    comment_activity = [num_comments]\\n    comment_activity.extend(returnMeanMedStd(issue_comment_counts))\\n    issue_comment_stats = AddToTableList(issue_comment_stats, comment_activity, table_list_length)\\n    \\n    df_issue_comments_details[\\'comment_time\\'] = df_issue_comments_details.parallel_apply(\\n        lambda x: (x[\\'created_at\\']-x[\\'opened_date\\']).total_seconds(), axis = 1)\\n    comment_time_days = df_issue_comments_details.groupby(repo_col)[\\'comment_time\\'].mean().dropna()/86400\\n\\n    comment_time_activity = [num_comments]\\n    comment_time_activity.extend(returnMeanMedStd(comment_time_days))\\n    issue_comment_stats = AddToTableList(issue_comment_stats, comment_time_activity, table_list_length)\\n\\n    for days in [30, 60, 180]:\\n        df_issue_comments_details[f\\'comment_{days}_days\\'] = df_issue_comments_details[\\'comment_time\\'] <= days * 86400\\n\\n    comment_days_prop = [num_issues]\\n    comment_days_mean = [num_issues]\\n    for days in [30, 60, 180]:\\n        comment_timeline = f\\'comment_{days}_days\\'\\n        df_closed_prop = df_issue_comments_details.groupby(repo_col)[comment_timeline].mean()\\n        num_comments_days = df_issue_comments_details.query(f\\'{comment_timeline} == True\\')[[repo_col,\\'issue_comment_id\\']].drop_duplicates().shape[0]\\n        \\n        comment_days_prop.extend([num_comments_days])\\n        comment_days_prop.extend(returnMeanMedStd(df_closed_prop))\\n    \\n        df_closed_mean = df_issue_comments_details.query(f\\'{comment_timeline} == True\\').groupby(repo_col)[\\'type\\'].count()\\n        comment_days_mean.extend([num_comments_days])\\n        comment_days_mean.extend(returnMeanMedStd(df_closed_mean))\\n\\n    issue_comment_stats = AddToTableList(issue_comment_stats, comment_days_prop, table_list_length)\\n    issue_comment_stats = AddToTableList(issue_comment_stats, comment_days_mean, table_list_length)\\n\\n    return issue_comment_stats\\n'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# need to actually code up get pr stats\n",
    "# but also i think i said i wanted to reformat the table anyways...\n",
    "\"\"\"\n",
    "def GetPRStats(df_issue_selected, df_pr_selected, table_list_length, repo_col):\n",
    "    pr_stats = []\n",
    "    months_active = pd.concat([\n",
    "        df_issue_selected[[repo_col, 'date']].drop_duplicates(),\n",
    "        df_pr_selected[[repo_col, 'date']].drop_duplicates()\n",
    "    ]).drop_duplicates().groupby(repo_col)['date'].count()\n",
    "    proj_activity = [\"\"]\n",
    "    proj_activity.extend(returnMeanMedStd(months_active))\n",
    "\n",
    "    pr_stats = AddToTableList(pr_stats, proj_activity, table_list_length)\n",
    "\n",
    "    opened_activity = OpenCloseStats(df_pr_selected, 'type == \"PullRequestEvent\" & pr_action == \"opened\"', [repo_col, 'pr_number'])\n",
    "    closed_activity = OpenCloseStats(df_pr_selected, 'type == \"PullRequestEvent\" & pr_action == \"closed\" & pr_merged_by_id.isna()', [repo_col, 'pr_number'])\n",
    "    merged_activity = OpenCloseStats(df_pr_selected, 'type == \"PullRequestEvent\" & pr_action == \"closed\" & ~pr_merged_by_id.isna()', [repo_col, 'pr_number'])\n",
    "   # separate categories for PR reviews and PR review comments\n",
    "    \n",
    "    comment_activity = OpenCloseStats(df_pr_selected, 'type == \"IssueCommentEvent\"')\n",
    "\n",
    "    issue_stats = AddToTableList(issue_stats, opened_activity, table_list_length)\n",
    "    issue_stats = AddToTableList(issue_stats, closed_activity, table_list_length)\n",
    "    issue_stats = AddToTableList(issue_stats, comment_activity, table_list_length)\n",
    "\n",
    "    # also have people for each of these \n",
    "    opened_people = PeopleStats(df_issue_selected, 'issue_action == \"opened\"')\n",
    "    closed_people = PeopleStats(df_issue_selected, 'issue_action == \"closed\"')\n",
    "    comment_people = PeopleStats(df_issue_selected, 'type == \"IssueCommentEvent\"')\n",
    "\n",
    "    issue_stats = AddToTableList(issue_stats, opened_people, table_list_length)\n",
    "    issue_stats = AddToTableList(issue_stats, closed_people, table_list_length)\n",
    "    issue_stats = AddToTableList(issue_stats, comment_people, table_list_length)\n",
    "    \n",
    "    return issue_stats\n",
    "\n",
    "\n",
    "def GetIssueClosingStats(df_issue_selected, df_pr_selected, table_list_length, repo_col):\n",
    "    issue_closing_stats = []\n",
    "    selcols = ['created_at',repo_col,'issue_number']\n",
    "\n",
    "    opened_issues = df_issue_selected.query('issue_action == \"opened\"')[\n",
    "        selcols].dropna().drop_duplicates().rename({'created_at':'opened_date'}, axis = 1)\n",
    "    closed_issues = df_issue_selected.query('issue_action == \"closed\"')[\n",
    "        selcols].dropna().drop_duplicates().rename({'created_at':'closed_date'}, axis = 1)\n",
    "    df_merged_issues = pd.merge(opened_issues, closed_issues, how = 'left')\n",
    "    df_merged_issues['closed'] = df_merged_issues['closed_date'].notna()\n",
    "    num_issues = df_merged_issues[[repo_col,'issue_number']].drop_duplicates().shape[0]\n",
    "    closed_pct_activity = [num_issues]\n",
    "    closed_pct = df_merged_issues.groupby(repo_col)['closed'].mean()\n",
    "    closed_pct_activity.extend(returnMeanMedStd(closed_pct))\n",
    "\n",
    "    issue_closing_stats = AddToTableList(issue_closing_stats, closed_pct_activity, table_list_length)\n",
    "\n",
    "    df_merged_issues['closing_time'] = df_merged_issues.parallel_apply(\n",
    "        lambda x: (x['closed_date']-x['opened_date']).total_seconds(), axis = 1)\n",
    "    closing_time_days = df_merged_issues.groupby(repo_col)['closing_time'].mean().dropna()/86400\n",
    "    closing_time_activity = [num_issues]\n",
    "    closing_time_activity = returnMeanMedStd(closing_time_days)\n",
    "    \n",
    "    issue_closing_stats = AddToTableList(issue_closing_stats, closing_time_activity, table_list_length)\n",
    "\n",
    "    for days in [30, 60, 180]:\n",
    "        df_merged_issues[f'closed_{days}_days'] = df_merged_issues['closing_time'] <= days * 86400\n",
    "        \n",
    "    closed_cond = []\n",
    "    closed_uncond = []\n",
    "    for days in [30, 60, 180]:\n",
    "        closed_timeline = f'closed_{days}_days'\n",
    "        df_subset_cond_mean = df_merged_issues.dropna().groupby(repo_col)[closed_timeline].mean()\n",
    "        num_closed_cond = df_merged_issues.dropna()[closed_timeline].sum()\n",
    "        closed_cond.extend([num_closed_cond])\n",
    "        closed_cond.extend(returnMeanMedStd(df_subset_cond_mean))\n",
    "                          \n",
    "        df_subset_uncond_mean = df_merged_issues.groupby(repo_col)[closed_timeline].mean()\n",
    "        num_closed_uncond = df_merged_issues.dropna()[closed_timeline].sum()\n",
    "        closed_uncond.extend([num_closed_uncond])\n",
    "        closed_uncond.extend(returnMeanMedStd(df_subset_uncond_mean))\n",
    "\n",
    "    issue_closing_stats = AddToTableList(issue_closing_stats, closed_cond, table_list_length)\n",
    "    issue_closing_stats = AddToTableList(issue_closing_stats, closed_uncond, table_list_length)\n",
    "\n",
    "    return issue_closing_stats\n",
    "\n",
    "def GetIssueCommentStats(df_issue_selected, df_pr_selected, table_list_length, repo_col):\n",
    "    issue_comment_stats = []\n",
    "    \n",
    "    df_issue_comments = df_issue_selected.query('type == \"IssueCommentEvent\"')\n",
    "    df_issue_time = df_issue_selected.query('issue_action == \"opened\"')[[repo_col,'issue_number','created_at']].dropna()\n",
    "    df_issue_time.rename({'created_at': 'opened_date'}, axis = 1, inplace = True)\n",
    "    df_issue_comments_details = pd.merge(df_issue_time, df_issue_comments, how = 'left', on = [repo_col, 'issue_number'])\n",
    "\n",
    "    num_comments = df_issue_comments_details[[repo_col,'issue_comment_id']].dropna().drop_duplicates().shape[0]\n",
    "    num_issues = df_issue_comments_details[[repo_col,'issue_number']].drop_duplicates().shape[0]\n",
    "    \n",
    "    df_no_comments = df_issue_comments_details.query('issue_comment_id.isna()')[[repo_col,'issue_number']].assign(\n",
    "        issue_comment_id = 0).set_index([repo_col,'issue_number'])\n",
    "    df_has_comments = df_issue_comments_details.query('~issue_comment_id.isna()').groupby(\n",
    "               [repo_col, 'issue_number'])['issue_comment_id'].nunique()\n",
    "    issue_comment_counts = pd.concat([df_no_comments,df_has_comments])['issue_comment_id']\n",
    "    comment_activity = [num_comments]\n",
    "    comment_activity.extend(returnMeanMedStd(issue_comment_counts))\n",
    "    issue_comment_stats = AddToTableList(issue_comment_stats, comment_activity, table_list_length)\n",
    "    \n",
    "    df_issue_comments_details['comment_time'] = df_issue_comments_details.parallel_apply(\n",
    "        lambda x: (x['created_at']-x['opened_date']).total_seconds(), axis = 1)\n",
    "    comment_time_days = df_issue_comments_details.groupby(repo_col)['comment_time'].mean().dropna()/86400\n",
    "\n",
    "    comment_time_activity = [num_comments]\n",
    "    comment_time_activity.extend(returnMeanMedStd(comment_time_days))\n",
    "    issue_comment_stats = AddToTableList(issue_comment_stats, comment_time_activity, table_list_length)\n",
    "\n",
    "    for days in [30, 60, 180]:\n",
    "        df_issue_comments_details[f'comment_{days}_days'] = df_issue_comments_details['comment_time'] <= days * 86400\n",
    "\n",
    "    comment_days_prop = [num_issues]\n",
    "    comment_days_mean = [num_issues]\n",
    "    for days in [30, 60, 180]:\n",
    "        comment_timeline = f'comment_{days}_days'\n",
    "        df_closed_prop = df_issue_comments_details.groupby(repo_col)[comment_timeline].mean()\n",
    "        num_comments_days = df_issue_comments_details.query(f'{comment_timeline} == True')[[repo_col,'issue_comment_id']].drop_duplicates().shape[0]\n",
    "        \n",
    "        comment_days_prop.extend([num_comments_days])\n",
    "        comment_days_prop.extend(returnMeanMedStd(df_closed_prop))\n",
    "    \n",
    "        df_closed_mean = df_issue_comments_details.query(f'{comment_timeline} == True').groupby(repo_col)['type'].count()\n",
    "        comment_days_mean.extend([num_comments_days])\n",
    "        comment_days_mean.extend(returnMeanMedStd(df_closed_mean))\n",
    "\n",
    "    issue_comment_stats = AddToTableList(issue_comment_stats, comment_days_prop, table_list_length)\n",
    "    issue_comment_stats = AddToTableList(issue_comment_stats, comment_days_mean, table_list_length)\n",
    "\n",
    "    return issue_comment_stats\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0105b5e3-1107-463a-9f1d-12ddb79e389f",
   "metadata": {},
   "outputs": [],
   "source": [
    "OUTDIR = Path('output/analysis/descriptives/')\n",
    "issue_table_list_length = 7\n",
    "issue_closing_table_list_length = 12\n",
    "issue_comment_table_list_length = 13\n",
    "repo_col = 'repo_name'\n",
    "\n",
    "pr_data_indir = glob.glob('drive/output/scrape/extract_github_data/pull_request_data/*.csv')\n",
    "pr_data_indir.extend(glob.glob('drive/output/scrape/extract_github_data/pull_request_review_data/*.csv'))\n",
    "pr_data_indir.extend(glob.glob('drive/output/scrape/extract_github_data/pull_request_review_comment_data/*.csv'))\n",
    "pr_cols = ['type','created_at','repo_id','repo_name','actor_id','actor_login','pr_number', 'pr_title',\n",
    "           'pr_body', 'pr_action','pr_merged_by_id','pr_merged_by_type','pr_label', 'pr_review_action',\n",
    "           'pr_review_id','pr_review_state', 'pr_review_body', 'pr_review_comment_body']\n",
    "df_pr = ReadPrIssueData(pr_data_indir, pr_cols)\n",
    "\n",
    "issue_data_indir = glob.glob('drive/output/scrape/extract_github_data/issue_data/*.csv')\n",
    "issue_data_indir.extend(glob.glob('drive/output/scrape/extract_github_data/issue_comment_data/*.csv'))\n",
    "issue_cols = ['type','created_at','repo_id','repo_name','actor_id','actor_login','issue_number', 'issue_body','issue_title',\n",
    "              'issue_action','issue_state', 'issue_comment_id', 'issue_user_id', 'issue_comment_body']\n",
    "df_issue = ReadPrIssueData(issue_data_indir, issue_cols)\n",
    "\n",
    "df_pr_selected = df_pr\n",
    "df_issue_selected = df_issue"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da25bb11-913a-4a55-a262-05692e57fd48",
   "metadata": {},
   "source": [
    "### Aggregate Activity Occurring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc24db0e-3f45-4d0e-a1f9-71e6a202aed2",
   "metadata": {},
   "outputs": [],
   "source": [
    "issue_stats = GetIssueStats(df_issue_selected, df_pr_selected, issue_table_list_length, repo_col)\n",
    "ExportTable(OUTDIR / 'issue_stats.txt', \n",
    "            issue_stats, 'issue_stats',\n",
    "           fmt = \"%s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c441c90d-1899-4328-9fc0-5afa0ff49df5",
   "metadata": {},
   "outputs": [],
   "source": [
    "issue_closing_stats = GetIssueClosingStats(df_issue_selected, df_pr_selected, issue_closing_table_list_length, repo_col)\n",
    "ExportTable(OUTDIR / 'issue_closing_stats.txt', \n",
    "            issue_closing_stats, 'issue_closing_stats',\n",
    "            fmt = \"%s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66111404-ddb9-451d-b3e0-b52e0e5f4246",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "issue_comment_stats = GetIssueCommentStats(df_issue_selected, df_pr_selected, issue_comment_table_list_length, repo_col)\n",
    "ExportTable(OUTDIR / 'issue_comment_stats.txt', \n",
    "            issue_comment_stats, 'issue_comment_stats',\n",
    "            fmt = \"%s\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "oss_hierarchy",
   "language": "python",
   "name": "oss_hierarchy"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
