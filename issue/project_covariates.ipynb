{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cbb7291b-a7b6-4e88-849a-378cfe90c33e",
   "metadata": {},
   "source": [
    "# WHEN I\"M DONE, I want to review what else I still need to calculate\n",
    "# BASICALLY document the math for all metrics in overleaf and add to the latex wha I still need to calculate\n",
    "# align the headers for this notebook and the category names in latex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "43310309-672d-4698-bb68-a33aaa8ec6c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir('../')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "acc5551e-574d-42f3-84d9-885e864f477e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import sys\n",
    "import glob\n",
    "import warnings\n",
    "import random\n",
    "from glob import glob \n",
    "import datetime\n",
    "import itertools\n",
    "import time\n",
    "from multiprocessing import pool\n",
    "from source.lib.helpers import *\n",
    "from pandarallel import pandarallel\n",
    "import ast\n",
    "import swifter\n",
    "from functools import reduce\n",
    "import yaml\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9292248b-9696-4f71-90e4-89f0a13dbfef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: Pandarallel will run on 32 workers.\n",
      "INFO: Pandarallel will use Memory file system to transfer data between the main process and workers.\n"
     ]
    }
   ],
   "source": [
    "warnings.filterwarnings(\"ignore\")\n",
    "pd.set_option('display.max_columns', None)\n",
    "pandarallel.initialize(progress_bar = True)\n",
    "\n",
    "indir_data = Path('drive/output/derived/contributor_stats/contributor_data')\n",
    "outdir_data = Path('drive/output/derived/project_outcomes')\n",
    "\n",
    "time_period = 6#int(sys.argv[1])\n",
    "df_contributor_panel = pd.read_parquet(indir_data / f\"major_contributors_major_months{time_period}_window732D_samplefull.parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccbe2717-d609-43c1-87c0-c25ff0953a89",
   "metadata": {},
   "source": [
    "## Organizational Characteristics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e855ac7-f294-4aa1-b908-84827b88e0e2",
   "metadata": {},
   "source": [
    "**Libraries.io API**: Use the \"Repos\" query, query for \"created at\" date. \n",
    "\n",
    "<span style=\"color:red\">Not done</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9bb43c7-9351-4875-b754-f9f192717bdd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f8cd3284-42bd-41df-8303-61027a9b93d4",
   "metadata": {},
   "source": [
    "**Forks + Stars**: Obtained from GitHub Archive, calculates number of stars/forks per time-period  \n",
    "<span style=\"color:blue\">do validation on how accurate the statistics are</span>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4484b72d-ebbc-41f9-b1be-671ec128a6e0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def AggregateStarsForks(indir, time_period, colname):\n",
    "    df_agg = pd.concat(\n",
    "        (pd.read_csv(file, usecols = ['created_at','repo_name', 'org_login']) for file in indir.glob(\"*.csv\")),\n",
    "        ignore_index=True\n",
    "    )\n",
    "    df_agg = ImputeTimePeriod(df_agg, time_period)\n",
    "    df = df_agg.groupby(['repo_name','time_period']).size().reset_index()\n",
    "    df.rename(columns = {0:colname}, inplace = True)\n",
    "\n",
    "    org_list = df_agg['org_login'].dropna().unique().tolist()\n",
    "    return org_list, df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3b2c3f51-edbd-4cd6-a011-090f238c6ce5",
   "metadata": {},
   "outputs": [],
   "source": [
    "indir_watch = Path(\"drive/output/scrape/extract_github_data/watch_data\")\n",
    "indir_fork = Path(\"drive/output/scrape/extract_github_data/fork_data\")\n",
    "star_orgs, df_stars = AggregateStarsForks(indir_watch, time_period, 'stars')\n",
    "df_stars.rename(columns = {'stars':'stars_accumulated'}, inplace = True)\n",
    "fork_orgs, df_fork = AggregateStarsForks(indir_fork, time_period, 'forks')\n",
    "df_fork.rename(columns = {'forks':'forks_accumulated'}, inplace = True)\n",
    "initial_org_list = list(set(star_orgs + fork_orgs))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfb32831-0c42-4159-a28b-33252e2a3f9f",
   "metadata": {},
   "source": [
    "**Creator Type (Org/Indiv)**: Whether a project was created by an organization or an individual. I obtained this by obtaining all organization names from fork, star, issues, issue comments and PR, PR reviews and PR review comment data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "af15ba61-1899-486f-ab68-a16501e73927",
   "metadata": {},
   "outputs": [],
   "source": [
    "def GetUniqueOrgs(dir_path):\n",
    "    dfs = [pd.read_csv(f, usecols=[\"org_login\"], dtype=str) for f in dir_path.glob(\"*.csv\")]\n",
    "    if not dfs:\n",
    "        return set()\n",
    "    combined = pd.concat(dfs, ignore_index=True)\n",
    "    return combined[\"org_login\"].dropna().unique().tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "02b93eab-0069-43c8-b747-a9c9243b68dc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Have collected 2221 unique organizations\n"
     ]
    }
   ],
   "source": [
    "indir_issue_raw = Path(\"drive/output/scrape/extract_github_data/issue_data\")\n",
    "indir_issue_comment_raw = Path(\"drive/output/scrape/extract_github_data/issue_comment_data\")\n",
    "indir_pull_request_raw = Path(\"drive/output/scrape/extract_github_data/pull_request_data\")\n",
    "indir_pull_request_review_comment_raw = Path(\"drive/output/scrape/extract_github_data/pull_request_review_comment_data\")\n",
    "indir_pull_request_review_raw = Path(\"drive/output/scrape/extract_github_data/pull_request_review_data\")\n",
    "\n",
    "all_orgs = initial_org_list\n",
    "for indir in [indir_issue_raw,indir_issue_comment_raw,indir_pull_request_raw, indir_pull_request_review_comment_raw,indir_pull_request_review_raw]:\n",
    "    all_orgs += GetUniqueOrgs(indir)\n",
    "\n",
    "unique_orgs_list = list(set(all_orgs))\n",
    "print(\"Have collected {} unique organizations\".format(len(unique_orgs_list)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e23a05d5-c71b-4267-adcc-e1b259286c13",
   "metadata": {},
   "source": [
    "**Project License**: Used license obtained from PyPI as of scraping date. Two cleaning changes made:\n",
    "- Changed all licenses with GPL to \"GNU General Public License\"\n",
    "- Labelled all licenses that are not MIT/BSD/Apache/GNU as \"other\"\n",
    "\n",
    "<span style=\"color:blue\">Adjust for changes to license by projects, originally downloaded on October 27, 2023 or September 29, 2024</span>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "c7194d18-54ac-4c8f-a356-0ba0eb0287df",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ddafb68d606e4445bc6f47b59a48192f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HBox(children=(IntProgress(value=0, description='0.00%', max=275), Label(value='0 / 275'))), HB…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "indir_license = Path('output/derived/collect_github_repos')\n",
    "def ProcessLicenseData(indir_license):\n",
    "    special_licenses = ['MIT License', 'BSD License', 'Apache Software License', 'GNU General Public License']\n",
    "    df = pd.read_csv(indir_license / 'linked_pypi_github.csv', index_col=0)\n",
    "    df = df[['github repository', 'license']].query('`github repository` != \"Unavailable\"')\n",
    "    df['license'] = df['license'].parallel_apply(ast.literal_eval).apply(\n",
    "        lambda x: sorted(\n",
    "            set(\n",
    "                ['GNU General Public License' if 'GPL' in license else license if license in special_licenses else 'Other License' \n",
    "                 for license in x if license != \"Unavailable\"]\n",
    "            )\n",
    "        )\n",
    "    )\n",
    "    return df\n",
    "def AddIndicators(df_license):\n",
    "    df = df_license.rename(columns={'github repository': 'repo_name'})\n",
    "    dummies = df['license'].apply(lambda x: x if isinstance(x, list) else []).str.join('|').str.get_dummies(sep='|')\n",
    "    dummies = dummies.rename(columns=lambda x: x.replace(' ', '_'))\n",
    "    return pd.concat([df, dummies], axis=1)\n",
    "    \n",
    "df_license = ProcessLicenseData(indir_license)\n",
    "df_license_indicators = AddIndicators(df_license)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74edabe8-773c-4fb2-acf2-4085f6ffd00e",
   "metadata": {},
   "source": [
    "**Truck Factor**\n",
    "- Obtained truck factor per time-period\n",
    "- Obtained \"creation date\" based off earliest commit date (<span style=\"color:blue\">There are some nonsensical commit dates so I should filter for only dates after git/svn was created</span>)\n",
    "- Merged each contributor to their set of emails and identified whether they had a [company domain](https://github.com/liaochris/undergrad_thesis/blob/main/data/inputs/company_domain_match_list.yaml), a .edu email or another domain. Then, I found the proportion of contributors with educational, or corporate email domains in each project and time period <span style=\"color:blue\"> company + industry list </span>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9deb4d40-3110-4c9b-977f-6f90186e2ca5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def CleanTruckFactor(indir_truckfactor, time_period):\n",
    "    df_truckfactor = pd.read_csv(indir_truckfactor / 'truckfactor.csv')\n",
    "    df = df_truckfactor.dropna().copy()\n",
    "    df['repo_name'] = (\n",
    "        df['repo_name']\n",
    "        .str.replace('drive/output/scrape/get_weekly_truck_factor/truckfactor_', '')\n",
    "        .str.replace('.csv', '')\n",
    "        .str.replace('_', '/', 1)\n",
    "    )\n",
    "    df = df.rename(columns={'date': 'created_at'})\n",
    "    df = ImputeTimePeriod(df, time_period)\n",
    "    \n",
    "    df_truckfactor_period = (\n",
    "        df\n",
    "        .sort_values('created_at')\n",
    "        .groupby(['time_period', 'repo_name'], as_index=False)\n",
    "        .tail(1)\n",
    "        .sort_values(['repo_name', 'time_period'])\n",
    "    )\n",
    "    df_truckfactor_period['authors'] = df_truckfactor_period['authors'].str.split(r'\\s*\\|\\s*')\n",
    "    df_truckfactor_period = df_truckfactor_period[['repo_name', 'time_period', 'created_at', 'week', 'year', 'truckfactor', 'authors']]\n",
    "\n",
    "    return df_truckfactor_period\n",
    "\n",
    "\n",
    "def AggregateCommitters(indir_committers):\n",
    "    df_committers = pd.read_csv(indir_committers / 'committers_info.csv', index_col=0)\n",
    "    df_clean = df_committers[['repo_name', 'commit_name', 'email_address', 'actor_id']].copy()\n",
    "    df_clean[['commit_name', 'email_address']] = df_clean[['commit_name', 'email_address']].applymap(ast.literal_eval)\n",
    "    \n",
    "    df_grouped = (\n",
    "        df_clean\n",
    "        .explode('commit_name')\n",
    "        .groupby(['commit_name', 'actor_id'])['repo_name']\n",
    "        .apply(list)\n",
    "        .reset_index()\n",
    "    )\n",
    "    \n",
    "    actor_emails = (\n",
    "        df_clean\n",
    "        .explode('email_address')[['email_address', 'actor_id']]\n",
    "        .drop_duplicates()\n",
    "        .dropna()\n",
    "        .groupby('actor_id')['email_address']\n",
    "        .apply(list)\n",
    "        .reset_index()\n",
    "    )\n",
    "    \n",
    "    return df_grouped, actor_emails, df_clean\n",
    "\n",
    "def MergeTruckFactorEmail(df_truckfactor_period, df_committers_grouped, actor_email_addresses):\n",
    "    df = df_truckfactor_period.explode('authors')\n",
    "    df = pd.merge(\n",
    "        df,\n",
    "        df_committers_grouped.rename(columns={'repo_name': 'repo'}),\n",
    "        how='left',\n",
    "        left_on='authors',\n",
    "        right_on='commit_name'\n",
    "    )\n",
    "    \n",
    "    group_cols = ['repo_name', 'time_period', 'authors']\n",
    "    df['merge_count'] = df.groupby(group_cols)['truckfactor'].transform('count')\n",
    "    df['repo_equal'] = (\n",
    "        df['repo'].apply(lambda x: isinstance(x, list)) &\n",
    "        df.apply(lambda x: x['repo_name'] in x['repo'] if isinstance(x['repo'], list) else False, axis=1)\n",
    "    ).astype(int)\n",
    "    df['merge_repo_match'] = df.groupby(group_cols)['repo_equal'].transform('sum')\n",
    "    \n",
    "    condition_na = (df['merge_count'] > 1) & ~((df['merge_repo_match'] == 1) & (df['repo_equal'] == 1))\n",
    "    df_na = df[condition_na].drop(['merge_count', 'repo_equal', 'merge_repo_match', 'repo'], axis=1)\n",
    "    df_na['commit_name'] = np.nan\n",
    "    \n",
    "    condition_valid = (df['merge_count'] == 1) | ((df['merge_count'] > 1) & (df['merge_repo_match'] == 1) & (df['repo_equal'] == 1))\n",
    "    df_valid = df[condition_valid].drop(['merge_count', 'repo_equal', 'merge_repo_match', 'repo'], axis=1)\n",
    "    \n",
    "    df_agg = pd.concat([df_valid, df_na.drop_duplicates()], ignore_index=True)\n",
    "    df_agg = df_agg.sort_values('commit_name').drop_duplicates(['repo_name', 'time_period', 'authors']).sort_values(['repo_name', 'time_period', 'authors'])\n",
    "    df_agg = pd.merge(df_agg, actor_email_addresses, how='left')\n",
    "    \n",
    "    return df_agg\n",
    "\n",
    "def AddCompanyAndIndustryColumns(df_truckfactor_agg):\n",
    "    with open('issue/company_domain_match_list.yaml', 'r') as f:\n",
    "        domains_list = yaml.load(f, Loader=yaml.FullLoader)\n",
    "    \n",
    "    domain_to_company = {}\n",
    "    domain_to_industry = {}\n",
    "    regex_mappings = []\n",
    "    \n",
    "    for entry in domains_list:\n",
    "        company = entry.get('company')\n",
    "        industry = entry.get('industry')\n",
    "        for domain in entry.get('domains', []):\n",
    "            domain_to_company[domain.lower()] = company\n",
    "            domain_to_industry[domain.lower()] = industry\n",
    "\n",
    "    \n",
    "    df_truckfactor_agg['domain'] = df_truckfactor_agg['email_address'].apply(\n",
    "        lambda x: [re.search(r'@([^@]+)$', email).group(1) for email in x  if isinstance(email, str) and re.search(r'@([^@]+)$', email)]\n",
    "        if isinstance(x, list) else np.nan)\n",
    "    df_truckfactor_agg['company'] = df_truckfactor_agg['domain'].apply(lambda x: [domain_to_company.get(email, np.nan) for email in x]\n",
    "                                       if isinstance(x, list) else np.nan)\n",
    "    df_truckfactor_agg['industry'] = df_truckfactor_agg['domain'].apply(lambda x: [domain_to_industry.get(email, np.nan) for email in x]\n",
    "                                       if isinstance(x, list) else np.nan)\n",
    "    \n",
    "    for col in ['company','industry']:\n",
    "        df_truckfactor_agg[col] = df_truckfactor_agg[col].apply(lambda x: sorted(set([email for email in x if not pd.isnull(email)])) \n",
    "                                                                if isinstance(x, list) else x)\n",
    "        df_truckfactor_agg[col] = df_truckfactor_agg[col].apply(lambda x: x if isinstance(x, list) and len(x)>0 else np.nan)\n",
    "        \n",
    "    return df_truckfactor_agg\n",
    "\n",
    "def AddEducationalColumns(df_truckfactor_agg):\n",
    "    df_truckfactor_agg['educational'] = df_truckfactor_agg['email_address'].apply(\n",
    "        lambda x: any([email.endswith(\".edu\") for email in x]) if isinstance(x, list) else False)\n",
    "    return df_truckfactor_agg\n",
    "\n",
    "def UniqueNonNull(s):\n",
    "    return s.dropna().unique()\n",
    "\n",
    "indir_truckfactor = Path('drive/output/scrape/get_weekly_truck_factor')\n",
    "df_truckfactor_period = CleanTruckFactor(indir_truckfactor, time_period)\n",
    "\n",
    "indir_committers = Path('drive/output/scrape/link_committers_profile')\n",
    "df_committers_grouped, actor_email_addresses, df_committers_clean = AggregateCommitters(indir_committers)\n",
    "\n",
    "df_truckfactor_agg = MergeTruckFactorEmail(df_truckfactor_period, df_committers_grouped, actor_email_addresses)\n",
    "df_truckfactor_agg = AddCompanyAndIndustryColumns(df_truckfactor_agg)\n",
    "df_truckfactor_agg = AddEducationalColumns(df_truckfactor_agg)\n",
    "df_truckfactor_repo_summary = df_truckfactor_agg.assign(company_bool = 1-df_truckfactor_agg['company'].isna()).groupby(['repo_name','time_period','truckfactor']).agg(\n",
    "    corporate_pct = ('company_bool','mean'),\n",
    "    educational_pct = ('educational','mean')).reset_index()\n",
    "\n",
    "# filter for after git was created? or after svn was created?\n",
    "df_creation_date = df_truckfactor_period[['repo_name','created_at']].assign(\n",
    "    created_year = df_truckfactor_period['created_at'].dt.year)\\\n",
    "    .query('created_year>1991')\\\n",
    "    .sort_values('created_at').drop_duplicates('repo_name')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c33a5acf-ee52-4195-aec4-26b0f0b6c5c0",
   "metadata": {},
   "source": [
    "**Organizational size**: \\# of people making contributions, defined as opening/closing an issue, commenting on an issue, PR, reviewing a PR, merging a PR, or committing code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b9a56d15-004c-43cc-96a8-b12ba0f3e9eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_contributor_count = df_contributor_panel.groupby(['repo_name','time_period'])['actor_id'].count().reset_index().rename(\n",
    "    {'actor_id':'contributors'}, axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef44f356-82ae-4fef-b950-bbcbd63b65a9",
   "metadata": {},
   "source": [
    "### Combine Covariates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "id": "26cc1184-b39d-460b-9c1f-1cea23bad89b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def MakeBalanced(df_repo_panel):\n",
    "    df_repo_panel['first_period'] = df_repo_panel.groupby('repo_name')['time_period'].transform('min')\n",
    "    df_repo_panel['final_period'] = df_repo_panel.groupby('repo_name')['time_period'].transform('max')\n",
    "    time_periods = df_repo_panel['time_period'].unique().tolist()\n",
    "    df_balanced = df_repo_panel[['repo_name']].drop_duplicates()\n",
    "    df_balanced['time_period'] = [time_periods for i in range(df_balanced.shape[0])]\n",
    "    df_balanced = df_balanced.explode('time_period')\n",
    "    df_repo_panel_full = pd.merge(df_balanced, df_repo_panel, how = 'left')\n",
    "    df_repo_panel_full[['first_period','final_period']] = df_repo_panel_full.groupby(['repo_name'])[['first_period','final_period']].ffill()\n",
    "    df_repo_panel_full = df_repo_panel_full.query('time_period >= first_period & time_period <= final_period')\n",
    "\n",
    "    return df_repo_panel_full.drop(['first_period','final_period'], axis = 1).sort_values(['repo_name','time_period'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "id": "52eb669c-94cc-4f29-8641-4037a32ca47f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_repo_characteristics = reduce(lambda left, right: left.merge(right, how='outer'),\n",
    "    [\n",
    "        df_stars,\n",
    "        df_fork,\n",
    "        df_license_indicators.assign(\n",
    "            license=df_license_indicators['license'].apply(lambda x: \"|\".join(x) if isinstance(x, list) and x else np.nan)\n",
    "        ),\n",
    "        df_creation_date,\n",
    "        df_truckfactor_repo_summary,\n",
    "        df_contributor_count\n",
    "    ]\n",
    ")\n",
    "\n",
    "df_repo_characteristics = MakeBalanced(df_repo_characteristics)\n",
    "df_repo_characteristics.columns = df_repo_characteristics.columns.str.lower()\n",
    "\n",
    "na_zero_cols = ['stars_accumulated', 'forks_accumulated', 'contributors']\n",
    "df_repo_characteristics[na_zero_cols] = df_repo_characteristics[na_zero_cols].fillna(0)\n",
    "\n",
    "fill_cols = [\n",
    "    'license', 'apache_software_license', 'bsd_license',\n",
    "    'gnu_general_public_license', 'mit_license', 'other_license',\n",
    "    'created_at', 'created_year'\n",
    "]\n",
    "df_repo_characteristics[fill_cols] = df_repo_characteristics.groupby('repo_name')[fill_cols].transform(lambda x: x.ffill().bfill())\n",
    "\n",
    "truck_cols = ['truckfactor', 'corporate_pct', 'educational_pct']\n",
    "df_repo_characteristics[truck_cols] = df_repo_characteristics.groupby('repo_name')[truck_cols].transform('ffill')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26ecd9c8-aee3-4f39-99f6-24b70f3546a3",
   "metadata": {},
   "source": [
    "## Individual Characteristics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "666fbbe6-bc8e-457a-9343-37b4dc167bd3",
   "metadata": {},
   "source": [
    "**Email domain**: Whether in a project-contributor-time period, an individual made a commit with an educational or corporate email domain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "id": "cfb3b75e-e92f-4a5d-81fc-345b2b70cb6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_contributor_email_category = df_truckfactor_agg.assign(company_bool = ~df_truckfactor_agg['company'].isna())\\\n",
    "    .groupby(['repo_name','actor_id','time_period'])\\\n",
    "    .agg(contributor_email_educational = ('educational','max'), \n",
    "         contributor_email_corporate = ('company_bool','max')).reset_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa5c026e-23f9-4c82-9bdd-69e401b909b3",
   "metadata": {},
   "source": [
    "1) Max Rank for each individual in each period as a continuous rank variable\n",
    "2) Rank indicator\n",
    "3) % for each rank that they handled\n",
    "\n",
    "df_contributor_panel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "id": "b383dc67-2719-4204-81f7-5e4f2c0a1b1c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def ContributorRankCharacteristics(df_contributor_panel):\n",
    "    val_cols = [col for col in df_contributor_panel.columns if 'pct' in col]\n",
    "    df = df_contributor_panel.drop(columns=val_cols).rename(columns={'issue_number': 'issues_opened', 'pr': 'prs_opened'}).reset_index(drop=True)\n",
    "    \n",
    "    df['problem_identification_sum'] = df['issues_opened']\n",
    "    df['problem_identification'] = df['problem_identification_sum'] > 0\n",
    "    \n",
    "    df['problem_discussion_sum'] = df[['comments', 'own_pr_comments', 'helping_pr_comments', 'pr_reviews', 'pr_review_comments']].sum(axis=1)\n",
    "    df['problem_discussion'] = df['problem_discussion_sum'] > 0\n",
    "    \n",
    "    df['coding_sum'] = df[['prs_opened', 'commits']].sum(axis=1)\n",
    "    df['coding'] = df['coding_sum'] > 0\n",
    "    \n",
    "    df['problem_approval_sum'] = df[['prs_merged', 'issues_closed']].sum(axis=1)\n",
    "    df['problem_approval'] = df['problem_approval_sum'] > 0\n",
    "    \n",
    "    df['max_rank'] = np.select(\n",
    "        [\n",
    "            df['problem_approval'],\n",
    "            df['coding'],\n",
    "            df['problem_discussion']\n",
    "        ],\n",
    "        [4, 3, 2],\n",
    "        default=1\n",
    "    )\n",
    "    agg_cols = ['problem_identification', 'problem_discussion', 'coding', 'problem_approval']\n",
    "    grouped_sums = df.groupby(['repo_name', 'time_period'])[agg_cols].transform('sum')\n",
    "    for col in agg_cols:\n",
    "        df[f'{col}_share'] = df[col] / grouped_sums[col]\n",
    "    rank_activity_cols = [\n",
    "        'issues_opened', 'problem_identification_sum', 'comments', 'own_pr_comments',\n",
    "        'helping_pr_comments', 'pr_reviews', 'pr_review_comments', 'problem_discussion_sum',\n",
    "        'prs_opened', 'commits', 'coding_sum', 'prs_merged', 'issues_closed',\n",
    "        'problem_approval_sum'\n",
    "    ]\n",
    "    final_cols = ['actor_id', 'repo_name', 'time_period', 'max_rank'] + agg_cols + [f'{c}_share' for c in agg_cols] + rank_activity_cols\n",
    "    return df[final_cols]\n",
    "\n",
    "df_contributor_panel_rank = ContributorRankCharacteristics(df_contributor_panel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "id": "05f88fe5-4276-4c88-aec0-8364a71814b3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>actor_id</th>\n",
       "      <th>repo_name</th>\n",
       "      <th>time_period</th>\n",
       "      <th>max_rank</th>\n",
       "      <th>problem_identification_share</th>\n",
       "      <th>problem_discussion_share</th>\n",
       "      <th>coding_share</th>\n",
       "      <th>problem_approval_share</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>376272.0</td>\n",
       "      <td>1e0ng/simhash</td>\n",
       "      <td>2020-07-01</td>\n",
       "      <td>3</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1264873.0</td>\n",
       "      <td>1e0ng/simhash</td>\n",
       "      <td>2020-07-01</td>\n",
       "      <td>4</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.5</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1264873.0</td>\n",
       "      <td>1e0ng/simhash</td>\n",
       "      <td>2021-01-01</td>\n",
       "      <td>2</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1264873.0</td>\n",
       "      <td>1e0ng/simhash</td>\n",
       "      <td>2021-07-01</td>\n",
       "      <td>4</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.333333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1264873.0</td>\n",
       "      <td>1e0ng/simhash</td>\n",
       "      <td>2022-01-01</td>\n",
       "      <td>4</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.5</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1554030</th>\n",
       "      <td>443794.0</td>\n",
       "      <td>zzzsochi/trans</td>\n",
       "      <td>2016-01-01</td>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1554031</th>\n",
       "      <td>443794.0</td>\n",
       "      <td>zzzsochi/trans</td>\n",
       "      <td>2016-07-01</td>\n",
       "      <td>4</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.5</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1554032</th>\n",
       "      <td>2354108.0</td>\n",
       "      <td>zzzsochi/trans</td>\n",
       "      <td>2016-07-01</td>\n",
       "      <td>2</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1554033</th>\n",
       "      <td>4735252.0</td>\n",
       "      <td>zzzsochi/trans</td>\n",
       "      <td>2015-07-01</td>\n",
       "      <td>4</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1554034</th>\n",
       "      <td>5304276.0</td>\n",
       "      <td>zzzsochi/trans</td>\n",
       "      <td>2016-07-01</td>\n",
       "      <td>3</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1554035 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          actor_id       repo_name time_period  max_rank  \\\n",
       "0         376272.0   1e0ng/simhash  2020-07-01         3   \n",
       "1        1264873.0   1e0ng/simhash  2020-07-01         4   \n",
       "2        1264873.0   1e0ng/simhash  2021-01-01         2   \n",
       "3        1264873.0   1e0ng/simhash  2021-07-01         4   \n",
       "4        1264873.0   1e0ng/simhash  2022-01-01         4   \n",
       "...            ...             ...         ...       ...   \n",
       "1554030   443794.0  zzzsochi/trans  2016-01-01         3   \n",
       "1554031   443794.0  zzzsochi/trans  2016-07-01         4   \n",
       "1554032  2354108.0  zzzsochi/trans  2016-07-01         2   \n",
       "1554033  4735252.0  zzzsochi/trans  2015-07-01         4   \n",
       "1554034  5304276.0  zzzsochi/trans  2016-07-01         3   \n",
       "\n",
       "         problem_identification_share  problem_discussion_share  coding_share  \\\n",
       "0                            0.500000                  0.500000           0.5   \n",
       "1                            0.500000                  0.500000           0.5   \n",
       "2                            0.333333                  0.333333           NaN   \n",
       "3                            0.142857                  0.142857           1.0   \n",
       "4                            0.166667                  0.166667           0.5   \n",
       "...                               ...                       ...           ...   \n",
       "1554030                           NaN                       NaN           1.0   \n",
       "1554031                      0.333333                  0.333333           0.5   \n",
       "1554032                      0.333333                  0.333333           0.0   \n",
       "1554033                      0.000000                  0.000000           0.0   \n",
       "1554034                      0.333333                  0.333333           0.5   \n",
       "\n",
       "         problem_approval_share  \n",
       "0                      0.000000  \n",
       "1                      1.000000  \n",
       "2                      0.000000  \n",
       "3                      0.333333  \n",
       "4                      1.000000  \n",
       "...                         ...  \n",
       "1554030                     NaN  \n",
       "1554031                1.000000  \n",
       "1554032                0.000000  \n",
       "1554033                0.500000  \n",
       "1554034                0.000000  \n",
       "\n",
       "[1554035 rows x 8 columns]"
      ]
     },
     "execution_count": 202,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agg_cols = ['problem_identification', 'problem_discussion', 'coding', 'problem_approval']\n",
    "df_contributor_panel_rank[['actor_id','repo_name','time_period','max_rank'] + [f'{c}_share' for c in agg_cols]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff8ab5c1-ab0b-4e71-8dea-21c0cbe54854",
   "metadata": {},
   "source": [
    "Part of truck factor, indicator\n",
    "- drive/output/scrape/get_weekly_truck_factor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "id": "d6591658-b16f-4007-8a18-efcd3cc15499",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_truckfactor_dates = df_truckfactor_agg[['repo_name','time_period','actor_id']].assign(\n",
    "    truckfactor_member = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "id": "4eb1952c-db30-42c6-a56d-17eac49f8d0e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>type</th>\n",
       "      <th>created_at</th>\n",
       "      <th>repo_id</th>\n",
       "      <th>repo_name</th>\n",
       "      <th>actor_id</th>\n",
       "      <th>actor_login</th>\n",
       "      <th>pr_title</th>\n",
       "      <th>pr_number</th>\n",
       "      <th>pr_body</th>\n",
       "      <th>pr_action</th>\n",
       "      <th>pr_merged_by_id</th>\n",
       "      <th>pr_merged_by_type</th>\n",
       "      <th>pr_label</th>\n",
       "      <th>pr_review_action</th>\n",
       "      <th>pr_review_id</th>\n",
       "      <th>pr_review_body</th>\n",
       "      <th>pr_review_state</th>\n",
       "      <th>pr_review_comment_body</th>\n",
       "      <th>date</th>\n",
       "      <th>year</th>\n",
       "      <th>period</th>\n",
       "      <th>time_period</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>15463</th>\n",
       "      <td>PullRequestEvent</td>\n",
       "      <td>2020-10-16 15:25:56+00:00</td>\n",
       "      <td>12833146.0</td>\n",
       "      <td>1e0ng/simhash</td>\n",
       "      <td>1264873.0</td>\n",
       "      <td>1e0ng</td>\n",
       "      <td>None</td>\n",
       "      <td>50.0</td>\n",
       "      <td>None</td>\n",
       "      <td>closed</td>\n",
       "      <td>1264873.0</td>\n",
       "      <td>User</td>\n",
       "      <td>[]</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>2020-10</td>\n",
       "      <td>2020</td>\n",
       "      <td>1</td>\n",
       "      <td>2020-07-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40847</th>\n",
       "      <td>PullRequestEvent</td>\n",
       "      <td>2020-10-06 21:09:52+00:00</td>\n",
       "      <td>12833146.0</td>\n",
       "      <td>1e0ng/simhash</td>\n",
       "      <td>376272.0</td>\n",
       "      <td>jcushman</td>\n",
       "      <td>None</td>\n",
       "      <td>50.0</td>\n",
       "      <td>None</td>\n",
       "      <td>opened</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>[]</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>2020-10</td>\n",
       "      <td>2020</td>\n",
       "      <td>1</td>\n",
       "      <td>2020-07-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12040</th>\n",
       "      <td>PullRequestReviewEvent</td>\n",
       "      <td>2020-10-09 13:09:15+00:00</td>\n",
       "      <td>12833146.0</td>\n",
       "      <td>1e0ng/simhash</td>\n",
       "      <td>1264873.0</td>\n",
       "      <td>1e0ng</td>\n",
       "      <td>Use numpy to calculate simhash</td>\n",
       "      <td>50.0</td>\n",
       "      <td>Per #49, here is code to use numpy for calcula...</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>created</td>\n",
       "      <td>505652288.0</td>\n",
       "      <td>None</td>\n",
       "      <td>commented</td>\n",
       "      <td>None</td>\n",
       "      <td>2020-10</td>\n",
       "      <td>2020</td>\n",
       "      <td>1</td>\n",
       "      <td>2020-07-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12559</th>\n",
       "      <td>PullRequestReviewEvent</td>\n",
       "      <td>2020-10-09 19:59:56+00:00</td>\n",
       "      <td>12833146.0</td>\n",
       "      <td>1e0ng/simhash</td>\n",
       "      <td>376272.0</td>\n",
       "      <td>jcushman</td>\n",
       "      <td>Use numpy to calculate simhash</td>\n",
       "      <td>50.0</td>\n",
       "      <td>Per #49, here is code to use numpy for calcula...</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>created</td>\n",
       "      <td>505955117.0</td>\n",
       "      <td>None</td>\n",
       "      <td>commented</td>\n",
       "      <td>None</td>\n",
       "      <td>2020-10</td>\n",
       "      <td>2020</td>\n",
       "      <td>1</td>\n",
       "      <td>2020-07-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17133</th>\n",
       "      <td>PullRequestReviewEvent</td>\n",
       "      <td>2020-10-16 15:25:42+00:00</td>\n",
       "      <td>12833146.0</td>\n",
       "      <td>1e0ng/simhash</td>\n",
       "      <td>1264873.0</td>\n",
       "      <td>1e0ng</td>\n",
       "      <td>Use numpy to calculate simhash</td>\n",
       "      <td>50.0</td>\n",
       "      <td>Per #49, here is code to use numpy for calcula...</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>created</td>\n",
       "      <td>510575371.0</td>\n",
       "      <td>None</td>\n",
       "      <td>approved</td>\n",
       "      <td>None</td>\n",
       "      <td>2020-10</td>\n",
       "      <td>2020</td>\n",
       "      <td>1</td>\n",
       "      <td>2020-07-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24597</th>\n",
       "      <td>PullRequestReviewEvent</td>\n",
       "      <td>2020-10-09 20:06:32+00:00</td>\n",
       "      <td>12833146.0</td>\n",
       "      <td>1e0ng/simhash</td>\n",
       "      <td>376272.0</td>\n",
       "      <td>jcushman</td>\n",
       "      <td>Use numpy to calculate simhash</td>\n",
       "      <td>50.0</td>\n",
       "      <td>Per #49, here is code to use numpy for calcula...</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>created</td>\n",
       "      <td>505958608.0</td>\n",
       "      <td>None</td>\n",
       "      <td>commented</td>\n",
       "      <td>None</td>\n",
       "      <td>2020-10</td>\n",
       "      <td>2020</td>\n",
       "      <td>1</td>\n",
       "      <td>2020-07-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3385</th>\n",
       "      <td>PullRequestReviewCommentEvent</td>\n",
       "      <td>2020-10-09 13:05:14+00:00</td>\n",
       "      <td>12833146.0</td>\n",
       "      <td>1e0ng/simhash</td>\n",
       "      <td>1264873.0</td>\n",
       "      <td>1e0ng</td>\n",
       "      <td>Use numpy to calculate simhash</td>\n",
       "      <td>50.0</td>\n",
       "      <td>Per #49, here is code to use numpy for calcula...</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>👍</td>\n",
       "      <td>2020-10</td>\n",
       "      <td>2020</td>\n",
       "      <td>1</td>\n",
       "      <td>2020-07-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6489</th>\n",
       "      <td>PullRequestReviewCommentEvent</td>\n",
       "      <td>2020-10-09 19:59:55+00:00</td>\n",
       "      <td>12833146.0</td>\n",
       "      <td>1e0ng/simhash</td>\n",
       "      <td>376272.0</td>\n",
       "      <td>jcushman</td>\n",
       "      <td>Use numpy to calculate simhash</td>\n",
       "      <td>50.0</td>\n",
       "      <td>Per #49, here is code to use numpy for calcula...</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>I selected both of those values based on where...</td>\n",
       "      <td>2020-10</td>\n",
       "      <td>2020</td>\n",
       "      <td>1</td>\n",
       "      <td>2020-07-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7889</th>\n",
       "      <td>PullRequestReviewCommentEvent</td>\n",
       "      <td>2020-10-09 13:04:31+00:00</td>\n",
       "      <td>12833146.0</td>\n",
       "      <td>1e0ng/simhash</td>\n",
       "      <td>1264873.0</td>\n",
       "      <td>1e0ng</td>\n",
       "      <td>Use numpy to calculate simhash</td>\n",
       "      <td>50.0</td>\n",
       "      <td>Per #49, here is code to use numpy for calcula...</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>Could you elaborate on how 50 is calculated? A...</td>\n",
       "      <td>2020-10</td>\n",
       "      <td>2020</td>\n",
       "      <td>1</td>\n",
       "      <td>2020-07-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8885</th>\n",
       "      <td>PullRequestReviewCommentEvent</td>\n",
       "      <td>2020-10-09 13:09:01+00:00</td>\n",
       "      <td>12833146.0</td>\n",
       "      <td>1e0ng/simhash</td>\n",
       "      <td>1264873.0</td>\n",
       "      <td>1e0ng</td>\n",
       "      <td>Use numpy to calculate simhash</td>\n",
       "      <td>50.0</td>\n",
       "      <td>Per #49, here is code to use numpy for calcula...</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>👍</td>\n",
       "      <td>2020-10</td>\n",
       "      <td>2020</td>\n",
       "      <td>1</td>\n",
       "      <td>2020-07-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12574</th>\n",
       "      <td>PullRequestReviewCommentEvent</td>\n",
       "      <td>2020-10-09 13:04:52+00:00</td>\n",
       "      <td>12833146.0</td>\n",
       "      <td>1e0ng/simhash</td>\n",
       "      <td>1264873.0</td>\n",
       "      <td>1e0ng</td>\n",
       "      <td>Use numpy to calculate simhash</td>\n",
       "      <td>50.0</td>\n",
       "      <td>Per #49, here is code to use numpy for calcula...</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>👍</td>\n",
       "      <td>2020-10</td>\n",
       "      <td>2020</td>\n",
       "      <td>1</td>\n",
       "      <td>2020-07-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14419</th>\n",
       "      <td>PullRequestReviewCommentEvent</td>\n",
       "      <td>2020-10-09 20:06:31+00:00</td>\n",
       "      <td>12833146.0</td>\n",
       "      <td>1e0ng/simhash</td>\n",
       "      <td>376272.0</td>\n",
       "      <td>jcushman</td>\n",
       "      <td>Use numpy to calculate simhash</td>\n",
       "      <td>50.0</td>\n",
       "      <td>Per #49, here is code to use numpy for calcula...</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>Yeah for sure. I think that could be supported...</td>\n",
       "      <td>2020-10</td>\n",
       "      <td>2020</td>\n",
       "      <td>1</td>\n",
       "      <td>2020-07-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14533</th>\n",
       "      <td>PullRequestReviewCommentEvent</td>\n",
       "      <td>2020-10-09 13:05:40+00:00</td>\n",
       "      <td>12833146.0</td>\n",
       "      <td>1e0ng/simhash</td>\n",
       "      <td>1264873.0</td>\n",
       "      <td>1e0ng</td>\n",
       "      <td>Use numpy to calculate simhash</td>\n",
       "      <td>50.0</td>\n",
       "      <td>Per #49, here is code to use numpy for calcula...</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>Could you elaborate on how 200 is calculated?</td>\n",
       "      <td>2020-10</td>\n",
       "      <td>2020</td>\n",
       "      <td>1</td>\n",
       "      <td>2020-07-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16741</th>\n",
       "      <td>PullRequestReviewCommentEvent</td>\n",
       "      <td>2020-10-09 13:08:27+00:00</td>\n",
       "      <td>12833146.0</td>\n",
       "      <td>1e0ng/simhash</td>\n",
       "      <td>1264873.0</td>\n",
       "      <td>1e0ng</td>\n",
       "      <td>Use numpy to calculate simhash</td>\n",
       "      <td>50.0</td>\n",
       "      <td>Per #49, here is code to use numpy for calcula...</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>This adds a limitation here. I just hope no on...</td>\n",
       "      <td>2020-10</td>\n",
       "      <td>2020</td>\n",
       "      <td>1</td>\n",
       "      <td>2020-07-01</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                type                created_at     repo_id  \\\n",
       "15463               PullRequestEvent 2020-10-16 15:25:56+00:00  12833146.0   \n",
       "40847               PullRequestEvent 2020-10-06 21:09:52+00:00  12833146.0   \n",
       "12040         PullRequestReviewEvent 2020-10-09 13:09:15+00:00  12833146.0   \n",
       "12559         PullRequestReviewEvent 2020-10-09 19:59:56+00:00  12833146.0   \n",
       "17133         PullRequestReviewEvent 2020-10-16 15:25:42+00:00  12833146.0   \n",
       "24597         PullRequestReviewEvent 2020-10-09 20:06:32+00:00  12833146.0   \n",
       "3385   PullRequestReviewCommentEvent 2020-10-09 13:05:14+00:00  12833146.0   \n",
       "6489   PullRequestReviewCommentEvent 2020-10-09 19:59:55+00:00  12833146.0   \n",
       "7889   PullRequestReviewCommentEvent 2020-10-09 13:04:31+00:00  12833146.0   \n",
       "8885   PullRequestReviewCommentEvent 2020-10-09 13:09:01+00:00  12833146.0   \n",
       "12574  PullRequestReviewCommentEvent 2020-10-09 13:04:52+00:00  12833146.0   \n",
       "14419  PullRequestReviewCommentEvent 2020-10-09 20:06:31+00:00  12833146.0   \n",
       "14533  PullRequestReviewCommentEvent 2020-10-09 13:05:40+00:00  12833146.0   \n",
       "16741  PullRequestReviewCommentEvent 2020-10-09 13:08:27+00:00  12833146.0   \n",
       "\n",
       "           repo_name   actor_id actor_login                        pr_title  \\\n",
       "15463  1e0ng/simhash  1264873.0       1e0ng                            None   \n",
       "40847  1e0ng/simhash   376272.0    jcushman                            None   \n",
       "12040  1e0ng/simhash  1264873.0       1e0ng  Use numpy to calculate simhash   \n",
       "12559  1e0ng/simhash   376272.0    jcushman  Use numpy to calculate simhash   \n",
       "17133  1e0ng/simhash  1264873.0       1e0ng  Use numpy to calculate simhash   \n",
       "24597  1e0ng/simhash   376272.0    jcushman  Use numpy to calculate simhash   \n",
       "3385   1e0ng/simhash  1264873.0       1e0ng  Use numpy to calculate simhash   \n",
       "6489   1e0ng/simhash   376272.0    jcushman  Use numpy to calculate simhash   \n",
       "7889   1e0ng/simhash  1264873.0       1e0ng  Use numpy to calculate simhash   \n",
       "8885   1e0ng/simhash  1264873.0       1e0ng  Use numpy to calculate simhash   \n",
       "12574  1e0ng/simhash  1264873.0       1e0ng  Use numpy to calculate simhash   \n",
       "14419  1e0ng/simhash   376272.0    jcushman  Use numpy to calculate simhash   \n",
       "14533  1e0ng/simhash  1264873.0       1e0ng  Use numpy to calculate simhash   \n",
       "16741  1e0ng/simhash  1264873.0       1e0ng  Use numpy to calculate simhash   \n",
       "\n",
       "       pr_number                                            pr_body pr_action  \\\n",
       "15463       50.0                                               None    closed   \n",
       "40847       50.0                                               None    opened   \n",
       "12040       50.0  Per #49, here is code to use numpy for calcula...      None   \n",
       "12559       50.0  Per #49, here is code to use numpy for calcula...      None   \n",
       "17133       50.0  Per #49, here is code to use numpy for calcula...      None   \n",
       "24597       50.0  Per #49, here is code to use numpy for calcula...      None   \n",
       "3385        50.0  Per #49, here is code to use numpy for calcula...      None   \n",
       "6489        50.0  Per #49, here is code to use numpy for calcula...      None   \n",
       "7889        50.0  Per #49, here is code to use numpy for calcula...      None   \n",
       "8885        50.0  Per #49, here is code to use numpy for calcula...      None   \n",
       "12574       50.0  Per #49, here is code to use numpy for calcula...      None   \n",
       "14419       50.0  Per #49, here is code to use numpy for calcula...      None   \n",
       "14533       50.0  Per #49, here is code to use numpy for calcula...      None   \n",
       "16741       50.0  Per #49, here is code to use numpy for calcula...      None   \n",
       "\n",
       "       pr_merged_by_id pr_merged_by_type pr_label pr_review_action  \\\n",
       "15463        1264873.0              User       []             None   \n",
       "40847              NaN              None       []             None   \n",
       "12040              NaN              None     None          created   \n",
       "12559              NaN              None     None          created   \n",
       "17133              NaN              None     None          created   \n",
       "24597              NaN              None     None          created   \n",
       "3385               NaN              None     None             None   \n",
       "6489               NaN              None     None             None   \n",
       "7889               NaN              None     None             None   \n",
       "8885               NaN              None     None             None   \n",
       "12574              NaN              None     None             None   \n",
       "14419              NaN              None     None             None   \n",
       "14533              NaN              None     None             None   \n",
       "16741              NaN              None     None             None   \n",
       "\n",
       "       pr_review_id pr_review_body pr_review_state  \\\n",
       "15463           NaN           None            None   \n",
       "40847           NaN           None            None   \n",
       "12040   505652288.0           None       commented   \n",
       "12559   505955117.0           None       commented   \n",
       "17133   510575371.0           None        approved   \n",
       "24597   505958608.0           None       commented   \n",
       "3385            NaN           None            None   \n",
       "6489            NaN           None            None   \n",
       "7889            NaN           None            None   \n",
       "8885            NaN           None            None   \n",
       "12574           NaN           None            None   \n",
       "14419           NaN           None            None   \n",
       "14533           NaN           None            None   \n",
       "16741           NaN           None            None   \n",
       "\n",
       "                                  pr_review_comment_body     date  year  \\\n",
       "15463                                               None  2020-10  2020   \n",
       "40847                                               None  2020-10  2020   \n",
       "12040                                               None  2020-10  2020   \n",
       "12559                                               None  2020-10  2020   \n",
       "17133                                               None  2020-10  2020   \n",
       "24597                                               None  2020-10  2020   \n",
       "3385                                                  👍   2020-10  2020   \n",
       "6489   I selected both of those values based on where...  2020-10  2020   \n",
       "7889   Could you elaborate on how 50 is calculated? A...  2020-10  2020   \n",
       "8885                                                  👍   2020-10  2020   \n",
       "12574                                                 👍   2020-10  2020   \n",
       "14419  Yeah for sure. I think that could be supported...  2020-10  2020   \n",
       "14533      Could you elaborate on how 200 is calculated?  2020-10  2020   \n",
       "16741  This adds a limitation here. I just hope no on...  2020-10  2020   \n",
       "\n",
       "       period time_period  \n",
       "15463       1  2020-07-01  \n",
       "40847       1  2020-07-01  \n",
       "12040       1  2020-07-01  \n",
       "12559       1  2020-07-01  \n",
       "17133       1  2020-07-01  \n",
       "24597       1  2020-07-01  \n",
       "3385        1  2020-07-01  \n",
       "6489        1  2020-07-01  \n",
       "7889        1  2020-07-01  \n",
       "8885        1  2020-07-01  \n",
       "12574       1  2020-07-01  \n",
       "14419       1  2020-07-01  \n",
       "14533       1  2020-07-01  \n",
       "16741       1  2020-07-01  "
      ]
     },
     "execution_count": 274,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#df_contributor_panel\n",
    "#df_issue.query('repo_name == \"1e0ng/simhash\" & actor_id == 376272.0')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8bcd85d-3158-48b7-8385-ab06dbd02303",
   "metadata": {},
   "source": [
    "Git blame (%)\n",
    "- <p style = \"color:blue\"> can't do yet</p>\n",
    "- May be helpful: https://github.com/HelgeCPH/truckfactor/blob/main/truckfactor/compute.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fa3f71e-5875-4e35-b7a1-618f36d35f7c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2b894435-d1d0-4675-be13-2fa56f642d36",
   "metadata": {},
   "source": [
    "Grouped at the period level but can also consider aggregating across periods\n",
    "\n",
    "For each (2), (3), (4) that they were involved in\n",
    "1) what \\% of the work did they do\n",
    "2) what was their HHI\n",
    "- Can weight by average or comment-weighted average\n",
    "3) what was the \\% of work they're involved in that had other people involved\n",
    "\n",
    "**notes**\n",
    "- Use drive/output/derived/data_export/df_issue.parquet, drive/output/derived/data_export/df_pr_commits.parquet, drive/output/derived/data_export/df_pr.parquet, drive/output/derived/data_export/df_push_commits.parquet\n",
    "- (1) can't have HHI since only one person can open each issue, also (1) has share calculated already\n",
    "- (3) can just be code commits on merged PRs\n",
    "- (4) also can't have HHI  since only one person can close/merge each issue/PR, also (4) has share calculated already\n",
    "\n",
    "note that the same discussion can be in multiple time periods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "11a050ba-be73-401d-b52f-b2f0115a80c5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_issue = pd.read_parquet('drive/output/derived/data_export/df_issue.parquet')\n",
    "df_issue = ImputeTimePeriod(df_issue, time_period)\n",
    "\n",
    "df_pr = pd.read_parquet('drive/output/derived/data_export/df_pr.parquet')\n",
    "df_pr = ImputeTimePeriod(df_pr, time_period)\n",
    "\n",
    "df_pr_commits = pd.read_parquet('drive/output/derived/data_export/df_pr_commits.parquet')\n",
    "df_pr_commits['created_at'] = pd.to_datetime(df_pr_commits['commit time'], unit = 's')\n",
    "df_pr_commits = df_pr_commits.query('~created_at.isna()')\n",
    "df_pr_commits = ImputeTimePeriod(df_pr_commits, time_period)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "65d4566f-0820-4534-ab6f-76ffc59aaa3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_issue_comments = df_issue.query('type == \"IssueCommentEvent\"').sort_values('created_at').drop_duplicates('issue_comment_id')\n",
    "df_pr_rc = df_pr.query('type == \"PullRequestReviewCommentEvent\"')\n",
    "df_pr_rc['issue_number'] =  df_pr_rc['pr_number']\n",
    "df_pr_rc = df_pr_rc.sort_values('created_at').drop_duplicates('pr_review_comment_body')\n",
    "\n",
    "df_comments_actor = pd.concat([df_pr_rc[['repo_name','actor_id','time_period','issue_number','created_at']],\n",
    "                         df_issue_comments[['repo_name','actor_id','time_period','issue_number','created_at']]])\\\n",
    "    .groupby(['repo_name','actor_id','time_period','issue_number'])['created_at'].count()\\\n",
    "    .reset_index().rename({'created_at':'comments'}, axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "a04d119a-04cc-42d4-b703-5ff01ecd8fe8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def AggregateHHICount(df, id_col, count_col):\n",
    "    indiv_total_col = f'indiv_total_{count_col}'\n",
    "    discussions_col = f'{count_col}_discussions_involved'\n",
    "    total_col = f'total_{count_col}'\n",
    "    share_col = f'{count_col}_share'\n",
    "    hhi_col = f'{count_col}_hhi'\n",
    "    indiv_col = f'{count_col}_indiv'\n",
    "    wm = lambda x: np.average(x, weights=df.loc[x.index, count_col])\n",
    "\n",
    "    df[discussions_col] = df.groupby(['repo_name','time_period','actor_id'])[count_col].transform('count')\n",
    "    df[indiv_total_col] = df.groupby(['repo_name','time_period','actor_id'])[count_col].transform('sum')\n",
    "    df[total_col] = df.groupby(['repo_name','time_period',id_col])[count_col].transform('sum')\n",
    "    df[share_col] = df[count_col]/df[total_col]\n",
    "    df[hhi_col] = df.assign(share_sq = lambda x: x[share_col]**2)\\\n",
    "        .groupby(['repo_name','time_period',id_col])['share_sq'].transform('sum')\n",
    "    df[indiv_col] = (df[share_col]<1).astype(int)\n",
    "    df[f'{share_col}_wt'] = df[share_col] * df[count_col]/df[indiv_total_col]\n",
    "    df[f'{hhi_col}_wt'] = df[hhi_col] * df[count_col]/df[indiv_total_col]\n",
    "    \n",
    "    df_grouped = df.groupby(['repo_name','actor_id','time_period',indiv_total_col,discussions_col]).agg(\n",
    "        **{f'{share_col}_avg': (share_col, 'mean'),\n",
    "           f'{share_col}_avg_wt': (f'{share_col}_wt', 'sum'),\n",
    "           f'{hhi_col}_avg': (hhi_col, 'mean'),\n",
    "           f'{hhi_col}_avg_wt': (f'{hhi_col}_wt', 'sum'),\n",
    "           f'pct_cooperation_{count_col}': (indiv_col, 'mean')}).reset_index()\n",
    "    \n",
    "    return df_grouped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "1f9b4c05-6fa2-4042-8c97-e57a3be6d9e8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_comments_grouped = AggregateHHICount(df_comments_actor, 'issue_number', 'comments')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "29ac0d17-2051-42bb-bcce-2cc62902ea6e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_committers_ids = df_committers_clean.explode('commit_name').explode('email_address')[['commit_name','email_address','actor_id']].drop_duplicates()\n",
    "df_pr_commits_id = pd.merge(df_pr_commits, df_committers_ids, how = 'left', left_on = ['commit author name','commit author email'], right_on = ['commit_name','email_address'])\n",
    "df_pr_commits_id['commits'] = 1\n",
    "df_pr_commits_id['commits_lt_100_fc'] = (df_pr_commits_id['commit files changed count']<100).astype(int)\n",
    "\n",
    "#### NOT GOING TO WEIGHT BY ADDITIONS, DELETIONS FOR NOW BUT GOOD TO HAVE\n",
    "df_pr_commits_actor = df_pr_commits_id.query('~actor_id.isna()').groupby(['repo_name','time_period','actor_id','pr_number']).agg(\n",
    "    commits_count = ('commits','sum'),\n",
    "    commits_lt_100_fc_count = ('commits_lt_100_fc','sum'),\n",
    "    commits_add_sum = ('commit additions','sum'),\n",
    "    commits_del_sum = ('commit deletions','sum'),\n",
    "    commits_change_sum = ('commit changes total','sum'),\n",
    "    commits_files_changed_count = ('commit files changed count','sum')).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "23e9106c-eb3e-40b3-b0da-073ba5fa76e3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# identify for each PR unique commits\n",
    "# separate analysis for commits with <100 files\n",
    "# also do stuff with file changes\n",
    "\n",
    "df_pr_commits_grouped = AggregateHHICount(df_pr_commits_actor, 'pr_number', 'commits_count')\n",
    "df_pr_commits_grouped = df_pr_commits_grouped.rename({'commits_count_discussions_involved':'pr_discussions_involved'}, axis = 1)\n",
    "\n",
    "df_pr_commits_lt_100_fc_grouped = AggregateHHICount(df_pr_commits_actor, 'pr_number', 'commits_lt_100_fc_count')\n",
    "df_pr_commits_lt_100_fc_grouped = df_pr_commits_lt_100_fc_grouped.rename({'commits_lt_100_fc_count_discussions_involved':'lt_100_fc_pr_discussions_involved'}, axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d7cb2ff-14fa-4df4-b115-7ac9e073489b",
   "metadata": {},
   "source": [
    "## Organizational Structure"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ece4718a-3820-4526-87cc-55d51f6c83bd",
   "metadata": {},
   "source": [
    "**Layer Count**\n",
    "- For each layer, %  of all individuals involved in a layer, # of people involved\n",
    "- Also number of layers where there's someone in that layer uniquely and not in any other layer \n",
    "\n",
    "df_contributor_rank - I think I can aggregate this\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "c9e0811f-c203-4a12-b48f-26d68d22d5c7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e5ab130147ec4ce8a66c73a7e96fd866",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HBox(children=(IntProgress(value=0, description='0.00%', max=2748), Label(value='0 / 2748'))), …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import itertools\n",
    "\n",
    "LAYERS = agg_cols\n",
    "\n",
    "def find_all_min_covers_for_group(grp):\n",
    "    \"\"\"\n",
    "    Given the subset of the DataFrame for one (repo_name, time_period),\n",
    "    return ALL minimal subsets of LAYERS that cover every actor in this group.\n",
    "\n",
    "    Cover criterion:\n",
    "      For each row (actor), at least one column in the chosen subset is True.\n",
    "    \"\"\"\n",
    "    # We'll just slice the boolean columns\n",
    "    subset_matrix = grp[LAYERS]\n",
    "\n",
    "    # We'll track all minimal subsets found\n",
    "    # If we never find any, it returns an empty list (though that likely won't happen\n",
    "    # if there's at least one True in each group).\n",
    "    min_solutions = []\n",
    "\n",
    "    # 1) We try subsets of size 1, then size 2, etc., up to size 4\n",
    "    # 2) Once we find ANY solutions for a given size r, we stop (those are minimal).\n",
    "    for r in range(1, len(LAYERS) + 1):\n",
    "        possible_combos = itertools.combinations(LAYERS, r)\n",
    "        found_any = False\n",
    "\n",
    "        for combo in possible_combos:\n",
    "            # If every row is covered by 'combo', we keep it\n",
    "            # \"covered\" means row-wise .any(axis=1) is True for all rows\n",
    "            if subset_matrix[list(combo)].any(axis=1).all():\n",
    "                min_solutions.append(combo)\n",
    "                found_any = True\n",
    "\n",
    "        if found_any:\n",
    "            # We found at least one solution of size r, so there's no need\n",
    "            # to look at larger subsets.\n",
    "            break\n",
    "\n",
    "    return pd.Series({'all_min_layers': min_solutions})\n",
    "\n",
    "df_panel_min_layers = df_contributor_panel_rank\\\n",
    "    .groupby(['repo_name', 'time_period'], as_index=False)\\\n",
    "    .parallel_apply(find_all_min_covers_for_group)\n",
    "df_panel_min_layers['min_layer_size'] = df_panel_min_layers['all_min_layers'].apply(lambda x: len(x[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "464173fd-a982-4693-a9a9-af5c274b2ba2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_panel_rank_count = df_contributor_panel_rank.groupby(['repo_name','time_period'])[agg_cols].sum()\n",
    "df_panel_rank_count.columns = [f\"{col}_layer_count\" for col in df_panel_rank_count.columns]\n",
    "df_panel_rank_pct = df_contributor_panel_rank.groupby(['repo_name','time_period'])[agg_cols].mean()\n",
    "df_panel_rank_pct.columns = [f\"{col}_layer_pct\" for col in df_panel_rank_count.columns]\n",
    "df_panel_rank_count_pct = df_panel_rank_pct.join(df_panel_rank_count, how = 'outer').reset_index()\n",
    "df_panel_rank = df_panel_rank_pct.join(df_panel_rank_count, how = 'outer').reset_index()\n",
    "\n",
    "df_panel_layers = (df_panel_rank_count>0).sum(axis = 1).reset_index().rename({0:'layer_count'}, axis = 1)\n",
    "df_panel_layers = pd.merge(df_panel_min_layers, df_panel_layers)\n",
    "\n",
    "#df_panel_rank df_panel_layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c1357f1-c938-433a-af2a-19af381068e1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "cda8e483-6d70-4c5c-9ad9-d10592f0552e",
   "metadata": {},
   "source": [
    "**overlap**\n",
    "- For each overlap combination (of two), % overlap between the areas\n",
    "- Degree of overlap - % of work done in lower layer by someone whose in a specific higher layer\n",
    "- Degree of overlap - % of work done in lower layer by someone whose also in a higher layer (sum of the above, can also try)\n",
    "\n",
    "always use lower layer as nominator\n",
    "\n",
    "df_contributor_panel_rank can also be used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "590c2836-a2af-4a62-bdbb-de2607790f61",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>repo_name</th>\n",
       "      <th>time_period</th>\n",
       "      <th>problem_identification_problem_discussion_overlap</th>\n",
       "      <th>problem_identification_problem_discussion_work_done</th>\n",
       "      <th>problem_identification_code_writing_overlap</th>\n",
       "      <th>problem_identification_code_writing_work_done</th>\n",
       "      <th>problem_identification_problem_approval_overlap</th>\n",
       "      <th>problem_identification_problem_approval_work_done</th>\n",
       "      <th>problem_discussion_code_writing_overlap</th>\n",
       "      <th>problem_discussion_code_writing_work_done</th>\n",
       "      <th>problem_discussion_problem_approval_overlap</th>\n",
       "      <th>problem_discussion_problem_approval_work_done</th>\n",
       "      <th>code_writing_problem_approval_overlap</th>\n",
       "      <th>code_writing_problem_approval_work_done</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1e0ng/simhash</td>\n",
       "      <td>2020-07-01</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.619048</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.571429</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1e0ng/simhash</td>\n",
       "      <td>2021-01-01</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1e0ng/simhash</td>\n",
       "      <td>2021-07-01</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>0.631579</td>\n",
       "      <td>0.428571</td>\n",
       "      <td>0.789474</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>0.583333</td>\n",
       "      <td>0.428571</td>\n",
       "      <td>0.722222</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1e0ng/simhash</td>\n",
       "      <td>2022-01-01</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.571429</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4Catalyzer/flask-resty-tenants</td>\n",
       "      <td>2016-01-01</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.428571</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.428571</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.450000</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.450000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87905</th>\n",
       "      <td>zyga/guacamole</td>\n",
       "      <td>2015-01-01</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87906</th>\n",
       "      <td>zyga/guacamole</td>\n",
       "      <td>2018-01-01</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87907</th>\n",
       "      <td>zzzsochi/trans</td>\n",
       "      <td>2015-07-01</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87908</th>\n",
       "      <td>zzzsochi/trans</td>\n",
       "      <td>2016-01-01</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87909</th>\n",
       "      <td>zzzsochi/trans</td>\n",
       "      <td>2016-07-01</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.800000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>87910 rows × 14 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                            repo_name time_period  \\\n",
       "0                       1e0ng/simhash  2020-07-01   \n",
       "1                       1e0ng/simhash  2021-01-01   \n",
       "2                       1e0ng/simhash  2021-07-01   \n",
       "3                       1e0ng/simhash  2022-01-01   \n",
       "4      4Catalyzer/flask-resty-tenants  2016-01-01   \n",
       "...                               ...         ...   \n",
       "87905                  zyga/guacamole  2015-01-01   \n",
       "87906                  zyga/guacamole  2018-01-01   \n",
       "87907                  zzzsochi/trans  2015-07-01   \n",
       "87908                  zzzsochi/trans  2016-01-01   \n",
       "87909                  zzzsochi/trans  2016-07-01   \n",
       "\n",
       "       problem_identification_problem_discussion_overlap  \\\n",
       "0                                                    1.0   \n",
       "1                                                    1.0   \n",
       "2                                                    1.0   \n",
       "3                                                    1.0   \n",
       "4                                                    1.0   \n",
       "...                                                  ...   \n",
       "87905                                                1.0   \n",
       "87906                                                1.0   \n",
       "87907                                                1.0   \n",
       "87908                                                NaN   \n",
       "87909                                                1.0   \n",
       "\n",
       "       problem_identification_problem_discussion_work_done  \\\n",
       "0                                                    1.0     \n",
       "1                                                    1.0     \n",
       "2                                                    1.0     \n",
       "3                                                    1.0     \n",
       "4                                                    1.0     \n",
       "...                                                  ...     \n",
       "87905                                                1.0     \n",
       "87906                                                1.0     \n",
       "87907                                                1.0     \n",
       "87908                                                NaN     \n",
       "87909                                                1.0     \n",
       "\n",
       "       problem_identification_code_writing_overlap  \\\n",
       "0                                         1.000000   \n",
       "1                                         0.000000   \n",
       "2                                         0.142857   \n",
       "3                                         0.166667   \n",
       "4                                         0.333333   \n",
       "...                                            ...   \n",
       "87905                                     1.000000   \n",
       "87906                                     0.000000   \n",
       "87907                                     1.000000   \n",
       "87908                                          NaN   \n",
       "87909                                     0.666667   \n",
       "\n",
       "       problem_identification_code_writing_work_done  \\\n",
       "0                                           1.000000   \n",
       "1                                                NaN   \n",
       "2                                           0.631579   \n",
       "3                                           0.500000   \n",
       "4                                           0.428571   \n",
       "...                                              ...   \n",
       "87905                                       1.000000   \n",
       "87906                                            NaN   \n",
       "87907                                       1.000000   \n",
       "87908                                            NaN   \n",
       "87909                                       0.666667   \n",
       "\n",
       "       problem_identification_problem_approval_overlap  \\\n",
       "0                                             0.500000   \n",
       "1                                             0.666667   \n",
       "2                                             0.428571   \n",
       "3                                             0.166667   \n",
       "4                                             0.333333   \n",
       "...                                                ...   \n",
       "87905                                         0.000000   \n",
       "87906                                         0.000000   \n",
       "87907                                         1.000000   \n",
       "87908                                              NaN   \n",
       "87909                                         0.333333   \n",
       "\n",
       "       problem_identification_problem_approval_work_done  \\\n",
       "0                                               0.800000   \n",
       "1                                               0.500000   \n",
       "2                                               0.789474   \n",
       "3                                               0.500000   \n",
       "4                                               0.428571   \n",
       "...                                                  ...   \n",
       "87905                                                NaN   \n",
       "87906                                                NaN   \n",
       "87907                                           1.000000   \n",
       "87908                                                NaN   \n",
       "87909                                           0.333333   \n",
       "\n",
       "       problem_discussion_code_writing_overlap  \\\n",
       "0                                     1.000000   \n",
       "1                                     0.000000   \n",
       "2                                     0.142857   \n",
       "3                                     0.166667   \n",
       "4                                     0.333333   \n",
       "...                                        ...   \n",
       "87905                                 1.000000   \n",
       "87906                                 0.000000   \n",
       "87907                                 1.000000   \n",
       "87908                                      NaN   \n",
       "87909                                 0.666667   \n",
       "\n",
       "       problem_discussion_code_writing_work_done  \\\n",
       "0                                       1.000000   \n",
       "1                                            NaN   \n",
       "2                                       0.583333   \n",
       "3                                       0.500000   \n",
       "4                                       0.450000   \n",
       "...                                          ...   \n",
       "87905                                   1.000000   \n",
       "87906                                        NaN   \n",
       "87907                                   1.000000   \n",
       "87908                                        NaN   \n",
       "87909                                   0.750000   \n",
       "\n",
       "       problem_discussion_problem_approval_overlap  \\\n",
       "0                                         0.500000   \n",
       "1                                         0.666667   \n",
       "2                                         0.428571   \n",
       "3                                         0.166667   \n",
       "4                                         0.333333   \n",
       "...                                            ...   \n",
       "87905                                     0.000000   \n",
       "87906                                     0.000000   \n",
       "87907                                     1.000000   \n",
       "87908                                          NaN   \n",
       "87909                                     0.333333   \n",
       "\n",
       "       problem_discussion_problem_approval_work_done  \\\n",
       "0                                           0.619048   \n",
       "1                                           0.500000   \n",
       "2                                           0.722222   \n",
       "3                                           0.500000   \n",
       "4                                           0.450000   \n",
       "...                                              ...   \n",
       "87905                                            NaN   \n",
       "87906                                            NaN   \n",
       "87907                                       1.000000   \n",
       "87908                                            NaN   \n",
       "87909                                       0.500000   \n",
       "\n",
       "       code_writing_problem_approval_overlap  \\\n",
       "0                                        0.5   \n",
       "1                                        NaN   \n",
       "2                                        1.0   \n",
       "3                                        0.5   \n",
       "4                                        1.0   \n",
       "...                                      ...   \n",
       "87905                                    0.0   \n",
       "87906                                    NaN   \n",
       "87907                                    1.0   \n",
       "87908                                    0.0   \n",
       "87909                                    0.5   \n",
       "\n",
       "       code_writing_problem_approval_work_done  \n",
       "0                                     0.571429  \n",
       "1                                          NaN  \n",
       "2                                     1.000000  \n",
       "3                                     0.571429  \n",
       "4                                     1.000000  \n",
       "...                                        ...  \n",
       "87905                                      NaN  \n",
       "87906                                      NaN  \n",
       "87907                                 1.000000  \n",
       "87908                                      NaN  \n",
       "87909                                 0.800000  \n",
       "\n",
       "[87910 rows x 14 columns]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_repo_panel_overlap = df_contributor_panel_rank[['repo_name', 'time_period']].drop_duplicates().reset_index(drop=True)\n",
    "\n",
    "for lower_layer, higher_layer in itertools.combinations(LAYERS, 2):\n",
    "    \n",
    "    overlap = (\n",
    "        df_contributor_panel_rank\n",
    "        .query(f'({lower_layer} == True)')\n",
    "        .groupby(['repo_name', 'time_period'])[higher_layer]\n",
    "        .mean()\n",
    "        .reset_index()\n",
    "        .rename(columns={higher_layer: f\"{lower_layer}_{higher_layer}_overlap\"})\n",
    "    )\n",
    "    df_repo_panel_overlap = pd.merge(df_repo_panel_overlap, overlap, on=['repo_name', 'time_period'], how='left')\n",
    "    \n",
    "    lower_higher_numerator = (\n",
    "        df_contributor_panel_rank\n",
    "        .query(f'({lower_layer} == True) & ({higher_layer} == True)')\n",
    "        .groupby(['repo_name', 'time_period'])[f\"{lower_layer}_sum\"]\n",
    "        .sum()\n",
    "        .reset_index()\n",
    "        .rename(columns={f\"{lower_layer}_sum\": f\"{lower_layer}_{higher_layer}_work_done_numerator\"})\n",
    "    )\n",
    "    lower_higher_denominator = (\n",
    "        df_contributor_panel_rank\n",
    "        .query(f'({lower_layer} == True)')\n",
    "        .groupby(['repo_name', 'time_period'])[f\"{lower_layer}_sum\"]\n",
    "        .sum()\n",
    "        .reset_index()\n",
    "        .rename(columns={f\"{lower_layer}_sum\": f\"{lower_layer}_{higher_layer}_work_done_denominator\"})\n",
    "    )\n",
    "    \n",
    "    work_done = pd.merge(\n",
    "        lower_higher_numerator, \n",
    "        lower_higher_denominator, \n",
    "        on=['repo_name', 'time_period'], \n",
    "        how='right'  # Ensure all groups with lower_layer=True are included\n",
    "    )\n",
    "    \n",
    "    work_done[f\"{lower_layer}_{higher_layer}_work_done\"] = work_done.apply(\n",
    "        lambda row: (\n",
    "            row[f\"{lower_layer}_{higher_layer}_work_done_numerator\"] / \n",
    "            row[f\"{lower_layer}_{higher_layer}_work_done_denominator\"]\n",
    "        ) if row[f\"{lower_layer}_{higher_layer}_work_done_denominator\"] > 0 else 0,\n",
    "        axis=1\n",
    "    )\n",
    "    \n",
    "    # e) Select Relevant Columns and Fill NaN with 0\n",
    "    work_done = work_done[['repo_name', 'time_period', f\"{lower_layer}_{higher_layer}_work_done\"]]\n",
    "    \n",
    "    # Merge the work_done data into the summary DataFrame\n",
    "    df_repo_panel_overlap = pd.merge(df_repo_panel_overlap, work_done, on=['repo_name', 'time_period'], how='left')\n",
    "\n",
    "df_repo_panel_overlap"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60ac2fd7-29bf-452a-a90b-70b3e1214279",
   "metadata": {},
   "source": [
    "**work split**\n",
    "- HHI for each layer\n",
    "\n",
    "df_contributor_panel\n",
    "\n",
    "- git blame, for each file, find the HHI\n",
    "  - average using all files\n",
    "  - average using LOC\n",
    "\n",
    "\n",
    "<p style = \"color:blue\"> can't do yet</p>\n",
    "- May be helpful: https://github.com/HelgeCPH/truckfactor/blob/main/truckfactor/compute.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "46311846-c2a7-4149-9a42-42c8b316bd0f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_repo_hhi = df_contributor_panel_rank[['repo_name', 'time_period']].drop_duplicates().reset_index(drop=True)\n",
    "\n",
    "# Iterate through each layer to calculate HHI\n",
    "for layer in LAYERS:\n",
    "    share_col = f\"{layer}_share\"\n",
    "    hhi_col = f\"{layer}_HHI\"\n",
    "    hhi = (\n",
    "        df_contributor_panel_rank\n",
    "        .query(f\"{layer} == True\")\n",
    "        .assign(share_sq = df_contributor_panel_rank[share_col]**2)\n",
    "        .groupby(['repo_name', 'time_period'])['share_sq']\n",
    "        .sum()\n",
    "        .reset_index()\n",
    "        .rename(columns={'share_sq': hhi_col})\n",
    "    )\n",
    "    \n",
    "    df_repo_hhi = pd.merge(df_repo_hhi, hhi, on=['repo_name', 'time_period'], how='left')\n",
    "# df_repo_hhi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccd9f8c4-a561-4f7f-95c5-7449e43fe3dd",
   "metadata": {},
   "source": [
    "**cooperation**\n",
    "- calculate HHI for each discussion thread (2), PR linked commits (3) and PR review (4)\n",
    "  - can use average across discussions or average weighted by qty of discussion\n",
    "- \\# of distinct individuals per discussion\n",
    "  - can use average across discussions or average weighted by qty of discussion\n",
    "- % of each with more than one distinct individual\n",
    "\n",
    "Use drive/output/derived/data_export/df_issue.parquet, drive/output/derived/data_export/df_pr_commits.parquet, drive/output/derived/data_export/df_pr.parquet, drive/output/derived/data_export/df_push_commits.parquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "89e64bd2-03d6-4f8c-b04b-9353f8669908",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import itertools\n",
    "\n",
    "def calculate_hhi_cooperation(df, entity, metric):\n",
    "    metric_share = f\"{metric}_share\"                # e.g., 'comments_share'\n",
    "    entity_number = f\"{entity}_number\"              # e.g., 'issue_number'\n",
    "    metric_col = metric                             # e.g., 'comments'\n",
    "    total_metric = f\"total_{metric}\"                # e.g., 'total_comments'\n",
    "    repo_metric = f\"repo_{metric}\"                  # e.g., 'repo_comments'\n",
    "    \n",
    "    hhi_col = f\"{entity}_hhi_{metric}\"              # e.g., 'issue_hhi_comments'\n",
    "    cooperation_col = f\"{metric}_cooperation\"       # e.g., 'comments_cooperation'\n",
    "    wt_work_done_col = f\"{entity}_{metric}_wt\"         # e.g., 'issue_comments_wt'\n",
    "    \n",
    "    df[hhi_col] = df.assign(share_sq = df[metric_share] ** 2)\\\n",
    "        .groupby(['repo_name', 'time_period', entity_number])['share_sq'].transform('sum')\n",
    "    \n",
    "    df[cooperation_col] = df[hhi_col] != 1\n",
    "    df[repo_metric] = df.groupby(['repo_name', 'time_period'])[metric_col].transform('sum')\n",
    "    \n",
    "    df[wt_work_done_col] = df[hhi_col] * df[total_metric] / df[repo_metric]\n",
    "    \n",
    "    df_unique = df.drop_duplicates(['repo_name', 'time_period', entity_number])\n",
    "    \n",
    "    df_cooperation = df_unique.groupby(['repo_name', 'time_period']).agg(\n",
    "        hhi_metric = (hhi_col, 'mean'),\n",
    "        hhi_metric_wt = (wt_work_done_col, 'sum'),\n",
    "        metric_cooperation_pct = (cooperation_col, 'mean')\n",
    "    ).reset_index()\n",
    "    \n",
    "    df_cooperation = df_cooperation.rename(columns={\n",
    "        'hhi_metric': f\"hhi_{metric}\",\n",
    "        'hhi_metric_wt': f\"hhi_{metric}_wt\",\n",
    "        'metric_cooperation_pct': f\"{metric}_cooperation_pct\"\n",
    "    })\n",
    "    \n",
    "    \n",
    "    return df_cooperation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "23ea974c-c60a-4875-b541-6c88bc4444ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_comments_cooperation = calculate_hhi_cooperation(df_comments_actor, 'issue','comments')\n",
    "df_commits_cooperation = calculate_hhi_cooperation(df_pr_commits_actor, 'pr','commits_count')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e22b8b12-535b-43c1-9d1c-45dd602311a0",
   "metadata": {},
   "source": [
    "**communication**\n",
    "- Given a contributor's rank, what % of discussion occurs with people whose rank is (1), (2), (3), (4)\n",
    "  - get 1/x weight if they are in x ranks (for the % of discussion)\n",
    "  - Average weighted by contributions of person, or by person\n",
    "- Can aggregate for each contributor into a weighted average of the average rank they communicate with "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "dc74876b-1d76-4916-8820-0b0d82a75e52",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_comments_actor_rank = pd.merge(df_comments_actor, df_contributor_panel_rank[['actor_id','repo_name','time_period','max_rank'] + agg_cols].drop_duplicates(), how = 'left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "998c39a1-b449-49ae-a2f5-d38e54ec143f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def RankParticipationFlags(df, entity='issue'):\n",
    "    entity_number = f\"{entity}_number\"\n",
    "\n",
    "    ranks = [1, 2, 3, 4]\n",
    "\n",
    "    rank_cols = [f'rank_{rank}' for rank in ranks]\n",
    "    for rank in ranks:\n",
    "        df[f'rank_{rank}'] = (df['max_rank'] == rank).astype(int)\n",
    "\n",
    "    group_cols = ['repo_name', 'time_period', entity_number]\n",
    "    rank_presence = df.groupby(group_cols)[rank_cols].max().reset_index()\n",
    "\n",
    "    participation_cols = [f'participated_rank_{rank}' for rank in ranks]\n",
    "    rename_dict = {f'rank_{rank}': f'participated_rank_{rank}' for rank in ranks}\n",
    "    rank_presence = rank_presence.rename(columns=rename_dict)\n",
    "\n",
    "    df = df.merge(rank_presence, on=group_cols, how='left')\n",
    "    df = df.drop(columns=rank_cols)\n",
    "\n",
    "    return df\n",
    "\n",
    "def calculate_rank_metrics(df, entity_number='issue_number', metric='comments', ranks=[1,2,3,4],\n",
    "                           weight_column='comments_discussions_involved'):\n",
    "\n",
    "    df = RankParticipationFlags(df)\n",
    "    # weighted by number of ppl in discussion \n",
    "    df_ppl_wt = df.groupby(['repo_name','time_period','max_rank']).agg(\n",
    "        participated_rank_1 = ('participated_rank_1','mean'),\n",
    "        participated_rank_2 = ('participated_rank_2','mean'),\n",
    "        participated_rank_3 = ('participated_rank_3','mean'),\n",
    "        participated_rank_4 = ('participated_rank_4','mean'),\n",
    "        issue_rank_count = ('actor_id','count')\n",
    "    )\n",
    "\n",
    "    # issue by issue average\n",
    "    df_wt_by_issue = df.drop_duplicates(['repo_name','time_period',entity_number,'max_rank']).groupby(['repo_name','time_period','max_rank']).agg(\n",
    "        participated_rank_1_wt_by_issue = ('participated_rank_1','mean'),\n",
    "        participated_rank_2_wt_by_issue = ('participated_rank_2','mean'),\n",
    "        participated_rank_3_wt_by_issue = ('participated_rank_3','mean'),\n",
    "        participated_rank_4_wt_by_issue = ('participated_rank_4','mean')\n",
    "    )\n",
    "\n",
    "    ## ADD OTHER WEIGHTS I WANT TO DO\n",
    "    df_rank_count = df.drop_duplicates(['repo_name','time_period','max_rank','actor_id']).groupby(\n",
    "        ['repo_name','time_period','max_rank'])['actor_id'].count().rename('rank_contributor_count')\n",
    "    \n",
    "    df_rank_summary = df_ppl_wt.join(df_wt_by_issue).join(df_rank_count)\n",
    "    \n",
    "    return df_rank_summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "3984922f-8c7a-4ab1-89f1-af5f3fbc68e4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_comments_rank_summary = calculate_rank_metrics(df_comments_actor_rank)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "932f7426-3183-43c5-a1ae-733068e0df90",
   "metadata": {},
   "outputs": [],
   "source": [
    "def CalculateSize(df_contributor_panel):\n",
    "    df_repo_size = df_contributor_panel.groupby(['repo_name','time_period']).agg({\n",
    "        'actor_id':'count','problem_identification': 'sum','problem_discussion':'sum','problem_solving':'sum',\n",
    "        'solution_incorporation':'sum'}).rename({\n",
    "        'actor_id':\"contributor_count\",\"problem_identification\":\"problem_identifier_count\",\"problem_discussion\":\"problem_discusser_count\",\n",
    "        \"problem_solving\":\"problem_solver_count\",\"solution_incorporation\":\"solution_incorporator_count\"}, axis = 1).reset_index()\n",
    "    return df_repo_size\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc24b100-494d-4875-9776-d0da200a3d0b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_repo_size = CalculateSize(df_contributor_panel)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80a28bb8-126d-4045-89c7-674e6a09ca42",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_repo_panel = pd.merge(df_repo_size, df_repo_hhi, how = 'outer').merge(df_repo_overlap, how = 'outer')\n",
    "df_repo_panel['first_period'] = df_repo_panel.groupby('repo_name')['time_period'].transform('min')\n",
    "df_repo_panel['final_period'] = df_repo_panel.groupby('repo_name')['time_period'].transform('max')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67a969e1-2382-4a7a-9832-83ba30b34855",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "time_periods = df_repo_panel['time_period'].unique().tolist()\n",
    "df_balanced = df_repo_panel[['repo_name']].drop_duplicates()\n",
    "df_balanced['time_period'] = [time_periods for i in range(df_balanced.shape[0])]\n",
    "df_balanced = df_balanced.explode('time_period')\n",
    "df_repo_panel_full = pd.merge(df_balanced, df_repo_panel, how = 'left')\n",
    "df_repo_panel_full[['first_period','final_period']] = df_repo_panel_full.groupby(['repo_name'])[['first_period','final_period']].ffill()\n",
    "df_repo_panel_full = df_repo_panel_full.query('time_period >= first_period')\n",
    "df_repo_panel_full[[col for col in df_repo_panel_full.columns if 'hhi' not in col]] = df_repo_panel_full[[col for col in df_repo_panel_full.columns if 'hhi' not in col]].fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e1c9677-e871-4986-b315-709458c5b577",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_repo_overlap = CalculateOverlap(df_contributor_panel)\n",
    "#df_repo_panel_full\n",
    "#df_contributor_selected_panel"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
