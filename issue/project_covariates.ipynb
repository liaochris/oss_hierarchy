{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cbb7291b-a7b6-4e88-849a-378cfe90c33e",
   "metadata": {},
   "source": [
    "# WHEN I\"M DONE, I want to review what else I still need to calculate\n",
    "# BASICALLY document the math for all metrics in overleaf and add to the latex wha I still need to calculate\n",
    "# align the headers for this notebook and the category names in latex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "43310309-672d-4698-bb68-a33aaa8ec6c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir('../')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "acc5551e-574d-42f3-84d9-885e864f477e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import sys\n",
    "import glob\n",
    "import warnings\n",
    "import random\n",
    "from glob import glob \n",
    "import datetime\n",
    "import itertools\n",
    "import time\n",
    "from multiprocessing import pool\n",
    "from source.lib.helpers import *\n",
    "from pandarallel import pandarallel\n",
    "import ast\n",
    "from functools import reduce\n",
    "import yaml\n",
    "import re\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9292248b-9696-4f71-90e4-89f0a13dbfef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: Pandarallel will run on 6 workers.\n",
      "INFO: Pandarallel will use Memory file system to transfer data between the main process and workers.\n"
     ]
    }
   ],
   "source": [
    "warnings.filterwarnings(\"ignore\")\n",
    "pd.set_option('display.max_columns', None)\n",
    "pandarallel.initialize(progress_bar = False)\n",
    "\n",
    "indir_data = Path('drive/output/derived/contributor_stats/contributor_data')\n",
    "outdir_data = Path('drive/output/derived/project_outcomes')\n",
    "\n",
    "time_period = 6#int(sys.argv[1])\n",
    "df_contributor_panel = pd.read_parquet(indir_data / f\"major_contributors_major_months{time_period}_window732D_samplefull.parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccbe2717-d609-43c1-87c0-c25ff0953a89",
   "metadata": {},
   "source": [
    "## Organizational Characteristics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e855ac7-f294-4aa1-b908-84827b88e0e2",
   "metadata": {},
   "source": [
    "**Libraries.io API**: Use the \"Repos\" query, query for \"created at\" date. \n",
    "\n",
    "<span style=\"color:red\">Not done</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9bb43c7-9351-4875-b754-f9f192717bdd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f8cd3284-42bd-41df-8303-61027a9b93d4",
   "metadata": {},
   "source": [
    "**Forks + Stars**: Obtained from GitHub Archive, calculates number of stars/forks per time-period  \n",
    "<span style=\"color:blue\">do validation on how accurate the statistics are</span>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4484b72d-ebbc-41f9-b1be-671ec128a6e0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def AggregateStarsForks(indir, time_period, colname):\n",
    "    df_agg = pd.concat(\n",
    "        (pd.read_csv(file, usecols = ['created_at','repo_name', 'org_login']) for file in indir.glob(\"*.csv\")),\n",
    "        ignore_index=True\n",
    "    )\n",
    "    df_agg = ImputeTimePeriod(df_agg, time_period)\n",
    "    df = df_agg.groupby(['repo_name','time_period']).size().reset_index()\n",
    "    df.rename(columns = {0:colname}, inplace = True)\n",
    "\n",
    "    org_list = df_agg['org_login'].dropna().unique().tolist()\n",
    "    return org_list, df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3b2c3f51-edbd-4cd6-a011-090f238c6ce5",
   "metadata": {},
   "outputs": [],
   "source": [
    "indir_watch = Path(\"drive/output/scrape/extract_github_data/watch_data\")\n",
    "indir_fork = Path(\"drive/output/scrape/extract_github_data/fork_data\")\n",
    "star_orgs, df_stars = AggregateStarsForks(indir_watch, time_period, 'stars')\n",
    "df_stars.rename(columns = {'stars':'stars_gained'}, inplace = True)\n",
    "fork_orgs, df_fork = AggregateStarsForks(indir_fork, time_period, 'forks')\n",
    "df_fork.rename(columns = {'forks':'forks_gained'}, inplace = True)\n",
    "initial_org_list = list(set(star_orgs + fork_orgs))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfb32831-0c42-4159-a28b-33252e2a3f9f",
   "metadata": {},
   "source": [
    "**Creator Type (Org/Indiv)**: Whether a project was created by an organization or an individual. I obtained this by obtaining all organization names from fork, star, issues, issue comments and PR, PR reviews and PR review comment data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "af15ba61-1899-486f-ab68-a16501e73927",
   "metadata": {},
   "outputs": [],
   "source": [
    "def GetUniqueOrgs(dir_path):\n",
    "    dfs = [pd.read_csv(f, usecols=[\"org_login\"], dtype=str) for f in dir_path.glob(\"*.csv\")]\n",
    "    if not dfs:\n",
    "        return set()\n",
    "    combined = pd.concat(dfs, ignore_index=True)\n",
    "    return combined[\"org_login\"].dropna().unique().tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "02b93eab-0069-43c8-b747-a9c9243b68dc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Have collected 2221 unique organizations\n"
     ]
    }
   ],
   "source": [
    "indir_issue_raw = Path(\"drive/output/scrape/extract_github_data/issue_data\")\n",
    "indir_issue_comment_raw = Path(\"drive/output/scrape/extract_github_data/issue_comment_data\")\n",
    "indir_pull_request_raw = Path(\"drive/output/scrape/extract_github_data/pull_request_data\")\n",
    "indir_pull_request_review_comment_raw = Path(\"drive/output/scrape/extract_github_data/pull_request_review_comment_data\")\n",
    "indir_pull_request_review_raw = Path(\"drive/output/scrape/extract_github_data/pull_request_review_data\")\n",
    "\n",
    "all_orgs = initial_org_list\n",
    "for indir in [indir_issue_raw,indir_issue_comment_raw,indir_pull_request_raw, indir_pull_request_review_comment_raw,indir_pull_request_review_raw]:\n",
    "    all_orgs += GetUniqueOrgs(indir)\n",
    "\n",
    "unique_orgs_list = list(set(all_orgs))\n",
    "print(\"Have collected {} unique organizations\".format(len(unique_orgs_list)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e23a05d5-c71b-4267-adcc-e1b259286c13",
   "metadata": {},
   "source": [
    "**Project License**: Used license obtained from PyPI as of scraping date. Two cleaning changes made:\n",
    "- Changed all licenses with GPL to \"GNU General Public License\"\n",
    "- Labelled all licenses that are not MIT/BSD/Apache/GNU as \"other\"\n",
    "\n",
    "<span style=\"color:blue\">Adjust for changes to license by projects, originally downloaded on October 27, 2023 or September 29, 2024</span>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c7194d18-54ac-4c8f-a356-0ba0eb0287df",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "indir_license = Path('output/derived/collect_github_repos')\n",
    "def ProcessLicenseData(indir_license):\n",
    "    special_licenses = ['MIT License', 'BSD License', 'Apache Software License', 'GNU General Public License']\n",
    "    df = pd.read_csv(indir_license / 'linked_pypi_github.csv', index_col=0)\n",
    "    df = df[['github repository', 'license']].query('`github repository` != \"Unavailable\"')\n",
    "    df['license'] = df['license'].parallel_apply(ast.literal_eval).apply(\n",
    "        lambda x: sorted(\n",
    "            set(\n",
    "                ['GNU General Public License' if 'GPL' in license else license if license in special_licenses else 'Other License' \n",
    "                 for license in x if license != \"Unavailable\"]\n",
    "            )\n",
    "        )\n",
    "    )\n",
    "    return df\n",
    "def AddIndicators(df_license):\n",
    "    df = df_license.rename(columns={'github repository': 'repo_name'})\n",
    "    dummies = df['license'].apply(lambda x: x if isinstance(x, list) else []).str.join('|').str.get_dummies(sep='|')\n",
    "    dummies = dummies.rename(columns=lambda x: x.replace(' ', '_'))\n",
    "    return pd.concat([df, dummies], axis=1)\n",
    "    \n",
    "df_license = ProcessLicenseData(indir_license)\n",
    "df_license_indicators = AddIndicators(df_license)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74edabe8-773c-4fb2-acf2-4085f6ffd00e",
   "metadata": {},
   "source": [
    "**Truck Factor**\n",
    "- Obtained truck factor per time-period\n",
    "- Obtained \"creation date\" based off earliest commit date (<span style=\"color:blue\">There are some nonsensical commit dates so I should filter for only dates after git/svn was created</span>)\n",
    "- Merged each contributor to their set of emails and identified whether they had a [company domain](https://github.com/liaochris/undergrad_thesis/blob/main/data/inputs/company_domain_match_list.yaml), a .edu email or another domain. Then, I found the proportion of contributors with educational, or corporate email domains in each project and time period <span style=\"color:blue\"> company + industry list </span>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9deb4d40-3110-4c9b-977f-6f90186e2ca5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def CleanTruckFactor(indir_truckfactor, time_period):\n",
    "    df_truckfactor = pd.read_csv(indir_truckfactor / 'truckfactor.csv')\n",
    "    df = df_truckfactor.dropna().copy()\n",
    "    df['repo_name'] = (\n",
    "        df['repo_name']\n",
    "        .str.replace('drive/output/scrape/get_weekly_truck_factor/truckfactor_', '')\n",
    "        .str.replace('.csv', '')\n",
    "        .str.replace('_', '/', 1)\n",
    "    )\n",
    "    df = df.rename(columns={'date': 'created_at'})\n",
    "    df = ImputeTimePeriod(df, time_period)\n",
    "    \n",
    "    df_truckfactor_period = (\n",
    "        df\n",
    "        .sort_values('created_at')\n",
    "        .groupby(['time_period', 'repo_name'], as_index=False)\n",
    "        .tail(1)\n",
    "        .sort_values(['repo_name', 'time_period'])\n",
    "    )\n",
    "    df_truckfactor_period['authors'] = df_truckfactor_period['authors'].str.split(r'\\s*\\|\\s*')\n",
    "    df_truckfactor_period = df_truckfactor_period[['repo_name', 'time_period', 'created_at', 'week', 'year', 'truckfactor', 'authors']]\n",
    "\n",
    "    return df_truckfactor_period\n",
    "\n",
    "\n",
    "def AggregateCommitters(indir_committers):\n",
    "    df_committers = pd.read_csv(indir_committers / 'committers_info.csv', index_col=0)\n",
    "    df_clean = df_committers[['repo_name', 'commit_name', 'email_address', 'actor_id']].copy()\n",
    "    df_clean[['commit_name', 'email_address']] = df_clean[['commit_name', 'email_address']].applymap(ast.literal_eval)\n",
    "    \n",
    "    df_grouped = (\n",
    "        df_clean\n",
    "        .explode('commit_name')\n",
    "        .groupby(['commit_name', 'actor_id'])['repo_name']\n",
    "        .apply(list)\n",
    "        .reset_index()\n",
    "    )\n",
    "    \n",
    "    actor_emails = (\n",
    "        df_clean\n",
    "        .explode('email_address')[['email_address', 'actor_id']]\n",
    "        .drop_duplicates()\n",
    "        .dropna()\n",
    "        .groupby('actor_id')['email_address']\n",
    "        .apply(list)\n",
    "        .reset_index()\n",
    "    )\n",
    "    \n",
    "    return df_grouped, actor_emails, df_clean\n",
    "\n",
    "def MergeTruckFactorEmail(df_truckfactor_period, df_committers_grouped, actor_email_addresses):\n",
    "    df = df_truckfactor_period.explode('authors')\n",
    "    df = pd.merge(\n",
    "        df,\n",
    "        df_committers_grouped.rename(columns={'repo_name': 'repo'}),\n",
    "        how='left',\n",
    "        left_on='authors',\n",
    "        right_on='commit_name'\n",
    "    )\n",
    "    \n",
    "    group_cols = ['repo_name', 'time_period', 'authors']\n",
    "    df['merge_count'] = df.groupby(group_cols)['truckfactor'].transform('count')\n",
    "    df['repo_equal'] = (\n",
    "        df['repo'].apply(lambda x: isinstance(x, list)) &\n",
    "        df.apply(lambda x: x['repo_name'] in x['repo'] if isinstance(x['repo'], list) else False, axis=1)\n",
    "    ).astype(int)\n",
    "    df['merge_repo_match'] = df.groupby(group_cols)['repo_equal'].transform('sum')\n",
    "    \n",
    "    condition_na = (df['merge_count'] > 1) & ~((df['merge_repo_match'] == 1) & (df['repo_equal'] == 1))\n",
    "    df_na = df[condition_na].drop(['merge_count', 'repo_equal', 'merge_repo_match', 'repo'], axis=1)\n",
    "    df_na['commit_name'] = np.nan\n",
    "    \n",
    "    condition_valid = (df['merge_count'] == 1) | ((df['merge_count'] > 1) & (df['merge_repo_match'] == 1) & (df['repo_equal'] == 1))\n",
    "    df_valid = df[condition_valid].drop(['merge_count', 'repo_equal', 'merge_repo_match', 'repo'], axis=1)\n",
    "    \n",
    "    df_agg = pd.concat([df_valid, df_na.drop_duplicates()], ignore_index=True)\n",
    "    df_agg = df_agg.sort_values('commit_name').drop_duplicates(['repo_name', 'time_period', 'authors']).sort_values(['repo_name', 'time_period', 'authors'])\n",
    "    df_agg = pd.merge(df_agg, actor_email_addresses, how='left')\n",
    "    \n",
    "    return df_agg\n",
    "\n",
    "def AddCompanyAndIndustryColumns(df_truckfactor_agg):\n",
    "    with open('issue/company_domain_match_list.yaml', 'r') as f:\n",
    "        domains_list = yaml.load(f, Loader=yaml.FullLoader)\n",
    "    \n",
    "    domain_to_company = {}\n",
    "    domain_to_industry = {}\n",
    "    regex_mappings = []\n",
    "    \n",
    "    for entry in domains_list:\n",
    "        company = entry.get('company')\n",
    "        industry = entry.get('industry')\n",
    "        for domain in entry.get('domains', []):\n",
    "            domain_to_company[domain.lower()] = company\n",
    "            domain_to_industry[domain.lower()] = industry\n",
    "\n",
    "    \n",
    "    df_truckfactor_agg['domain'] = df_truckfactor_agg['email_address'].apply(\n",
    "        lambda x: [re.search(r'@([^@]+)$', email).group(1) for email in x  if isinstance(email, str) and re.search(r'@([^@]+)$', email)]\n",
    "        if isinstance(x, list) else np.nan)\n",
    "    df_truckfactor_agg['company'] = df_truckfactor_agg['domain'].apply(lambda x: [domain_to_company.get(email, np.nan) for email in x]\n",
    "                                       if isinstance(x, list) else np.nan)\n",
    "    df_truckfactor_agg['industry'] = df_truckfactor_agg['domain'].apply(lambda x: [domain_to_industry.get(email, np.nan) for email in x]\n",
    "                                       if isinstance(x, list) else np.nan)\n",
    "    \n",
    "    for col in ['company','industry']:\n",
    "        df_truckfactor_agg[col] = df_truckfactor_agg[col].apply(lambda x: sorted(set([email for email in x if not pd.isnull(email)])) \n",
    "                                                                if isinstance(x, list) else x)\n",
    "        df_truckfactor_agg[col] = df_truckfactor_agg[col].apply(lambda x: x if isinstance(x, list) and len(x)>0 else np.nan)\n",
    "        \n",
    "    return df_truckfactor_agg\n",
    "\n",
    "def AddEducationalColumns(df_truckfactor_agg):\n",
    "    df_truckfactor_agg['educational'] = df_truckfactor_agg['email_address'].apply(\n",
    "        lambda x: any([email.endswith(\".edu\") for email in x]) if isinstance(x, list) else False)\n",
    "    return df_truckfactor_agg\n",
    "\n",
    "def UniqueNonNull(s):\n",
    "    return s.dropna().unique()\n",
    "\n",
    "indir_truckfactor = Path('drive/output/scrape/get_weekly_truck_factor')\n",
    "df_truckfactor_period = CleanTruckFactor(indir_truckfactor, time_period)\n",
    "\n",
    "indir_committers = Path('drive/output/scrape/link_committers_profile')\n",
    "df_committers_grouped, actor_email_addresses, df_committers_clean = AggregateCommitters(indir_committers)\n",
    "\n",
    "df_truckfactor_agg = MergeTruckFactorEmail(df_truckfactor_period, df_committers_grouped, actor_email_addresses)\n",
    "df_truckfactor_agg = AddCompanyAndIndustryColumns(df_truckfactor_agg)\n",
    "df_truckfactor_agg = AddEducationalColumns(df_truckfactor_agg)\n",
    "df_truckfactor_repo_summary = df_truckfactor_agg.assign(company_bool = 1-df_truckfactor_agg['company'].isna()).groupby(['repo_name','time_period','truckfactor']).agg(\n",
    "    corporate_pct = ('company_bool','mean'),\n",
    "    educational_pct = ('educational','mean')).reset_index()\n",
    "\n",
    "# filter for after git was created? or after svn was created?\n",
    "df_creation_date = df_truckfactor_period[['repo_name','created_at']].assign(\n",
    "    created_year = df_truckfactor_period['created_at'].dt.year)\\\n",
    "    .query('created_year>1991')\\\n",
    "    .sort_values('created_at').drop_duplicates('repo_name')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c33a5acf-ee52-4195-aec4-26b0f0b6c5c0",
   "metadata": {},
   "source": [
    "**Organizational size**: \\# of people making contributions, defined as opening/closing an issue, commenting on an issue, PR, reviewing a PR, merging a PR, or committing code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b9a56d15-004c-43cc-96a8-b12ba0f3e9eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_contributor_count = df_contributor_panel.groupby(['repo_name','time_period'])['actor_id'].count().reset_index().rename(\n",
    "    {'actor_id':'contributors'}, axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef44f356-82ae-4fef-b950-bbcbd63b65a9",
   "metadata": {},
   "source": [
    "### Combine Covariates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "26cc1184-b39d-460b-9c1f-1cea23bad89b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def MakeBalanced(df_repo_panel):\n",
    "    df_repo_panel['first_period'] = df_repo_panel.groupby('repo_name')['time_period'].transform('min')\n",
    "    df_repo_panel['final_period'] = df_repo_panel.groupby('repo_name')['time_period'].transform('max')\n",
    "    time_periods = df_repo_panel['time_period'].unique().tolist()\n",
    "    df_balanced = df_repo_panel[['repo_name']].drop_duplicates()\n",
    "    df_balanced['time_period'] = [time_periods for i in range(df_balanced.shape[0])]\n",
    "    df_balanced = df_balanced.explode('time_period')\n",
    "    df_repo_panel_full = pd.merge(df_balanced, df_repo_panel, how = 'left')\n",
    "    df_repo_panel_full[['first_period','final_period']] = df_repo_panel_full.groupby(['repo_name'])[['first_period','final_period']].ffill()\n",
    "    df_repo_panel_full = df_repo_panel_full.query('time_period >= first_period & time_period <= final_period')\n",
    "\n",
    "    return df_repo_panel_full.drop(['first_period','final_period'], axis = 1).sort_values(['repo_name','time_period'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "52eb669c-94cc-4f29-8641-4037a32ca47f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_repo_characteristics = reduce(lambda left, right: left.merge(right, how='outer'),\n",
    "    [\n",
    "        df_stars,\n",
    "        df_fork,\n",
    "        df_license_indicators.assign(\n",
    "            license=df_license_indicators['license'].apply(lambda x: \"|\".join(x) if isinstance(x, list) and x else np.nan)\n",
    "        ),\n",
    "        df_creation_date,\n",
    "        df_truckfactor_repo_summary,\n",
    "        df_contributor_count\n",
    "    ]\n",
    ")\n",
    "\n",
    "df_repo_characteristics = MakeBalanced(df_repo_characteristics)\n",
    "df_repo_characteristics.columns = df_repo_characteristics.columns.str.lower()\n",
    "\n",
    "def FillRepoCharacteristics(df):\n",
    "    na_zero_cols = ['stars_gained', 'forks_gained', 'contributors']\n",
    "    df[na_zero_cols] = df[na_zero_cols].fillna(0)\n",
    "    \n",
    "    fill_cols = [\n",
    "        'license', 'apache_software_license', 'bsd_license',\n",
    "        'gnu_general_public_license', 'mit_license', 'other_license',\n",
    "        'created_at', 'created_year'\n",
    "    ]\n",
    "    df[fill_cols] = df.groupby('repo_name')[fill_cols].transform(lambda x: x.ffill().bfill())\n",
    "    \n",
    "    truck_cols = ['truckfactor', 'corporate_pct', 'educational_pct']\n",
    "    df[truck_cols] = df.groupby('repo_name')[truck_cols].transform('ffill')\n",
    "    return df\n",
    "df_repo_characteristics = FillRepoCharacteristics(df_repo_characteristics)\n",
    "df_repo_characteristics = df_repo_characteristics.query('time_period>=\"2015-01-01\"')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "6cf1002c-8a9e-4da9-9b08-60d71ba99b3d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>repo_name</th>\n",
       "      <th>time_period</th>\n",
       "      <th>stars_gained</th>\n",
       "      <th>forks_gained</th>\n",
       "      <th>license</th>\n",
       "      <th>apache_software_license</th>\n",
       "      <th>bsd_license</th>\n",
       "      <th>gnu_general_public_license</th>\n",
       "      <th>mit_license</th>\n",
       "      <th>other_license</th>\n",
       "      <th>created_at</th>\n",
       "      <th>created_year</th>\n",
       "      <th>truckfactor</th>\n",
       "      <th>corporate_pct</th>\n",
       "      <th>educational_pct</th>\n",
       "      <th>contributors</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>007gzs/django_restframework_apiview</td>\n",
       "      <td>2017-01-01</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>GNU General Public License</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2017-06-27 05:05:52</td>\n",
       "      <td>2017.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>007gzs/django_restframework_apiview</td>\n",
       "      <td>2017-07-01</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>GNU General Public License</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2017-06-27 05:05:52</td>\n",
       "      <td>2017.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>007gzs/django_restframework_apiview</td>\n",
       "      <td>2018-01-01</td>\n",
       "      <td>18.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>GNU General Public License</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2017-06-27 05:05:52</td>\n",
       "      <td>2017.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>007gzs/django_restframework_apiview</td>\n",
       "      <td>2018-07-01</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>GNU General Public License</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2017-06-27 05:05:52</td>\n",
       "      <td>2017.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>007gzs/django_restframework_apiview</td>\n",
       "      <td>2019-01-01</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>GNU General Public License</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2017-06-27 05:05:52</td>\n",
       "      <td>2017.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>616569</th>\n",
       "      <td>zzzsochi/trans</td>\n",
       "      <td>2020-01-01</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2011-11-29 07:08:52</td>\n",
       "      <td>2011.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>616570</th>\n",
       "      <td>zzzsochi/trans</td>\n",
       "      <td>2020-07-01</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2011-11-29 07:08:52</td>\n",
       "      <td>2011.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>616571</th>\n",
       "      <td>zzzsochi/trans</td>\n",
       "      <td>2021-01-01</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2011-11-29 07:08:52</td>\n",
       "      <td>2011.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>616578</th>\n",
       "      <td>zzzsochi/trans</td>\n",
       "      <td>2021-07-01</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2011-11-29 07:08:52</td>\n",
       "      <td>2011.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>616579</th>\n",
       "      <td>zzzsochi/trans</td>\n",
       "      <td>2022-01-01</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2011-11-29 07:08:52</td>\n",
       "      <td>2011.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>135112 rows Ã— 16 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                  repo_name time_period  stars_gained  \\\n",
       "0       007gzs/django_restframework_apiview  2017-01-01           2.0   \n",
       "1       007gzs/django_restframework_apiview  2017-07-01           0.0   \n",
       "2       007gzs/django_restframework_apiview  2018-01-01          18.0   \n",
       "3       007gzs/django_restframework_apiview  2018-07-01           1.0   \n",
       "4       007gzs/django_restframework_apiview  2019-01-01           1.0   \n",
       "...                                     ...         ...           ...   \n",
       "616569                       zzzsochi/trans  2020-01-01           0.0   \n",
       "616570                       zzzsochi/trans  2020-07-01           1.0   \n",
       "616571                       zzzsochi/trans  2021-01-01           0.0   \n",
       "616578                       zzzsochi/trans  2021-07-01           0.0   \n",
       "616579                       zzzsochi/trans  2022-01-01           1.0   \n",
       "\n",
       "        forks_gained                     license  apache_software_license  \\\n",
       "0                0.0  GNU General Public License                      0.0   \n",
       "1                0.0  GNU General Public License                      0.0   \n",
       "2                0.0  GNU General Public License                      0.0   \n",
       "3                0.0  GNU General Public License                      0.0   \n",
       "4                1.0  GNU General Public License                      0.0   \n",
       "...              ...                         ...                      ...   \n",
       "616569           1.0                         NaN                      0.0   \n",
       "616570           1.0                         NaN                      0.0   \n",
       "616571           0.0                         NaN                      0.0   \n",
       "616578           0.0                         NaN                      0.0   \n",
       "616579           0.0                         NaN                      0.0   \n",
       "\n",
       "        bsd_license  gnu_general_public_license  mit_license  other_license  \\\n",
       "0               0.0                         1.0          0.0            0.0   \n",
       "1               0.0                         1.0          0.0            0.0   \n",
       "2               0.0                         1.0          0.0            0.0   \n",
       "3               0.0                         1.0          0.0            0.0   \n",
       "4               0.0                         1.0          0.0            0.0   \n",
       "...             ...                         ...          ...            ...   \n",
       "616569          0.0                         0.0          0.0            0.0   \n",
       "616570          0.0                         0.0          0.0            0.0   \n",
       "616571          0.0                         0.0          0.0            0.0   \n",
       "616578          0.0                         0.0          0.0            0.0   \n",
       "616579          0.0                         0.0          0.0            0.0   \n",
       "\n",
       "                created_at  created_year  truckfactor  corporate_pct  \\\n",
       "0      2017-06-27 05:05:52        2017.0          1.0            0.0   \n",
       "1      2017-06-27 05:05:52        2017.0          1.0            0.0   \n",
       "2      2017-06-27 05:05:52        2017.0          1.0            0.0   \n",
       "3      2017-06-27 05:05:52        2017.0          1.0            0.0   \n",
       "4      2017-06-27 05:05:52        2017.0          1.0            0.0   \n",
       "...                    ...           ...          ...            ...   \n",
       "616569 2011-11-29 07:08:52        2011.0          1.0            0.0   \n",
       "616570 2011-11-29 07:08:52        2011.0          1.0            0.0   \n",
       "616571 2011-11-29 07:08:52        2011.0          1.0            0.0   \n",
       "616578 2011-11-29 07:08:52        2011.0          1.0            0.0   \n",
       "616579 2011-11-29 07:08:52        2011.0          1.0            0.0   \n",
       "\n",
       "        educational_pct  contributors  \n",
       "0                   0.0           2.0  \n",
       "1                   0.0           1.0  \n",
       "2                   0.0           1.0  \n",
       "3                   0.0           0.0  \n",
       "4                   0.0           2.0  \n",
       "...                 ...           ...  \n",
       "616569              0.0           0.0  \n",
       "616570              0.0           0.0  \n",
       "616571              0.0           0.0  \n",
       "616578              0.0           0.0  \n",
       "616579              0.0           0.0  \n",
       "\n",
       "[135112 rows x 16 columns]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_repo_characteristics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26ecd9c8-aee3-4f39-99f6-24b70f3546a3",
   "metadata": {},
   "source": [
    "## Individual Characteristics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "666fbbe6-bc8e-457a-9343-37b4dc167bd3",
   "metadata": {},
   "source": [
    "**Email domain**: Whether in a project-contributor-time period, an individual made a commit with an educational or corporate email domain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cfb3b75e-e92f-4a5d-81fc-345b2b70cb6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_contributor_email_category = df_truckfactor_agg.assign(company_bool = ~df_truckfactor_agg['company'].isna())\\\n",
    "    .groupby(['repo_name','actor_id','time_period'])\\\n",
    "    .agg(contributor_email_educational = ('educational','max'), \n",
    "         contributor_email_corporate = ('company_bool','max')).reset_index()\n",
    "df_contributor_email_category[['contributor_email_educational','contributor_email_corporate']] = df_contributor_email_category[['contributor_email_educational','contributor_email_corporate']].astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa5c026e-23f9-4c82-9bdd-69e401b909b3",
   "metadata": {},
   "source": [
    "1) Max Rank for each individual in each period as a continuous rank variable\n",
    "2) Rank indicator\n",
    "3) % for each rank that they handled\n",
    "\n",
    "df_contributor_panel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b383dc67-2719-4204-81f7-5e4f2c0a1b1c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def ContributorRankCharacteristics(df_contributor_panel):\n",
    "    val_cols = [col for col in df_contributor_panel.columns if 'pct' in col]\n",
    "    df = df_contributor_panel.drop(columns=val_cols).rename(columns={'issue_number': 'issues_opened', 'pr': 'prs_opened'}).reset_index(drop=True)\n",
    "    \n",
    "    df['problem_identification_sum'] = df['issues_opened']\n",
    "    df['problem_identification'] = df['problem_identification_sum'] > 0\n",
    "    \n",
    "    df['problem_discussion_sum'] = df[['comments', 'own_pr_comments', 'helping_pr_comments', 'pr_reviews', 'pr_review_comments']].sum(axis=1)\n",
    "    df['problem_discussion'] = df['problem_discussion_sum'] > 0\n",
    "    \n",
    "    df['coding_sum'] = df[['prs_opened', 'commits']].sum(axis=1)\n",
    "    df['coding'] = df['coding_sum'] > 0\n",
    "    \n",
    "    df['problem_approval_sum'] = df[['prs_merged', 'issues_closed']].sum(axis=1)\n",
    "    df['problem_approval'] = df['problem_approval_sum'] > 0\n",
    "    \n",
    "    df['max_rank'] = np.select(\n",
    "        [\n",
    "            df['problem_approval'],\n",
    "            df['coding'],\n",
    "            df['problem_discussion']\n",
    "        ],\n",
    "        [4, 3, 2],\n",
    "        default=1\n",
    "    )\n",
    "    agg_cols = ['problem_identification', 'problem_discussion', 'coding', 'problem_approval']\n",
    "    sum_cols = [f'{c}_sum' for c in agg_cols]\n",
    "    grouped_sums = df.groupby(['repo_name', 'time_period'])[sum_cols].transform('sum')\n",
    "    for col in sum_cols:\n",
    "        df[f'{col.replace(\"_sum\",\"\")}_share'] = df[col] / grouped_sums[col]\n",
    "    df['total_share'] = df[sum_cols].sum(axis = 1) / grouped_sums.sum(axis = 1)\n",
    "    rank_activity_cols = [\n",
    "        'issues_opened', 'problem_identification_sum', 'comments', 'own_pr_comments',\n",
    "        'helping_pr_comments', 'pr_reviews', 'pr_review_comments', 'problem_discussion_sum',\n",
    "        'prs_opened', 'commits', 'coding_sum', 'prs_merged', 'issues_closed',\n",
    "        'problem_approval_sum'\n",
    "    ]\n",
    "    final_cols = ['actor_id', 'repo_name', 'time_period', 'max_rank'] + agg_cols + [f'{c}_share' for c in agg_cols] + rank_activity_cols + ['total_share']\n",
    "    return df[final_cols]\n",
    "\n",
    "df_contributor_panel_rank = ContributorRankCharacteristics(df_contributor_panel)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff8ab5c1-ab0b-4e71-8dea-21c0cbe54854",
   "metadata": {},
   "source": [
    "**Truckfactor Member**: Indicator variable for whether in that time period they were part of the truckfactor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d6591658-b16f-4007-8a18-efcd3cc15499",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_truckfactor_dates = df_truckfactor_agg[['repo_name','time_period','actor_id']].assign(\n",
    "    truckfactor_member = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8bcd85d-3158-48b7-8385-ab06dbd02303",
   "metadata": {},
   "source": [
    "**% of LOC authored**: Using the git blame %, identify the % of all code that the contributor had authored in the codebase in that project-time period \n",
    "\n",
    "<span style=\"color:red\">To Do</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fa3f71e-5875-4e35-b7a1-618f36d35f7c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2b894435-d1d0-4675-be13-2fa56f642d36",
   "metadata": {},
   "source": [
    "- **Work Share**: For each project-contributor-time period, in **(1) discussions** or **(2) pull requests**, what % of the work did they do? <span style=\"color:blue\">Note that for the three below, one problem can reappear in multiple time periods if work is done on the same problem in multiple periods. I may want to experiment with alternatives, such as counting it all in the original period</span>\n",
    "\n",
    "  - Share, average: $\\frac{1}{|P_i|} \\sum_{p \\in P_i}^n S_{p, i}$ where $P_i$ contains all problems $p$ (comments or pull requests) contributor $i$ was involved in and $S_{p, i}$ represents the share of work contributor $i$ did in problem $p$\n",
    "  - Share, average weighted by work: $\\sum_{p \\in P_i}^n S_{p, i} \\cdot \\frac{C_{p,i}}{\\sum_{p \\in P_i} C_{p,i}}$  where $C_{p, i}$ represents the quantity of work contributor $i$ did in problem $p$. Now, instead of weighting each problem equally, we weight each problem $p$ by how much work contributor $i$ did in problem $p$\n",
    "- **Individual HHI**: For each project-contributor-time period, in **(1) discussions** or **(2) pull requests** that they were involved in, what was the HHI? This tells us how balanced the work was when they were involved\n",
    "  - Share, average: $\\frac{1}{|P_i|} \\sum_{p \\in P_i}^n HHI_{p}$ where $P_i$ contains all problems $p$ (comments or pull requests) contributor $i$ was involved in and $HHI_{p}$ represents the HHI of problem $p$\n",
    "  - Share, average weighted by work: $\\sum_{p \\in P_i}^n HHI_{p} \\cdot \\frac{C_{p,i}}{\\sum_{p \\in P_i} C_{p,i}}$ where $C_{p, i}$ represents the quantity of work contributor $i$ did in problem $p$. Now, instead of weighting each problem equally, we weight each problem $p$ by how much work contributor $i$ did in problem $p$\n",
    "- **Cooperation**: % of **(1) discussions** or **(2) pull requests** that they were involved in that had another contributor besides them\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "11a050ba-be73-401d-b52f-b2f0115a80c5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "indir_export = Path('drive/output/derived/data_export')\n",
    "df_issue = pd.read_parquet(indir_export / 'df_issue.parquet')\n",
    "df_issue = ImputeTimePeriod(df_issue, time_period)\n",
    "\n",
    "df_pr = pd.read_parquet(indir_export / 'df_pr.parquet')\n",
    "df_pr = ImputeTimePeriod(df_pr, time_period)\n",
    "\n",
    "df_pr_commits = pd.read_parquet(indir_export / 'df_pr_commits.parquet')\n",
    "\n",
    "def IndivLevelComments(df_issue, df_pr):\n",
    "    df_issue_comments = df_issue.query('type == \"IssueCommentEvent\"').sort_values('created_at').drop_duplicates('issue_comment_id')\n",
    "    df_pr_rc = df_pr.query('type == \"PullRequestReviewCommentEvent\"')\n",
    "    df_pr_rc['issue_number'] =  df_pr_rc['pr_number']\n",
    "    df_pr_rc = df_pr_rc.sort_values('created_at').drop_duplicates('pr_review_comment_body')\n",
    "    \n",
    "    df_comments_actor = pd.concat([df_pr_rc[['repo_name','actor_id','time_period','issue_number','created_at']],\n",
    "                                   df_issue_comments[['repo_name','actor_id','time_period','issue_number','created_at']]])\\\n",
    "        .groupby(['repo_name','actor_id','time_period','issue_number'])['created_at'].count()\\\n",
    "        .reset_index().rename({'created_at':'comments'}, axis = 1)\n",
    "    return df_comments_actor\n",
    "\n",
    "def IndivLevelCommits(df_pr_commits):\n",
    "    df_pr_commits['created_at'] = pd.to_datetime(df_pr_commits['commit time'], unit = 's')\n",
    "    df_pr_commits = df_pr_commits.query('~created_at.isna()')\n",
    "    df_pr_commits = ImputeTimePeriod(df_pr_commits, time_period)\n",
    "    df_committers_ids = df_committers_clean.explode('commit_name').explode('email_address')[['commit_name','email_address','actor_id']].drop_duplicates()\n",
    "    df_pr_commits_id = pd.merge(df_pr_commits, df_committers_ids, how = 'left', left_on = ['commit author name','commit author email'], right_on = ['commit_name','email_address'])\n",
    "    df_pr_commits_id['commits'] = 1\n",
    "    df_pr_commits_id['commits_100'] = (df_pr_commits_id['commit files changed count']<100).astype(int)\n",
    "\n",
    "    #### NOT GOING TO WEIGHT BY ADDITIONS, DELETIONS FOR NOW BUT GOOD TO HAVE\n",
    "    df_pr_commits_actor = df_pr_commits_id.query('~actor_id.isna()').groupby(['repo_name','time_period','actor_id','pr_number']).agg(\n",
    "        commits_count = ('commits','sum'),\n",
    "        commits_100_count = ('commits_100','sum'),\n",
    "        commits_add_sum = ('commit additions','sum'),\n",
    "        commits_del_sum = ('commit deletions','sum'),\n",
    "        commits_change_sum = ('commit changes total','sum'),\n",
    "        commits_files_changed_count = ('commit files changed count','sum')).reset_index()\n",
    "        \n",
    "    return df_pr_commits_actor\n",
    "\n",
    "def AggregateHHICount(df, id_col, count_col):\n",
    "    indiv_total_col = f'indiv_total_{count_col}'\n",
    "    discussions_col = f'{count_col}_discussions_involved'\n",
    "    total_col = f'total_{count_col}'\n",
    "    share_col = f'{count_col}_share'\n",
    "    hhi_col = f'{count_col}_hhi'\n",
    "    indiv_col = f'{count_col}_indiv'\n",
    "    wm = lambda x: np.average(x, weights=df.loc[x.index, count_col])\n",
    "\n",
    "    df[discussions_col] = df.groupby(['repo_name','time_period','actor_id'])[count_col].transform('count')\n",
    "    df[indiv_total_col] = df.groupby(['repo_name','time_period','actor_id'])[count_col].transform('sum')\n",
    "    df[total_col] = df.groupby(['repo_name','time_period',id_col])[count_col].transform('sum')\n",
    "    df[share_col] = df[count_col]/df[total_col]\n",
    "    df[hhi_col] = df.assign(share_sq = lambda x: x[share_col]**2)\\\n",
    "        .groupby(['repo_name','time_period',id_col])['share_sq'].transform('sum')\n",
    "    df[indiv_col] = (df[share_col]<1).astype(int)\n",
    "    df[f'{share_col}_wt'] = df[share_col] * df[count_col]/df[indiv_total_col]\n",
    "    df[f'{hhi_col}_wt'] = df[hhi_col] * df[count_col]/df[indiv_total_col]\n",
    "    \n",
    "    df_grouped = df.groupby(['repo_name','actor_id','time_period',indiv_total_col,discussions_col]).agg(\n",
    "        **{f'{share_col}_avg': (share_col, 'mean'),\n",
    "           f'{share_col}_avg_wt': (f'{share_col}_wt', 'sum'),\n",
    "           f'{hhi_col}_avg': (hhi_col, 'mean'),\n",
    "           f'{hhi_col}_avg_wt': (f'{hhi_col}_wt', 'sum'),\n",
    "           f'pct_cooperation_{count_col}': (indiv_col, 'mean')}).reset_index()\n",
    "    \n",
    "    return df_grouped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1f9b4c05-6fa2-4042-8c97-e57a3be6d9e8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_comments_actor = IndivLevelComments(df_issue, df_pr)\n",
    "df_pr_commits_actor = IndivLevelCommits(df_pr_commits)\n",
    "\n",
    "df_comments_grouped = AggregateHHICount(df_comments_actor, 'issue_number', 'comments')\n",
    "\n",
    "df_pr_commits_grouped = AggregateHHICount(df_pr_commits_actor, 'pr_number', 'commits_count')\n",
    "df_pr_commits_grouped = df_pr_commits_grouped.rename({'commits_count_discussions_involved':'pr_discussions_involved'}, axis = 1)\n",
    "\n",
    "df_pr_commits_100_grouped = AggregateHHICount(df_pr_commits_actor, 'pr_number', 'commits_100_count')\n",
    "df_pr_commits_100_grouped = df_pr_commits_100_grouped.rename({'commits_100_count_discussions_involved':'100_pr_discussions_involved'}, axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6b15992-0c13-40d0-8ffa-414ec938dea9",
   "metadata": {},
   "source": [
    "### Combine Covariates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0085f4e8-5218-4dc9-8d26-c47d239a0303",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def MakeBalancedContributor(df_contributor_panel):\n",
    "    df_contributor_panel['first_period'] = df_contributor_panel.groupby(['repo_name','actor_id'])['time_period'].transform('min')\n",
    "    df_contributor_panel['final_period'] = df_contributor_panel.groupby(['repo_name','actor_id'])['time_period'].transform('max')\n",
    "    time_periods = df_contributor_panel['time_period'].unique().tolist()\n",
    "    df_balanced = df_contributor_panel[['repo_name','actor_id','first_period','final_period']].drop_duplicates()\n",
    "    df_balanced['time_period'] = [time_periods for i in range(df_balanced.shape[0])]\n",
    "    df_balanced = df_balanced.explode('time_period')\n",
    "    df_balanced = df_balanced.query('time_period >= first_period & time_period <= final_period')\n",
    "    df_contributor_panel_full = pd.merge(df_balanced, df_contributor_panel, how = 'left')\n",
    "\n",
    "    return df_contributor_panel_full.drop(['first_period','final_period'], axis = 1).sort_values(['repo_name','actor_id','time_period'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "621d6e43-35b4-427d-8400-39a7b2246be9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "agg_cols = ['problem_identification', 'problem_discussion', 'coding', 'problem_approval']\n",
    "df_contributor_panel_share = df_contributor_panel_rank[['actor_id', 'repo_name', 'time_period', 'max_rank'] + [f'{c}_share' for c in agg_cols]]\n",
    "\n",
    "merged_dfs = [\n",
    "    df_contributor_email_category,\n",
    "    df_truckfactor_dates,\n",
    "    df_contributor_panel_share,\n",
    "    df_comments_grouped,\n",
    "    df_pr_commits_grouped,\n",
    "    df_pr_commits_100_grouped\n",
    "]\n",
    "\n",
    "df_contributor_characteristics = reduce(lambda left, right: left.merge(right, how='outer'), merged_dfs)\n",
    "df_contributor_characteristics = MakeBalancedContributor(df_contributor_characteristics)\n",
    "df_contributor_characteristics[['contributor_email_educational', 'contributor_email_corporate', 'truckfactor_member']] = df_contributor_characteristics[['contributor_email_educational', 'contributor_email_corporate', 'truckfactor_member']].fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6444acd8-52f7-47b3-9e06-25c22997f799",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_contributor_characteristics = df_contributor_characteristics.query('time_period>=\"2015-01-01\"')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d7cb2ff-14fa-4df4-b115-7ac9e073489b",
   "metadata": {},
   "source": [
    "## Organizational Structure"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ece4718a-3820-4526-87cc-55d51f6c83bd",
   "metadata": {},
   "source": [
    "**Layer Count**: Classify people into (at least) one of the four layers below, and for each layer, in that repo time-period, consider the quantity of contributors in that layer and the percentage of contributors who participated in that layer.  \n",
    "\n",
    "**List of layers**\n",
    "  - **Problem identification (Layer 1)**: Opened issue\n",
    "  - **Problem discussion (Layer 2)**: Issue Comments + PR Comments + PR Review Comments\n",
    "  - **Code writing (Layer 3)**: Commits in pushes, linked PRs or unlinked PRs (only include merged PRs)\n",
    "  - **Problem approval (Layer 4)**: Closed issue + merged PR\n",
    "\n",
    "**Min Layers**: Find the smallest subset of layers such that all contributors belong to at least one of the layers in the subset. All layers in the smallest subset are separated by \"|\" and if there are multiple smallest subsets, they are separated by \":\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c9e0811f-c203-4a12-b48f-26d68d22d5c7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "LAYERS = agg_cols\n",
    "def find_all_min_covers_for_group(grp):\n",
    "    \"\"\"\n",
    "    Given the subset of the DataFrame for one (repo_name, time_period),\n",
    "    return ALL minimal subsets of LAYERS that cover every actor in this group.\n",
    "\n",
    "    Cover criterion:\n",
    "      For each row (actor), at least one column in the chosen subset is True.\n",
    "    \"\"\"\n",
    "    # We'll just slice the boolean columns\n",
    "    subset_matrix = grp[LAYERS]\n",
    "\n",
    "    # We'll track all minimal subsets found\n",
    "    # If we never find any, it returns an empty list (though that likely won't happen\n",
    "    # if there's at least one True in each group).\n",
    "    min_solutions = []\n",
    "\n",
    "    # 1) We try subsets of size 1, then size 2, etc., up to size 4\n",
    "    # 2) Once we find ANY solutions for a given size r, we stop (those are minimal).\n",
    "    for r in range(1, len(LAYERS) + 1):\n",
    "        possible_combos = itertools.combinations(LAYERS, r)\n",
    "        found_any = False\n",
    "\n",
    "        for combo in possible_combos:\n",
    "            # If every row is covered by 'combo', we keep it\n",
    "            # \"covered\" means row-wise .any(axis=1) is True for all rows\n",
    "            if subset_matrix[list(combo)].any(axis=1).all():\n",
    "                min_solutions.append(combo)\n",
    "                found_any = True\n",
    "\n",
    "        if found_any:\n",
    "            # We found at least one solution of size r, so there's no need\n",
    "            # to look at larger subsets.\n",
    "            break\n",
    "\n",
    "    return pd.Series({'all_min_layers': min_solutions})\n",
    "\n",
    "df_panel_min_layers = df_contributor_panel_rank\\\n",
    "    .groupby(['repo_name', 'time_period'], as_index=False)\\\n",
    "    .parallel_apply(find_all_min_covers_for_group)\n",
    "df_panel_min_layers['min_layer_count'] = df_panel_min_layers['all_min_layers'].apply(lambda x: len(x[0]))\n",
    "df_panel_min_layers['all_min_layers'] = df_panel_min_layers['all_min_layers'].apply(lambda x: [\"|\".join(sorted(list(ele))) for ele in x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "464173fd-a982-4693-a9a9-af5c274b2ba2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def GetPanelRankCount(df_contributor_panel_rank, agg_cols):\n",
    "    df_panel_rank_count = df_contributor_panel_rank.groupby(['repo_name','time_period'])[agg_cols].sum()\n",
    "    df_panel_rank_count.columns = [f\"{col}_layer_contributor_count\" for col in df_panel_rank_count.columns]\n",
    "    \n",
    "    df_panel_rank_pct = df_contributor_panel_rank.groupby(['repo_name','time_period'])[agg_cols].mean()\n",
    "    df_panel_rank_pct.columns = [f\"{col}_layer_contributor_pct\" for col in df_panel_rank_pct.columns]\n",
    "    \n",
    "    df_panel_rank_count_pct = df_panel_rank_pct.join(df_panel_rank_count, how = 'outer').reset_index()\n",
    "    df_panel_rank = df_panel_rank_pct.join(df_panel_rank_count, how = 'outer').reset_index()\n",
    "    \n",
    "    df_panel_layer_count = (df_panel_rank_count>0).sum(axis = 1).reset_index().rename({0:'layer_count'}, axis = 1)\n",
    "    df_panel_rank = pd.merge(df_panel_rank, df_panel_layer_count)\n",
    "    \n",
    "    return df_panel_rank\n",
    "\n",
    "df_panel_rank = GetPanelRankCount(df_contributor_panel_rank, agg_cols)\n",
    "df_panel_rank_layers = pd.merge(df_panel_rank,df_panel_min_layers, how = 'outer')\n",
    "df_panel_rank_layers['all_min_layers'] = df_panel_rank_layers['all_min_layers'].apply(lambda x: \":\".join(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cda8e483-6d70-4c5c-9ad9-d10592f0552e",
   "metadata": {},
   "source": [
    "**Layer Overlap**\n",
    "  1. **Overlap Contributor Pct**: For each layer $i \\leq 4$ and layer $i < j \\leq 4$, what % of individuals are in layer $i$ and $j$. Mathematically, this is $\\frac{|\\{P \\in L_i \\cap L_j\\}|}{|\\{P \\in L_i\\}|}$.\n",
    "  2. **Degree of overlap**: For each layer $i \\leq 4$ and layer $i < j \\leq 4$, what % of work in the lower layer $i$ is done by individuals who are also in layer $j$: $\\frac{\\sum_{P \\in L_i \\cap L_j} W_P}{\\sum_{P \\in L_i} W_P}$\n",
    " \n",
    " The combinations are:\n",
    " - Problem Identification-Problem Discussion\n",
    " - Problem Identification-Coding\n",
    " - Problem Identification-Problem Approval\n",
    " - Problem Discussion-Coding\n",
    " - Problem Discussion-Problem Approval\n",
    " - Coding-Problem Approval\n",
    " \n",
    " I also calculate, for the degree of overlap, the % of work done by people who are in any higher layer.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "590c2836-a2af-4a62-bdbb-de2607790f61",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "def CalculateContributorOverlapTwoLayers(df_contributor_panel_rank, agg_cols):\n",
    "    df_repo_panel_overlap = df_contributor_panel_rank[['repo_name', 'time_period']].drop_duplicates().reset_index(drop=True)\n",
    "    for lower_layer, higher_layer in itertools.combinations(agg_cols, 2):\n",
    "        overlap = (\n",
    "            df_contributor_panel_rank\n",
    "            .query(f'({lower_layer} == True)')\n",
    "            .groupby(['repo_name', 'time_period'])[higher_layer]\n",
    "            .mean()\n",
    "            .reset_index()\n",
    "            .rename(columns={higher_layer: f\"{lower_layer}_{higher_layer}_contributor_overlap\"})\n",
    "        )\n",
    "        df_repo_panel_overlap = pd.merge(df_repo_panel_overlap, overlap, on=['repo_name', 'time_period'], how='left')\n",
    "        lower_higher_numerator = (\n",
    "            df_contributor_panel_rank\n",
    "            .query(f'({lower_layer} == True) & ({higher_layer} == True)')\n",
    "            .groupby(['repo_name', 'time_period'])[f\"{lower_layer}_sum\"]\n",
    "            .sum()\n",
    "            .reset_index()\n",
    "            .rename(columns={f\"{lower_layer}_sum\": f\"{lower_layer}_{higher_layer}_work_numerator\"})\n",
    "        )\n",
    "        lower_higher_denominator = (\n",
    "            df_contributor_panel_rank\n",
    "            .query(f'({lower_layer} == True)')\n",
    "            .groupby(['repo_name', 'time_period'])[f\"{lower_layer}_sum\"]\n",
    "            .sum()\n",
    "            .reset_index()\n",
    "            .rename(columns={f\"{lower_layer}_sum\": f\"{lower_layer}_{higher_layer}_work_denominator\"})\n",
    "        )\n",
    "        work_done = pd.merge(\n",
    "            lower_higher_numerator, \n",
    "            lower_higher_denominator, \n",
    "            on=['repo_name', 'time_period'], \n",
    "            how='right'  # Ensure all groups with lower_layer=True are included\n",
    "        )\n",
    "        work_done[f\"{lower_layer}_{higher_layer}_work\"] = work_done.apply(\n",
    "            lambda row: (\n",
    "                row[f\"{lower_layer}_{higher_layer}_work_numerator\"] / \n",
    "                row[f\"{lower_layer}_{higher_layer}_work_denominator\"]\n",
    "            ) if row[f\"{lower_layer}_{higher_layer}_work_denominator\"] > 0 and row[f\"{lower_layer}_{higher_layer}_work_numerator\"] > 0 else np.nan,\n",
    "            axis=1\n",
    "        )\n",
    "        work_done = work_done[['repo_name', 'time_period', f\"{lower_layer}_{higher_layer}_work\"]]\n",
    "        df_repo_panel_overlap = pd.merge(df_repo_panel_overlap, work_done, on=['repo_name', 'time_period'], how='left')\n",
    "\n",
    "    return df_repo_panel_overlap\n",
    "\n",
    "def CalculateContributorOverlap(df_contributor_panel_rank, agg_cols):\n",
    "    df_repo_panel_overlap = df_contributor_panel_rank[['repo_name', 'time_period']].drop_duplicates().reset_index(drop=True)\n",
    "    for i in range(len(agg_cols))[:-1]:\n",
    "        lower_layer = agg_cols[i]\n",
    "        higher_layer = agg_cols[(i+1):]\n",
    "        higher_layer_query = \" | \".join([f\"{col} == True\" for col in higher_layer])\n",
    "\n",
    "        overlap = (\n",
    "            df_contributor_panel_rank.assign(higher_layer = df_contributor_panel_rank[higher_layer].max(axis = 1))\n",
    "            .query(f'({lower_layer} == True)')\n",
    "            .groupby(['repo_name', 'time_period'])['higher_layer']\n",
    "            .mean()\n",
    "            .reset_index()\n",
    "            .rename(columns={'higher_layer': f\"{lower_layer}_higher_layer_contributor_overlap\"})\n",
    "        )\n",
    "        df_repo_panel_overlap = pd.merge(df_repo_panel_overlap, overlap, on=['repo_name', 'time_period'], how='left')\n",
    "\n",
    "        lower_higher_numerator = (\n",
    "            df_contributor_panel_rank\n",
    "            .query(f'({lower_layer} == True) & ({higher_layer_query})')\n",
    "            .groupby(['repo_name', 'time_period'])[f\"{lower_layer}_sum\"]\n",
    "            .sum()\n",
    "            .reset_index()\n",
    "            .rename(columns={f\"{lower_layer}_sum\": f\"{lower_layer}_higher_layer_work_numerator\"})\n",
    "        )\n",
    "        lower_higher_denominator = (\n",
    "            df_contributor_panel_rank\n",
    "            .query(f'({lower_layer} == True)')\n",
    "            .groupby(['repo_name', 'time_period'])[f\"{lower_layer}_sum\"]\n",
    "            .sum()\n",
    "            .reset_index()\n",
    "            .rename(columns={f\"{lower_layer}_sum\": f\"{lower_layer}_higher_layer_work_denominator\"})\n",
    "        )\n",
    "        work_done = pd.merge(\n",
    "            lower_higher_numerator, \n",
    "            lower_higher_denominator, \n",
    "            on=['repo_name', 'time_period'], \n",
    "            how='right'  # Ensure all groups with lower_layer=True are included\n",
    "        )\n",
    "        work_done[f\"{lower_layer}_higher_layer_work\"] = work_done.apply(\n",
    "            lambda row: (\n",
    "                row[f\"{lower_layer}_higher_layer_work_numerator\"] / \n",
    "                row[f\"{lower_layer}_higher_layer_work_denominator\"]\n",
    "            ) if row[f\"{lower_layer}_higher_layer_work_denominator\"] > 0 and row[f\"{lower_layer}_higher_layer_work_numerator\"] > 0 else np.nan,\n",
    "            axis=1\n",
    "        )\n",
    "        work_done = work_done[['repo_name', 'time_period', f\"{lower_layer}_higher_layer_work\"]]\n",
    "        df_repo_panel_overlap = pd.merge(df_repo_panel_overlap, work_done, on=['repo_name', 'time_period'], how='left')\n",
    "\n",
    "    return df_repo_panel_overlap\n",
    "\n",
    "\n",
    "df_repo_panel_overlap = CalculateContributorOverlap(df_contributor_panel_rank, agg_cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60ac2fd7-29bf-452a-a90b-70b3e1214279",
   "metadata": {},
   "source": [
    "**Layer HHI (Work Split)**\n",
    "  1. At the project-time period level, for all contributors $P$ who are involved in any layer $l_i$, $\\sum_{P \\in L_i} \\left(S_{P, L_i} \\right)^2$ where $S_{P,L_i}$ is the share of work that contributor $P$ did in layer $L_i$.\n",
    "  2. <span style=\"color:red\">TO DO: For each file $f$, using the git blame to determine the owner\n",
    "     1. HHI (by overall LOC across all files)\n",
    "     2. HHI (for each file, then average across all)</span>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6006f4d-845a-4393-8e0c-4e909835136c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "46311846-c2a7-4149-9a42-42c8b316bd0f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def CalculateLayerHHI(df_contributor_panel_rank, agg_cols):\n",
    "    df_repo_hhi = df_contributor_panel_rank[['repo_name', 'time_period']].drop_duplicates().reset_index(drop=True)\n",
    "\n",
    "    for layer in agg_cols + ['total']:\n",
    "        share_col = f\"{layer}_share\"\n",
    "        hhi_col = f\"{layer}_HHI\"\n",
    "        if layer != 'total':\n",
    "            df_contributor = df_contributor_panel_rank.query(f\"{layer} == True\")\n",
    "        else:\n",
    "            df_contributor = df_contributor_panel_rank\n",
    "        \n",
    "        hhi = (\n",
    "            df_contributor\n",
    "            .assign(share_sq = df_contributor_panel_rank[share_col]**2)\n",
    "            .groupby(['repo_name', 'time_period'])['share_sq']\n",
    "            .sum()\n",
    "            .reset_index()\n",
    "            .rename(columns={'share_sq': hhi_col})\n",
    "        )\n",
    "        \n",
    "        df_repo_hhi = pd.merge(df_repo_hhi, hhi, on=['repo_name', 'time_period'], how='left')\n",
    "    \n",
    "\n",
    "    \n",
    "    return df_repo_hhi\n",
    "df_repo_hhi = CalculateLayerHHI(df_contributor_panel_rank, agg_cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccd9f8c4-a561-4f7f-95c5-7449e43fe3dd",
   "metadata": {},
   "source": [
    "**Cooperation**:\n",
    "1. For each discussion (includes comments, PR reviews + PR review comments) + PR, I calculate, across all threads in a project-time period\n",
    "     1. **HHI:** $\\frac{1}{|T_p|} \\sum_{t \\in T_p}^n HHI_{t}$ where $T_i$ contains all threads $t$ (discussions or pull requests) in a project-time period and $HHI_{t}$ represents the HHI of thread t\n",
    "     2. **HHI, average weighted by discussion quantity:** $\\sum_{t \\in T_p}^n HHI_{t} \\cdot \\frac{C_{t}}{\\sum_{t \\in T_p} C_{t}}$ where $C_{t}$ represents the quantity of discussion done in thread t. Now, instead of weighting each problem equally, we weight each thread $t$ by how much work discussion occurred \n",
    "     3. **Average (across threads):** \\# of distinct individuals per thread\n",
    "     4. **Average (across threads, weighted by discussion quantity):** \\# of distinct individuals per thread, weighted by the same quantity as HHI, average weighted by discussion quantity\n",
    "     5. **Cooperation Occurred:** \\% of threads with more than one distinct individual\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "89e64bd2-03d6-4f8c-b04b-9353f8669908",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_hhi_cooperation(df, entity, metric):\n",
    "    metric_share = f\"{metric}_share\"               \n",
    "    entity_number = f\"{entity}_number\"             \n",
    "    metric_col = metric                            \n",
    "    total_metric = f\"total_{metric}\"               \n",
    "    repo_metric = f\"repo_{metric}\"                 \n",
    "    contributors_metric = f\"contributors_{metric}\"\n",
    "    contributors_metric_wt = f\"{contributors_metric}_wt\"\n",
    "    \n",
    "    hhi_col = f\"{entity}_hhi_{metric}\"             \n",
    "    cooperation_col = f\"{metric}_cooperation\"      \n",
    "    wt_work_done_col = f\"{entity}_{metric}_wt\"\n",
    "    \n",
    "    df[hhi_col] = df.assign(share_sq = df[metric_share] ** 2)\\\n",
    "        .groupby(['repo_name', 'time_period', entity_number])['share_sq'].transform('sum')\n",
    "    \n",
    "    df[cooperation_col] = df[hhi_col] != 1\n",
    "    df[repo_metric] = df.groupby(['repo_name', 'time_period'])[metric_col].transform('sum')\n",
    "    \n",
    "    df[wt_work_done_col] = df[hhi_col] * df[total_metric] / df[repo_metric]\n",
    "    \n",
    "    df[contributors_metric] = df.groupby(['repo_name', 'time_period', entity_number])[metric_share].transform('count')\n",
    "    df[contributors_metric_wt] = df[contributors_metric] * df[total_metric] / df[repo_metric]\n",
    "    \n",
    "    df_unique = df.drop_duplicates(['repo_name', 'time_period', entity_number])\n",
    "    \n",
    "    df_cooperation = df_unique.groupby(['repo_name', 'time_period']).agg(\n",
    "        hhi_metric = (hhi_col, 'mean'),\n",
    "        hhi_metric_wt = (wt_work_done_col, 'sum'),\n",
    "        contributors_metric = (contributors_metric, 'mean'),\n",
    "        contributors_metric_wt = (contributors_metric_wt, 'sum'),\n",
    "        metric_cooperation_pct = (cooperation_col, 'mean')\n",
    "    ).reset_index()\n",
    "    \n",
    "    df_cooperation = df_cooperation.rename(columns={\n",
    "        'hhi_metric': f\"hhi_{metric}\",\n",
    "        'hhi_metric_wt': f\"hhi_{metric}_wt\",\n",
    "        'contributors_metric':f\"contributors_{metric}\",\n",
    "        'contributors_metric_wt':f\"contributors_{metric}_wt\",\n",
    "        'metric_cooperation_pct': f\"{metric}_cooperation_pct\"\n",
    "    })\n",
    "    \n",
    "    \n",
    "    return df_cooperation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "23ea974c-c60a-4875-b541-6c88bc4444ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_comments_cooperation = calculate_hhi_cooperation(df_comments_actor, 'issue','comments')\n",
    "df_commits_cooperation = calculate_hhi_cooperation(df_pr_commits_actor, 'pr','commits_count')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e22b8b12-535b-43c1-9d1c-45dd602311a0",
   "metadata": {},
   "source": [
    "**communication**\n",
    "- Given a contributor's rank, what % of discussion occurs with people whose rank is (1), (2), (3), (4)\n",
    "  - get 1/x weight if they are in x ranks (for the % of discussion)\n",
    "  - Average weighted by contributions of person, or by person\n",
    "- Can aggregate for each contributor into a weighted average of the average rank they communicate with "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "dc74876b-1d76-4916-8820-0b0d82a75e52",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_comments_actor_rank = pd.merge(df_comments_actor, df_contributor_panel_rank[['actor_id','repo_name','time_period','max_rank'] + agg_cols].drop_duplicates(), how = 'left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "998c39a1-b449-49ae-a2f5-d38e54ec143f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def RankParticipationFlags(df, entity='issue'):\n",
    "    entity_number = f\"{entity}_number\"\n",
    "\n",
    "    ranks = [1, 2, 3, 4]\n",
    "\n",
    "    rank_cols = [f'rank_{rank}' for rank in ranks]\n",
    "    for rank in ranks:\n",
    "        df[f'rank_{rank}'] = (df['max_rank'] == rank).astype(int)\n",
    "    df['num_layers'] = df[rank_cols].sum()\n",
    "    \n",
    "    group_cols = ['repo_name', 'time_period', entity_number]\n",
    "    rank_presence = df.groupby(group_cols)[rank_cols].max().reset_index()\n",
    "\n",
    "    \n",
    "    participation_cols = [f'participated_rank_{rank}' for rank in ranks]\n",
    "    rename_dict = {f'rank_{rank}': f'participated_rank_{rank}' for rank in ranks}\n",
    "    rank_presence = rank_presence.rename(columns=rename_dict)\n",
    "\n",
    "    df = df.merge(rank_presence, on=group_cols, how='left')\n",
    "    df = df.drop(columns=rank_cols)\n",
    "\n",
    "    return df\n",
    "\n",
    "def calculate_rank_metrics(df, entity_number='issue_number', metric='comments', ranks=[1,2,3,4],\n",
    "                           weight_column='comments_discussions_involved'):\n",
    "\n",
    "    df = RankParticipationFlags(df)\n",
    "\n",
    "    # issue by issue average\n",
    "    df_wt_by_issue = df.drop_duplicates(['repo_name','time_period',entity_number,'max_rank']).groupby(['repo_name','time_period','max_rank']).agg(\n",
    "        participated_rank_1 = ('participated_rank_1','mean'),\n",
    "        participated_rank_2 = ('participated_rank_2','mean'),\n",
    "        participated_rank_3 = ('participated_rank_3','mean'),\n",
    "        participated_rank_4 = ('participated_rank_4','mean'),\n",
    "        discussion_count = (entity_number,'nunique')\n",
    "    )\n",
    "\n",
    "    df_rank_count = df.drop_duplicates(['repo_name','time_period','max_rank','actor_id']).groupby(\n",
    "        ['repo_name','time_period','max_rank'])['actor_id'].count().rename('rank_contributor_count')\n",
    "    \n",
    "    df_rank_summary = df_wt_by_issue.join(df_rank_count)\n",
    "    \n",
    "    return df_rank_summary.reset_index()\n",
    "\n",
    "def MakeWide(df_comments_rank_summary):\n",
    "    df_wide = df_comments_rank_summary.sort_values('max_rank').pivot_table(\n",
    "        index=['repo_name', 'time_period'],\n",
    "        columns='max_rank',\n",
    "        aggfunc='first'\n",
    "    )\n",
    "    df_wide.columns = [f\"max_rank_{int(rank)}_{col}\" for col, rank in df_wide.columns]\n",
    "    df_wide.reset_index(inplace=True)\n",
    "    return df_wide\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "ccfe660b-96ce-4b1b-93f3-58c5fdcf9580",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"rank_list = df_contributor_panel_rank.groupby(['repo_name','time_period']).agg({'max_rank':lambda x: sorted(set(x))}).rename({'max_rank':'rank_list'}, axis = 1).reset_index()\\ndf_rank_wide_cooperation = pd.merge(df_comments_rank_summary_wide, rank_list, how = 'left')\\n\\ndf_comments_rank_summary_wide[['max_rank_2_rank_contributor_count','max_rank_3_rank_contributor_count','max_rank_4_rank_contributor_count']] =     df_comments_rank_summary_wide[['max_rank_2_rank_contributor_count','max_rank_3_rank_contributor_count','max_rank_4_rank_contributor_count']].fillna(0)\\n\\nfor rank in [2,3,4]:\\n    df_rank_wide_cooperation[f'max_rank{rank}_discussion_count'] = df_rank_wide_cooperation.apply(\\n        lambda x: x[f'max_rank{rank}_discussion_count'] if not pd.isnull(x[f'max_rank{rank}_discussion_count']) else\\n        0 if rank in x['rank_list'] else np.nan, axis = 1)\\n    \\n\\n# max_rank_{rank}_discussion_count is 0 if rank is in rank_list\\n# max_rank_{rank}_{rank2} is 0 if rank is in rank_list & max_rank_{rank}_discussion_count>0 (not nan)\\n\""
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#df_comments_rank_summary = calculate_rank_metrics(df_comments_actor_rank)\n",
    "#df_comments_rank_summary_wide = MakeWide(df_comments_rank_summary)\n",
    "\"\"\"rank_list = df_contributor_panel_rank.groupby(['repo_name','time_period']).agg({'max_rank':lambda x: sorted(set(x))}).rename({'max_rank':'rank_list'}, axis = 1).reset_index()\n",
    "df_rank_wide_cooperation = pd.merge(df_comments_rank_summary_wide, rank_list, how = 'left')\n",
    "\n",
    "df_comments_rank_summary_wide[['max_rank_2_rank_contributor_count','max_rank_3_rank_contributor_count','max_rank_4_rank_contributor_count']] = \\\n",
    "    df_comments_rank_summary_wide[['max_rank_2_rank_contributor_count','max_rank_3_rank_contributor_count','max_rank_4_rank_contributor_count']].fillna(0)\n",
    "\n",
    "for rank in [2,3,4]:\n",
    "    df_rank_wide_cooperation[f'max_rank{rank}_discussion_count'] = df_rank_wide_cooperation.apply(\n",
    "        lambda x: x[f'max_rank{rank}_discussion_count'] if not pd.isnull(x[f'max_rank{rank}_discussion_count']) else\n",
    "        0 if rank in x['rank_list'] else np.nan, axis = 1)\n",
    "    \n",
    "\n",
    "# max_rank_{rank}_discussion_count is 0 if rank is in rank_list\n",
    "# max_rank_{rank}_{rank2} is 0 if rank is in rank_list & max_rank_{rank}_discussion_count>0 (not nan)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7581762d-85a3-4d8e-9b8b-f67425c6518c",
   "metadata": {},
   "source": [
    "## Combine Covariates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "ce71f433-8d4d-4b54-abce-ed44a003c8bb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_repo_panel_org_structure = df_panel_rank_layers.merge(df_repo_panel_overlap, how = 'outer').merge(df_repo_hhi, how = 'outer').merge(df_comments_cooperation, how = 'outer').merge(df_commits_cooperation, how = 'outer')\n",
    "df_repo_panel_org_structure = MakeBalanced(df_repo_panel_org_structure)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "936f79ec-a057-408a-914f-80971891237a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "072281fa-c7ce-43ea-8dce-8f49f1639fac",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_repo_panel_org_structure = df_repo_panel_org_structure.query('time_period>=\"2015-01-01\"')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c491344f-1033-4f2d-8bc7-c4a33094a789",
   "metadata": {},
   "source": [
    "# Combine All"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "e1917b92-9355-4a8c-95cc-7cd91988401a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>repo_name</th>\n",
       "      <th>actor_id</th>\n",
       "      <th>time_period</th>\n",
       "      <th>contributor_email_educational</th>\n",
       "      <th>contributor_email_corporate</th>\n",
       "      <th>truckfactor_member</th>\n",
       "      <th>max_rank</th>\n",
       "      <th>problem_identification_share</th>\n",
       "      <th>problem_discussion_share</th>\n",
       "      <th>coding_share</th>\n",
       "      <th>problem_approval_share</th>\n",
       "      <th>indiv_total_comments</th>\n",
       "      <th>comments_discussions_involved</th>\n",
       "      <th>comments_share_avg</th>\n",
       "      <th>comments_share_avg_wt</th>\n",
       "      <th>comments_hhi_avg</th>\n",
       "      <th>comments_hhi_avg_wt</th>\n",
       "      <th>pct_cooperation_comments</th>\n",
       "      <th>indiv_total_commits_count</th>\n",
       "      <th>pr_discussions_involved</th>\n",
       "      <th>commits_count_share_avg</th>\n",
       "      <th>commits_count_share_avg_wt</th>\n",
       "      <th>commits_count_hhi_avg</th>\n",
       "      <th>commits_count_hhi_avg_wt</th>\n",
       "      <th>pct_cooperation_commits_count</th>\n",
       "      <th>indiv_total_commits_100_count</th>\n",
       "      <th>100_pr_discussions_involved</th>\n",
       "      <th>commits_100_count_share_avg</th>\n",
       "      <th>commits_100_count_share_avg_wt</th>\n",
       "      <th>commits_100_count_hhi_avg</th>\n",
       "      <th>commits_100_count_hhi_avg_wt</th>\n",
       "      <th>pct_cooperation_commits_100_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>007gzs/django_restframework_apiview</td>\n",
       "      <td>148100.0</td>\n",
       "      <td>2017-01-01</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0625</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>007gzs/django_restframework_apiview</td>\n",
       "      <td>5856259.0</td>\n",
       "      <td>2017-01-01</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.9375</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>007gzs/django_restframework_apiview</td>\n",
       "      <td>5856259.0</td>\n",
       "      <td>2017-07-01</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>007gzs/django_restframework_apiview</td>\n",
       "      <td>5856259.0</td>\n",
       "      <td>2018-01-01</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>007gzs/django_restframework_apiview</td>\n",
       "      <td>5856259.0</td>\n",
       "      <td>2018-07-01</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1971724</th>\n",
       "      <td>zzzsochi/trans</td>\n",
       "      <td>443794.0</td>\n",
       "      <td>2016-01-01</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1971727</th>\n",
       "      <td>zzzsochi/trans</td>\n",
       "      <td>443794.0</td>\n",
       "      <td>2016-07-01</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.8000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.375</td>\n",
       "      <td>0.375</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1971736</th>\n",
       "      <td>zzzsochi/trans</td>\n",
       "      <td>2354108.0</td>\n",
       "      <td>2016-07-01</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.375</td>\n",
       "      <td>0.375</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1971737</th>\n",
       "      <td>zzzsochi/trans</td>\n",
       "      <td>4735252.0</td>\n",
       "      <td>2015-07-01</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1971738</th>\n",
       "      <td>zzzsochi/trans</td>\n",
       "      <td>5304276.0</td>\n",
       "      <td>2016-07-01</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.2000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.375</td>\n",
       "      <td>0.375</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1943565 rows Ã— 32 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                   repo_name   actor_id time_period  \\\n",
       "0        007gzs/django_restframework_apiview   148100.0  2017-01-01   \n",
       "1        007gzs/django_restframework_apiview  5856259.0  2017-01-01   \n",
       "2        007gzs/django_restframework_apiview  5856259.0  2017-07-01   \n",
       "3        007gzs/django_restframework_apiview  5856259.0  2018-01-01   \n",
       "9        007gzs/django_restframework_apiview  5856259.0  2018-07-01   \n",
       "...                                      ...        ...         ...   \n",
       "1971724                       zzzsochi/trans   443794.0  2016-01-01   \n",
       "1971727                       zzzsochi/trans   443794.0  2016-07-01   \n",
       "1971736                       zzzsochi/trans  2354108.0  2016-07-01   \n",
       "1971737                       zzzsochi/trans  4735252.0  2015-07-01   \n",
       "1971738                       zzzsochi/trans  5304276.0  2016-07-01   \n",
       "\n",
       "         contributor_email_educational  contributor_email_corporate  \\\n",
       "0                                  0.0                          0.0   \n",
       "1                                  0.0                          0.0   \n",
       "2                                  0.0                          0.0   \n",
       "3                                  0.0                          0.0   \n",
       "9                                  0.0                          0.0   \n",
       "...                                ...                          ...   \n",
       "1971724                            0.0                          0.0   \n",
       "1971727                            0.0                          0.0   \n",
       "1971736                            0.0                          0.0   \n",
       "1971737                            0.0                          0.0   \n",
       "1971738                            0.0                          0.0   \n",
       "\n",
       "         truckfactor_member  max_rank  problem_identification_share  \\\n",
       "0                       0.0       3.0                           NaN   \n",
       "1                       1.0       3.0                           NaN   \n",
       "2                       1.0       3.0                           NaN   \n",
       "3                       1.0       3.0                           NaN   \n",
       "9                       0.0       NaN                           NaN   \n",
       "...                     ...       ...                           ...   \n",
       "1971724                 1.0       3.0                           NaN   \n",
       "1971727                 1.0       4.0                      0.333333   \n",
       "1971736                 0.0       2.0                      0.333333   \n",
       "1971737                 0.0       4.0                      0.000000   \n",
       "1971738                 0.0       3.0                      0.333333   \n",
       "\n",
       "         problem_discussion_share  coding_share  problem_approval_share  \\\n",
       "0                             NaN        0.0625                     NaN   \n",
       "1                             NaN        0.9375                     NaN   \n",
       "2                             NaN        1.0000                     NaN   \n",
       "3                             NaN        1.0000                     NaN   \n",
       "9                             NaN           NaN                     NaN   \n",
       "...                           ...           ...                     ...   \n",
       "1971724                       NaN        1.0000                     NaN   \n",
       "1971727                      0.50        0.8000                1.000000   \n",
       "1971736                      0.25        0.0000                0.000000   \n",
       "1971737                      0.00        0.0000                0.333333   \n",
       "1971738                      0.25        0.2000                0.000000   \n",
       "\n",
       "         indiv_total_comments  comments_discussions_involved  \\\n",
       "0                         NaN                            NaN   \n",
       "1                         NaN                            NaN   \n",
       "2                         NaN                            NaN   \n",
       "3                         NaN                            NaN   \n",
       "9                         NaN                            NaN   \n",
       "...                       ...                            ...   \n",
       "1971724                   NaN                            NaN   \n",
       "1971727                   2.0                            1.0   \n",
       "1971736                   1.0                            1.0   \n",
       "1971737                   NaN                            NaN   \n",
       "1971738                   1.0                            1.0   \n",
       "\n",
       "         comments_share_avg  comments_share_avg_wt  comments_hhi_avg  \\\n",
       "0                       NaN                    NaN               NaN   \n",
       "1                       NaN                    NaN               NaN   \n",
       "2                       NaN                    NaN               NaN   \n",
       "3                       NaN                    NaN               NaN   \n",
       "9                       NaN                    NaN               NaN   \n",
       "...                     ...                    ...               ...   \n",
       "1971724                 NaN                    NaN               NaN   \n",
       "1971727                0.50                   0.50             0.375   \n",
       "1971736                0.25                   0.25             0.375   \n",
       "1971737                 NaN                    NaN               NaN   \n",
       "1971738                0.25                   0.25             0.375   \n",
       "\n",
       "         comments_hhi_avg_wt  pct_cooperation_comments  \\\n",
       "0                        NaN                       NaN   \n",
       "1                        NaN                       NaN   \n",
       "2                        NaN                       NaN   \n",
       "3                        NaN                       NaN   \n",
       "9                        NaN                       NaN   \n",
       "...                      ...                       ...   \n",
       "1971724                  NaN                       NaN   \n",
       "1971727                0.375                       1.0   \n",
       "1971736                0.375                       1.0   \n",
       "1971737                  NaN                       NaN   \n",
       "1971738                0.375                       1.0   \n",
       "\n",
       "         indiv_total_commits_count  pr_discussions_involved  \\\n",
       "0                              NaN                      NaN   \n",
       "1                              NaN                      NaN   \n",
       "2                              NaN                      NaN   \n",
       "3                              NaN                      NaN   \n",
       "9                              NaN                      NaN   \n",
       "...                            ...                      ...   \n",
       "1971724                        NaN                      NaN   \n",
       "1971727                        NaN                      NaN   \n",
       "1971736                        NaN                      NaN   \n",
       "1971737                        NaN                      NaN   \n",
       "1971738                        1.0                      1.0   \n",
       "\n",
       "         commits_count_share_avg  commits_count_share_avg_wt  \\\n",
       "0                            NaN                         NaN   \n",
       "1                            NaN                         NaN   \n",
       "2                            NaN                         NaN   \n",
       "3                            NaN                         NaN   \n",
       "9                            NaN                         NaN   \n",
       "...                          ...                         ...   \n",
       "1971724                      NaN                         NaN   \n",
       "1971727                      NaN                         NaN   \n",
       "1971736                      NaN                         NaN   \n",
       "1971737                      NaN                         NaN   \n",
       "1971738                      1.0                         1.0   \n",
       "\n",
       "         commits_count_hhi_avg  commits_count_hhi_avg_wt  \\\n",
       "0                          NaN                       NaN   \n",
       "1                          NaN                       NaN   \n",
       "2                          NaN                       NaN   \n",
       "3                          NaN                       NaN   \n",
       "9                          NaN                       NaN   \n",
       "...                        ...                       ...   \n",
       "1971724                    NaN                       NaN   \n",
       "1971727                    NaN                       NaN   \n",
       "1971736                    NaN                       NaN   \n",
       "1971737                    NaN                       NaN   \n",
       "1971738                    1.0                       1.0   \n",
       "\n",
       "         pct_cooperation_commits_count  indiv_total_commits_100_count  \\\n",
       "0                                  NaN                            NaN   \n",
       "1                                  NaN                            NaN   \n",
       "2                                  NaN                            NaN   \n",
       "3                                  NaN                            NaN   \n",
       "9                                  NaN                            NaN   \n",
       "...                                ...                            ...   \n",
       "1971724                            NaN                            NaN   \n",
       "1971727                            NaN                            NaN   \n",
       "1971736                            NaN                            NaN   \n",
       "1971737                            NaN                            NaN   \n",
       "1971738                            0.0                            1.0   \n",
       "\n",
       "         100_pr_discussions_involved  commits_100_count_share_avg  \\\n",
       "0                                NaN                          NaN   \n",
       "1                                NaN                          NaN   \n",
       "2                                NaN                          NaN   \n",
       "3                                NaN                          NaN   \n",
       "9                                NaN                          NaN   \n",
       "...                              ...                          ...   \n",
       "1971724                          NaN                          NaN   \n",
       "1971727                          NaN                          NaN   \n",
       "1971736                          NaN                          NaN   \n",
       "1971737                          NaN                          NaN   \n",
       "1971738                          1.0                          1.0   \n",
       "\n",
       "         commits_100_count_share_avg_wt  commits_100_count_hhi_avg  \\\n",
       "0                                   NaN                        NaN   \n",
       "1                                   NaN                        NaN   \n",
       "2                                   NaN                        NaN   \n",
       "3                                   NaN                        NaN   \n",
       "9                                   NaN                        NaN   \n",
       "...                                 ...                        ...   \n",
       "1971724                             NaN                        NaN   \n",
       "1971727                             NaN                        NaN   \n",
       "1971736                             NaN                        NaN   \n",
       "1971737                             NaN                        NaN   \n",
       "1971738                             1.0                        1.0   \n",
       "\n",
       "         commits_100_count_hhi_avg_wt  pct_cooperation_commits_100_count  \n",
       "0                                 NaN                                NaN  \n",
       "1                                 NaN                                NaN  \n",
       "2                                 NaN                                NaN  \n",
       "3                                 NaN                                NaN  \n",
       "9                                 NaN                                NaN  \n",
       "...                               ...                                ...  \n",
       "1971724                           NaN                                NaN  \n",
       "1971727                           NaN                                NaN  \n",
       "1971736                           NaN                                NaN  \n",
       "1971737                           NaN                                NaN  \n",
       "1971738                           1.0                                0.0  \n",
       "\n",
       "[1943565 rows x 32 columns]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_repo_covariates = pd.merge(df_repo_characteristics, df_repo_panel_org_structure, how = 'outer')\n",
    "df_repo_covariates = FillRepoCharacteristics(df_repo_covariates)\n",
    "df_repo_covariates\n",
    "df_contributor_characteristics"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
