{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e7ccf431-c012-4997-9aaf-dc0bd4f3bf23",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir('../')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2278abef-3ba8-4b9d-93ce-33e1cedeed85",
   "metadata": {},
   "outputs": [],
   "source": [
    "## import pandas as pd\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import sys\n",
    "import glob\n",
    "import warnings\n",
    "import random\n",
    "from pandarallel import pandarallel\n",
    "from source.lib.JMSLab import autofill\n",
    "from source.lib.helpers import *\n",
    "from ast import literal_eval\n",
    "from concurrent.futures import ThreadPoolExecutor, ProcessPoolExecutor\n",
    "from glob import glob \n",
    "import datetime\n",
    "import itertools\n",
    "import time\n",
    "from multiprocessing import pool\n",
    "from source.lib.helpers import *\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "pd.set_option('display.max_columns', None)\n",
    "pandarallel.initialize(progress_bar = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a2e1feca-055b-4b08-a973-25e7935aa766",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ConstructRepoPanel(df_issue_selected, df_pr_selected, time_period, SECONDS_IN_DAY, outdir):\n",
    "    df_repo_panel = pd.concat([df_issue_selected[['repo_name','time_period']].drop_duplicates(), \n",
    "                           df_pr_selected[['repo_name','time_period']].drop_duplicates()]).drop_duplicates()\\\n",
    "        .groupby('repo_name')\\\n",
    "        .agg({'time_period': ['min','max']})\\\n",
    "        .reset_index()\\\n",
    "        .rename({('time_period','min'): 'earliest-date',\n",
    "                 ('time_period','max'): 'latest_date'}, axis = 1)\n",
    "    df_repo_panel.columns = ['repo_name','earliest_date','latest_date']\n",
    "    df_repo_panel['time_period'] = df_repo_panel.apply(lambda x: pd.date_range(x['earliest_date'], x['latest_date'] , freq=f'{time_period}MS').tolist(), axis = 1)\n",
    "    df_repo_panel = df_repo_panel.drop(['earliest_date', 'latest_date'], axis = 1).explode('time_period')\n",
    "    df_issues_sans_comments = CreateIssueSansCommentsStats(df_issue_selected)\n",
    "    df_issues = CreateFullIssueDatasetWithComments(df_issue_selected, df_issues_sans_comments, SECONDS_IN_DAY)\n",
    "    df_issues_stats = CreateIssueStats(df_issues)\n",
    "    df_prs_sans_reviews = CreatePRSansReviewsStats(df_pr_selected)\n",
    "    df_prs_complete = CreateFullPRDatasetWithReviews(df_pr_selected, df_prs_sans_reviews, SECONDS_IN_DAY)\n",
    "    df_prs_stats = CreatePRStats(df_prs_complete)\n",
    "    df_stats = pd.merge(df_issues_stats, df_prs_stats, how = 'outer')\n",
    "    df_repo_panel_stats = pd.merge(df_repo_panel, df_stats, how = 'left')\n",
    "    df_repo_panel_stats.to_parquet(outdir / f'project_outcomes_{time_period}.parquet')\n",
    "\n",
    "def RemoveDuplicates(df, query, keepcols, duplicatecols, newcolname):\n",
    "    print(df.columns)\n",
    "    df_uq = df.query(query).sort_values('created_at', ascending = True)[keepcols]\\\n",
    "        .drop_duplicates(duplicatecols)\n",
    "    df_uq[newcolname] = 1\n",
    "    return df_uq\n",
    "\n",
    "def CreateIssueSansCommentsStats(df_issue_selected):\n",
    "    issue_keepcols = ['repo_name','issue_number','time_period', 'created_at']\n",
    "    issue_duplicatecols = ['repo_name','issue_number']\n",
    "    df_opened_issues = RemoveDuplicates(df_issue_selected, 'issue_action == \"opened\"', issue_keepcols, issue_duplicatecols, 'opened_issue')\n",
    "    df_closed_issues = RemoveDuplicates(df_issue_selected, 'issue_action == \"closed\"', issue_keepcols, issue_duplicatecols, 'closed_issue')\\\n",
    "        .rename({'time_period':'closed_time_period','created_at':'closed_at'}, axis = 1)\n",
    "    ## TODO: how many closed issues are unlinked\n",
    "    df_issues_sans_comments = pd.merge(df_opened_issues, df_closed_issues, how = 'left')\n",
    "    return df_issues_sans_comments\n",
    "\n",
    "def CreateFullIssueDatasetWithComments(df_issue_selected, df_issues_sans_comments, SECONDS_IN_DAY):\n",
    "    ic_keepcols = ['issue_number','issue_comment_id','repo_name','time_period', 'created_at']\n",
    "    ic_duplicatecols = ['repo_name','issue_number','time_period', 'created_at']\n",
    "    df_issue_comments = RemoveDuplicates(df_issue_selected, 'type == \"IssueCommentEvent\"',ic_keepcols, ic_duplicatecols, 'issue_comments')\\\n",
    "        .groupby(['repo_name','issue_number'])['issue_comments'].sum()\\\n",
    "        .reset_index()\n",
    "    # TODO: how many unlinked issues by issue comments\n",
    "    df_issues = pd.merge(df_issues_sans_comments, df_issue_comments, how = 'left')\n",
    "    for col in ['closed_issue','issue_comments']:    \n",
    "        df_issues[col] = df_issues[col].fillna(0)\n",
    "    df_issues['days_to_close'] = (df_issues['closed_at'] - df_issues['created_at']).apply(lambda x: x.total_seconds()/SECONDS_IN_DAY)\n",
    "    for day in closing_day_options:\n",
    "        df_issues[f'closed_in_{day}_days'] = pd.to_numeric(df_issues['days_to_close']<day).astype(int)\n",
    "    return df_issues\n",
    "    \n",
    "def CreateIssueStats(df_issues):\n",
    "    df_issues_stats = df_issues.groupby(['repo_name','time_period'])\\\n",
    "        .agg({'opened_issue': 'sum','closed_issue':['sum','mean'],\n",
    "              'issue_comments':['sum', 'mean'],\n",
    "              'closed_in_30_days':'mean', 'closed_in_60_days':'mean','closed_in_90_days':'mean',\n",
    "              'closed_in_180_days':'mean', 'closed_in_360_days':'mean'})\n",
    "    df_issues_stats.columns = df_issues_stats.columns.to_flat_index()\n",
    "    df_issues_stats = df_issues_stats.reset_index()\\\n",
    "        .rename(columns = {('opened_issue','sum'): 'opened_issues',\n",
    "                 ('closed_issue', 'sum'): 'closed_issues',\n",
    "                 ('closed_issue', 'mean'): 'p_issues_closed',\n",
    "                 ('issue_comments', 'sum'): 'issue_comments',\n",
    "                 ('issue_comments', 'mean'): 'avg_issue_commments',\n",
    "                 ('closed_in_30_days', 'mean'): 'p_issues_closed_30d',\n",
    "                 ('closed_in_60_days', 'mean'): 'p_issues_closed_60d',\n",
    "                 ('closed_in_90_days', 'mean'): 'p_issues_closed_90d',\n",
    "                 ('closed_in_180_days', 'mean'): 'p_issues_closed_180d',\n",
    "                 ('closed_in_360_days', 'mean'): 'p_issues_closed_360d'})\n",
    "    return df_issues_stats\n",
    "\n",
    "def CreatePRSansReviewsStats(df_pr_selected):\n",
    "    pr_keepcols = ['repo_name','pr_number','time_period', 'created_at']\n",
    "    pr_merge_keepcols = ['repo_name','pr_number','time_period', 'created_at', 'pr_merged_by_type']\n",
    "    pr_idcols = ['repo_name','pr_number']\n",
    "    df_opened_prs = RemoveDuplicates(df_pr_selected,'pr_action == \"opened\"', pr_keepcols, pr_idcols, 'opened_pr')\n",
    "    df_closed_prs = RemoveDuplicates(df_pr_selected,'pr_action == \"closed\" & pr_merged_by_id.isna()', pr_keepcols, pr_idcols, 'closed_unmerged_pr')\\\n",
    "        .rename({'time_period':'closed_unmerged_time_period','created_at':'closed_unmerged_at'}, axis = 1)\n",
    "    df_merged_prs = RemoveDuplicates(df_pr_selected,'pr_action==\"closed\" & ~pr_merged_by_id.isna()',pr_merge_keepcols,\n",
    "                                     pr_idcols,'merged_pr')\\\n",
    "        .rename({'time_period':'merged_time_period','created_at':'merged_at'}, axis = 1)\n",
    "    df_prs_sans_reviews = pd.merge(df_opened_prs, df_closed_prs, how = 'left').merge(df_merged_prs, how = 'left')\n",
    "    return df_prs_sans_reviews\n",
    "\n",
    "def CreateFullPRDatasetWithReviews(df_pr_selected, df_prs_sans_reviews, SECONDS_IN_DAY):\n",
    "    pr_review_keepcols = ['repo_name','pr_number','time_period', 'created_at','pr_review_id','pr_review_state']\n",
    "    pr_review_idcols = ['repo_name','pr_number','pr_review_id']\n",
    "    df_pr_reviews = RemoveDuplicates(df_pr_selected,'type == \"PullRequestReviewEvent\"',pr_review_keepcols,pr_review_idcols, 'pr_review')\n",
    "    for col in ['commented','approved','changes_requested']:\n",
    "        df_pr_reviews[f'review_state_{col}'] = pd.to_numeric(df_pr_reviews['pr_review_state']==col).astype(int)\n",
    "    df_pr_review_stats = df_pr_reviews.groupby(['repo_name','pr_number'])\\\n",
    "        [['pr_review','review_state_commented','review_state_approved','review_state_changes_requested']].sum().reset_index()\n",
    "    pr_rc_keepcols = ['repo_name','pr_number','time_period', 'created_at','pr_review_comment_body']\n",
    "    pr_rc_idcols = ['repo_name','pr_number','pr_review_comment_body'] #don't have review comment id's i believe\n",
    "    df_pr_review_comments = RemoveDuplicates(df_pr_selected,'type == \"PullRequestReviewCommentEvent\"',pr_rc_keepcols, pr_rc_idcols, 'pr_review_comment')\n",
    "    df_pr_review_comments_stats = df_pr_review_comments.groupby(['repo_name','pr_number'])\\\n",
    "        [['pr_review_comment']].sum().reset_index()\n",
    "    df_prs_complete = pd.merge(df_prs_sans_reviews, df_pr_review_stats, how = 'left').merge(df_pr_review_comments_stats, how = 'left')\n",
    "    for col in ['closed_unmerged_pr', 'merged_pr', 'pr_review','review_state_commented',\n",
    "                'review_state_approved','review_state_changes_requested','pr_review_comment']:    \n",
    "        df_prs_complete[col] = df_prs_complete[col].fillna(0)\n",
    "    df_prs_complete['pr_review_comments_total'] = df_prs_complete['pr_review']+df_prs_complete['pr_review_comment']\n",
    "    df_prs_complete['days_to_merge'] = (df_prs_complete['merged_at'] - df_prs_complete['created_at']).apply(lambda x: x.total_seconds()/SECONDS_IN_DAY)\n",
    "    for day in closing_day_options:\n",
    "        df_prs_complete[f'merged_in_{day}_days'] = pd.to_numeric(df_prs_complete['days_to_merge']<day).astype(int)\n",
    "    return df_prs_complete\n",
    "\n",
    "def CreatePRStats(df_prs_complete):\n",
    "    df_prs_stats = df_prs_complete.groupby(['repo_name','time_period'])\\\n",
    "        .agg({'opened_pr': 'sum','merged_pr':['sum','mean'],\n",
    "              'pr_review': ['sum','mean'], 'pr_review_comment': ['sum','mean'],\n",
    "              'review_state_commented':'mean', 'review_state_approved': 'mean',\n",
    "              'review_state_changes_requested': 'mean',\n",
    "              'merged_in_30_days':'mean', 'merged_in_60_days':'mean','merged_in_90_days':'mean',\n",
    "              'merged_in_180_days':'mean', 'merged_in_360_days':'mean'})\n",
    "    df_prs_stats.columns = df_prs_stats.columns.to_flat_index()\n",
    "    df_prs_stats = df_prs_stats.reset_index()\\\n",
    "        .rename(columns = {('opened_pr','sum'): 'opened_prs',\n",
    "                           ('merged_pr','sum'): 'merged_prs',\n",
    "                           ('merged_pr','mean'): 'p_prs_merged',\n",
    "                           ('pr_review','sum'): 'pr_reviews',\n",
    "                           ('pr_review','mean'): 'mean_reviews_per_pr',\n",
    "                           ('pr_review_comment','sum'):'pr_review_comments',\n",
    "                           ('pr_review_comment','mean'):'mean_review_comments_per_pr',\n",
    "                           ('review_state_commented','mean'):'p_review_state_commented',\n",
    "                           ('review_state_approved','mean'):'p_review_state_approved',\n",
    "                           ('review_state_changes_requested','mean'):'p_review_state_changes_requested',\n",
    "                           ('closed_issue', 'sum'): 'closed_issues',\n",
    "                           ('closed_issue', 'mean'): 'p_issues_closed',\n",
    "                           ('merged_in_30_days', 'mean'): 'p_prs_merged_30d',\n",
    "                           ('merged_in_60_days', 'mean'): 'p_prs_merged_60d',\n",
    "                           ('merged_in_90_days', 'mean'): 'p_prs_merged_90d',\n",
    "                           ('merged_in_180_days', 'mean'): 'p_prs_merged_180d',\n",
    "                           ('merged_in_360_days', 'mean'): 'p_prs_merged_360d'})\n",
    "    return df_prs_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c912a15-dcf4-43b3-966b-51864bcaea8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ConstructRepoContributorPanel(departure_candidates_rank_rolling, df_repo_panel_stats_hierarchy):\n",
    "    df_repo_sample = FilterForTreatedOnce(departure_candidates_rank_rolling, df_repo_panel_stats_hierarchy)\n",
    "    df_repo_sample = AddCovariates(df_repo_sample)\n",
    "    df_repo_sample = AddAlwaysActiveIndicators(df_repo_sample)\n",
    "    return df_repo_sample\n",
    "\n",
    "\n",
    "def FilterForTreatedOnce(departure_candidates_rank_rolling, df_repo_panel_stats_hierarchy):\n",
    "    repo_appears_once = departure_candidates_rank_rolling[['repo_name','actor_id']].drop_duplicates()\\\n",
    "        ['repo_name'].value_counts().reset_index()\\\n",
    "        .query('count==1')\\\n",
    "        ['repo_name'].tolist()\n",
    "    departure_repos = departure_candidates_rank_rolling['repo_name'].unique().tolist()\n",
    "    all_repos = df_repo_panel_stats_hierarchy['repo_name'].unique().tolist()\n",
    "    repo_never_appears = [repo for repo in all_repos if repo not in departure_repos]\n",
    "    df_repo_one_treatment = df_repo_panel_stats_hierarchy[\n",
    "        df_repo_panel_stats_hierarchy['repo_name'].isin(repo_never_appears + repo_appears_once)]\n",
    "    treated_date = departure_candidates_rank_rolling[departure_candidates_rank_rolling['repo_name'].isin(repo_appears_once)]\\\n",
    "        .query('time_period == final_period')\\\n",
    "        [['repo_name','final_period','rank','rank_ffilled','rolling_3period_rank','rolling_6period_rank']].drop_duplicates()\n",
    "    df_repo_sample = pd.merge(df_repo_one_treatment, treated_date, how = 'left', on = ['repo_name'])\n",
    "    return df_repo_sample\n",
    "\n",
    "def AddCovariates(df_repo_sample):\n",
    "    for col in [['opened_issues','closed_issues','opened_prs']]:\n",
    "        df_repo_sample[col] = df_repo_sample[col].fillna(0)\n",
    "    df_repo_sample['treatment'] = df_repo_sample.parallel_apply(\n",
    "        lambda x: 0 if pd.isnull(x['final_period']) else int(x['time_period']>x['final_period']), axis = 1)\n",
    "    df_repo_sample['active_all'] = df_repo_sample.apply(\n",
    "        lambda x: x['opened_issues']>0 and x['opened_prs']>0, axis = 1).astype(int)\n",
    "    df_repo_sample['mean_activity_all'] = df_repo_sample.groupby('repo_name')['active_all'].transform('mean')\n",
    "    df_repo_sample['periods_all'] = df_repo_sample.groupby('repo_name')['active_all'].transform('sum')\n",
    "    time_index_dict_rev = df_repo_sample['time_period'].sort_values().drop_duplicates().reset_index(drop = True).to_dict()\n",
    "    time_index_dict = {v: k for (k, v) in time_index_dict_rev.items()}\n",
    "    df_repo_sample['time_index'] = df_repo_sample['time_period'].apply(lambda x: time_index_dict[x])\n",
    "    return df_repo_sample\n",
    "\n",
    "\n",
    "def AddAlwaysActiveIndicators(df_repo_sample):\n",
    "    active_2019onwards = df_repo_sample.query('time_period>=\"2019-01-01\" & time_period < \"2023-07-01\"')\\\n",
    "        .groupby('repo_name')[['active_all']].sum().query('active_all == 9').index.tolist()\n",
    "    active_2018onwards = df_repo_sample.query('time_period>=\"2018-01-01\" & time_period < \"2023-07-01\"')\\\n",
    "        .groupby('repo_name')[['active_all']].sum().query('active_all == 11').index.tolist()\n",
    "    active_2017onwards = df_repo_sample.query('time_period>=\"2017-01-01\" & time_period < \"2023-07-01\"')\\\n",
    "        .groupby('repo_name')[['active_all']].sum().query('active_all == 13').index.tolist()\n",
    "    df_repo_sample['2017_sample'] = df_repo_sample['repo_name'].isin(active_2017onwards).astype(int)\n",
    "    df_repo_sample['2018_sample'] = df_repo_sample['repo_name'].isin(active_2018onwards).astype(int)\n",
    "    df_repo_sample['2019_sample'] = df_repo_sample['repo_name'].isin(active_2019onwards).astype(int)\n",
    "    return df_repo_sample\n",
    "\n",
    "\n",
    "def GetConsecutiveSum(df):\n",
    "    gb = df.groupby((df['active_all'] != df['active_all'].shift()).cumsum())\n",
    "    df['consecutive_periods'] = gb['periods_all'].cumsum()\n",
    "    df.loc[df['active_all'] == 0, 'consecutive_periods'] = 0\n",
    "    return df\n",
    "\n",
    "def GenerateDepartureCandidateRank(indir_committers_departure, indir_committers_rank, consecutive_req, post_period_req):\n",
    "    departure_candidates = pd.read_parquet(indir_committers_departure / f'contributors_major_months{time_period}_window{window.replace(\"D\",\"\")}criteria_issue_comments_75pct_general25pct_consecutive{consecutive_req}_post_period{post_period_req}threshold_mean_0.2.parquet')\n",
    "    df_committers_rank = pd.read_parquet(indir_committers_rank / f'contributor_rank_major_months{time_period}_window{window}.parquet')\n",
    "    departure_candidates_rank = pd.merge(departure_candidates, df_committers_rank[['repo_name','actor_id','time_period','user_type','rank']],\n",
    "                                         how = 'left')\n",
    "    departure_candidates_rank['rank_ffilled'] = departure_candidates_rank.groupby(['repo_name','actor_id'])['rank'].ffill()\n",
    "    rank_dict = { 'active user': 1, 'developer': 2, 'maintainer': 3}\n",
    "    rank_dict_inv = {v: k for k, v in rank_dict.items()}\n",
    "    departure_candidates_rank['rank_numeric'] = departure_candidates_rank['rank'].parallel_apply(lambda x: rank_dict.get(x, np.nan))\n",
    "    departure_candidates_rank_rolling = departure_candidates_rank\n",
    "    for periods in [3, 6]:\n",
    "        rolling_rank = departure_candidates_rank_rolling.groupby(['repo_name','actor_id']).rolling(periods, min_periods = 1)\\\n",
    "            ['rank_numeric'].max()\\\n",
    "            .rename(f'rolling_{periods}period_rank_numeric')\\\n",
    "            .reset_index().set_index('level_2')\\\n",
    "            .drop(['repo_name','actor_id'],axis = 1)\n",
    "        departure_candidates_rank_rolling = departure_candidates_rank_rolling.join(rolling_rank, how = 'left')\n",
    "        departure_candidates_rank_rolling[f'rolling_{periods}period_rank'] = departure_candidates_rank_rolling[f'rolling_{periods}period_rank_numeric'].parallel_apply(lambda x: rank_dict_inv.get(x, np.nan))\n",
    "    return departure_candidates_rank_rolling\n",
    "\n",
    "def GenerateRepoHierarchyStats(outdir_data, indir_committers_rank):\n",
    "    df_repo_panel_stats = pd.read_parquet(outdir_data / f'project_outcomes_{time_period}.parquet')\n",
    "    df_shares = pd.read_parquet(indir_committers_rank / f'shares_{time_period}_window{window}.parquet')\n",
    "    df_repo_panel_stats_hierarchy = pd.merge(df_repo_panel_stats, \n",
    "                                             df_shares[['repo_name','time_period','project_hierarchy_rank','active_user_hierarchy_rank','developer_hierarchy_rank']],\n",
    "                                             how = 'left')\n",
    "    return df_repo_panel_stats_hierarchy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7af3b159-dd73-466a-af8f-bfa397373c47",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOStream.flush timed out\n"
     ]
    }
   ],
   "source": [
    "def Main():\n",
    "    indir_committers_info = Path('drive/output/scrape/link_committers_profile')\n",
    "    indir_data = Path('drive/output/derived/data_export')\n",
    "    indir_committers_departure = Path('drive/output/derived/major_contributor_prospects/departed_contributors')\n",
    "    indir_committers_rank = Path('drive/output/derived/major_contributor_prospects/contributor_rank_panel')\n",
    "    outdir_data = Path('drive/output/derived/project_outcomes')\n",
    "    \n",
    "    \n",
    "    commit_cols = ['commits','commit additions','commit deletions','commit changes total','commit files changed count']\n",
    "    author_thresh = 1/3\n",
    "    time_period_months = [2, 3, 6] #[2,3,6]\n",
    "    rolling_window = ['367D', '732D', '1828D']\n",
    "    SECONDS_IN_DAY = 86400\n",
    "    closing_day_options = [30, 60, 90, 180, 360]\n",
    "    \n",
    "    df_issue = pd.read_parquet(indir_data / 'df_issue.parquet')\n",
    "    df_pr = pd.read_parquet(indir_data / 'df_pr.parquet')\n",
    "    \n",
    "    df_issue['created_at'] = pd.to_datetime(df_issue['created_at'])\n",
    "    df_pr['created_at'] = pd.to_datetime(df_pr['created_at'])\n",
    "    \n",
    "    selected_repos = df_issue[['repo_name']].drop_duplicates()['repo_name'].tolist()\n",
    "    \n",
    "    df_issue_selected = df_issue[(df_issue['repo_name'].isin(selected_repos)) & (df_issue['created_at']>='2015-01-01')]\n",
    "    df_pr_selected = df_pr[(df_pr['repo_name'].isin(selected_repos))  & (df_pr['created_at']>='2015-01-01')]\n",
    "    \n",
    "    for time_period in time_period_months:\n",
    "        df_issue_selected = ImputeTimePeriod(df_issue_selected, time_period)\n",
    "        df_pr_selected = ImputeTimePeriod(df_pr_selected, time_period)\n",
    "        ConstructRepoPanel(df_issue_selected, df_pr_selected, time_period, SECONDS_IN_DAY, outdir_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "fbc345ac-8d6a-475e-9533-53813e74d97d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'indir_committers_departure' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[54], line 5\u001b[0m\n\u001b[1;32m      3\u001b[0m time_period \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m3\u001b[39m\n\u001b[1;32m      4\u001b[0m window \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m732D\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m----> 5\u001b[0m departure_candidates_rank_rolling \u001b[38;5;241m=\u001b[39m GenerateDepartureCandidateRank(\u001b[43mindir_committers_departure\u001b[49m, indir_committers_rank, \u001b[38;5;241m6\u001b[39m, \u001b[38;5;241m4\u001b[39m)\n\u001b[1;32m      6\u001b[0m df_repo_panel_stats_hierarchy \u001b[38;5;241m=\u001b[39m GenerateRepoHierarchyStats(outdir_data, indir_committers_rank)\n\u001b[1;32m      7\u001b[0m df_repo_sample \u001b[38;5;241m=\u001b[39m ConstructRepoContributorPanel(departure_candidates_rank_rolling, df_repo_panel_stats_hierarchy)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'indir_committers_departure' is not defined"
     ]
    }
   ],
   "source": [
    "for time_period in time_period_months:\n",
    "    for window in rolling_window:\n",
    "        #time_period = 3\n",
    "        #window = '732D'\n",
    "        consecutive_req = 6\n",
    "        post_period_req = 4\n",
    "        departure_candidates_rank_rolling = GenerateDepartureCandidateRank(indir_committers_departure, indir_committers_rank, 6, 4)\n",
    "        df_repo_panel_stats_hierarchy = GenerateRepoHierarchyStats(outdir_data, indir_committers_rank)\n",
    "        df_repo_sample = ConstructRepoContributorPanel(departure_candidates_rank_rolling, df_repo_panel_stats_hierarchy)\n",
    "        df_repo_sample.to_csv(f'drive/output/derived/proof_of_concept/panel_major_months{time_period}_window{window}.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a1673db-c24e-4194-be96-f87f4b59e5ab",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "05b76940-b6dd-43dd-9aa2-a8735b3c48e5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "oss_hierarchy",
   "language": "python",
   "name": "oss_hierarchy"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
