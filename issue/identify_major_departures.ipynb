{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8308b099-6215-443a-bb62-8820f23f1b1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import glob\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from ast import literal_eval\n",
    "from pandarallel import pandarallel\n",
    "pd.set_option('display.max_columns', None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cc03b794-c8a7-46dd-9025-32f2f6781939",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir('../')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "07a3ae61-fed5-484e-a8fb-56d003e7aafe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def GetContributorData(indir, major_months, window):\n",
    "    df_contributors = pd.read_parquet(indir / f'major_contributors_major_months{major_months}_window{window}D_samplefull.parquet')\n",
    "    return df_contributors\n",
    "    \n",
    "def ContributorCount(df):\n",
    "    return df[['actor_id','repo_name']].drop_duplicates().shape[0]\n",
    "\n",
    "def GetConsecutiveSum(df):\n",
    "    gb = df.groupby((df['criteria_exceed'] != df['criteria_exceed'].shift()).cumsum())\n",
    "    df['consecutive_periods'] = gb['criteria_exceed'].cumsum()\n",
    "    df.loc[df['criteria_exceed'] == 0, 'consecutive_periods'] = 0\n",
    "    return df\n",
    "\n",
    "def GetCandidates(departure_candidates, post_period_length, decline_type, decline_stat, criteria_analysis_col, criteria_col, df_potential_consecutive):\n",
    "    df_candidates = pd.DataFrame()\n",
    "    for i in departure_candidates.index:\n",
    "        final_period = departure_candidates.loc[i, 'final_period']\n",
    "        total_consecutive_periods = departure_candidates.loc[i, 'consecutive_periods']\n",
    "        repo_name = departure_candidates.loc[i, 'repo_name']\n",
    "        actor_id = departure_candidates.loc[i, 'actor_id']\n",
    "        df_potential_consecutive_subset = df_potential_consecutive[df_potential_consecutive.apply(lambda x: x['repo_name'] == repo_name and x['actor_id'] == actor_id, axis = 1)]\n",
    "        df_candidate = pd.merge(df_potential_consecutive_subset, departure_candidates.loc[[i]].drop('consecutive_periods', axis = 1))\n",
    "\n",
    "        final_periods = df_candidate.query('time_period>final_period').head(post_period_length)\n",
    "        prior_periods = df_candidate.query('time_period<=final_period').sort_values('time_period').tail(total_consecutive_periods)\n",
    "        if final_periods.shape[0]>=post_period_length: \n",
    "            if final_periods[criteria_analysis_col].isna().sum() != 0: # project is still active\n",
    "                if decline_type == \"threshold_mean\":\n",
    "                    pre_period_mean = prior_periods[criteria_col].mean()\n",
    "                    post_period_mean = final_periods[criteria_col].mean()\n",
    "                    if pre_period_mean * decline_stat > post_period_mean:\n",
    "                        df_candidates = pd.concat([df_candidates, df_candidate])\n",
    "                if decline_type == \"threshold_pct\":\n",
    "                    decline_analysis_col = f'{criteria_col}_{int(decline_stat*100)}th_pct'\n",
    "                    final_periods_fulfill = final_periods.query(f'{criteria_col}<{decline_analysis_col}')\n",
    "                    if final_periods_fulfill.shape[0] == post_period_length:\n",
    "                        df_candidates = pd.concat([df_candidates, df_candidate])\n",
    "    return df_candidates\n",
    "\n",
    "def GetUniqueTruckFactor(indir_truck):\n",
    "    df_truckfactor = pd.concat([pd.read_csv(file).assign(repo_name = file) for file in indir_truck.glob('*.csv')]).drop('Unnamed: 0', axis = 1).reset_index(drop = True)\n",
    "    df_truckfactor['repo'] = df_truckfactor['repo_name'].apply(lambda x: str(x).replace('drive/output/scrape/get_weekly_truck_factor/truckfactor_','').replace('.csv','').replace(\"_\",\"/\",1))\n",
    "    df_truckfactors_uq = df_truckfactor[['repo','authors']].drop_duplicates().dropna()\n",
    "    df_truckfactors_uq['authors_list'] = df_truckfactors_uq['authors'].apply(lambda x: x.split(\" | \"))\n",
    "    df_truckfactors_uq = df_truckfactors_uq.explode('authors_list')[['repo','authors_list']].drop_duplicates()\n",
    "\n",
    "    return df_truckfactors_uq\n",
    "\n",
    "def GetUniqueCommitters(indir_committers):\n",
    "    df_committers_profile = pd.concat([pd.read_csv(indir_committers / 'committers_info_push.csv', index_col = 0),\n",
    "                                       pd.read_csv(indir_committers / 'committers_info_pr.csv', index_col = 0)]).reset_index(drop = True)\n",
    "    \n",
    "    # this is just a temporary quick solution - need a better solution\n",
    "    df_committers_profile.dropna(inplace = True)\n",
    "    df_committers_profile['committer_info'] = df_committers_profile['committer_info'].parallel_apply(ast.literal_eval)\n",
    "    df_committers_profile['actor_id'] = df_committers_profile['committer_info'].apply(lambda x: x[1])\n",
    "    committers_merge_map = df_committers_profile[['name','repo','actor_id']].drop_duplicates()\n",
    "\n",
    "    return committers_merge_map\n",
    "\n",
    "\n",
    "def CleanCommittersInfo(indir_committers_info):\n",
    "    # TODO: edit file so it can handle pushes\n",
    "    df_committers_info = pd.concat([pd.read_csv(indir_committers_info / 'committers_info_pr.csv', index_col = 0).dropna(),\n",
    "                                    pd.read_csv(indir_committers_info / 'committers_info_push.csv', index_col = 0).dropna()])\n",
    "    df_committers_info['committer_info'] = df_committers_info['committer_info'].apply(literal_eval)\n",
    "    df_committers_info['commit_repo'] = df_committers_info['commit_repo'].parallel_apply(literal_eval)\n",
    "    # TODO: handle cleaning so that it can handle the other cases\n",
    "    df_committers_info = df_committers_info[df_committers_info['committer_info'].apply(lambda x: len(x)==4)]\n",
    "    df_committers_info['repo'] = df_committers_info['commit_repo'].apply(lambda x: list(set([ele.split(\"_\")[1] for ele in x])))\n",
    "    df_committers_info = df_committers_info.explode('repo')\n",
    "    df_committers_info['actor_name'] = df_committers_info['committer_info'].apply(lambda x: x[0])\n",
    "    df_committers_info['actor_id'] = df_committers_info['committer_info'].apply(lambda x: x[1])\n",
    "\n",
    "    committers_match = df_committers_info[['name','email','user_type','actor_name','actor_id','repo']].drop_duplicates()\n",
    "    committers_match.rename({'actor_id':'commit_author_id'}, axis = 1, inplace = True)\n",
    "\n",
    "    return committers_match"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63aa8bf2-4758-40ca-9721-12d9a148542e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: Pandarallel will run on 8 workers.\n",
      "INFO: Pandarallel will use Memory file system to transfer data between the main process and workers.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "81596ebb74ff421e8b0f04160ac8d191",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HBox(children=(IntProgress(value=0, description='0.00%', max=52906), Label(value='0 / 52906')))â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "pandarallel.initialize(progress_bar = True)\n",
    "indir = Path('drive/output/derived/major_contributor_prospects/contributor_data')\n",
    "indir_truck = Path('drive/output/scrape/get_weekly_truck_factor')\n",
    "indir_committers = Path('drive/output/scrape/link_committers_profile')\n",
    "outdir = Path('drive/output/derived/major_contributor_prospects/departed_contributors')\n",
    "\n",
    "major_months_list = [3,6,12]\n",
    "window_list = [732] #window_list = [367, 732, 1828]\n",
    "consecutive_periods_major_months_dict = {3: [6, 12],\n",
    "                                         6: [3, 6],\n",
    "                                         12: [2, 3, 4]}\n",
    "issue_col = 'issue_comments'\n",
    "pr_col = 'pr'\n",
    "criteria_col_list = [issue_col, pr_col]\n",
    "criteria_pct_list = [75, 90]\n",
    "general_pct_list = [25] #general_pct_list = [25, 50]\n",
    "\n",
    "post_period_length_list = [2] #post_period_length_list = [2, 4]\n",
    "decline_threshold_list = [.2]#decline_threshold_list = [.1, .2, .3]\n",
    "decline_pct_list = [.25] #decline_pct_list = [.1, .2]\n",
    "\n",
    "df_truckfactors_uq = GetUniqueTruckFactor(indir_truck)\n",
    "committers_merge_map = CleanCommittersInfo(indir_committers)\n",
    "truckfactor_factor_id = pd.merge(df_truckfactors_uq, committers_merge_map, how = 'left', left_on = ['repo','authors_list'], right_on = ['repo','name'])\n",
    "truck_indivs = truckfactor_factor_id[['repo','commit_author_id']].rename({'commit_author_id':'actor_id'}, axis = 1).drop_duplicates()\n",
    "\n",
    "identifiers = ['repo_name','actor_id','time_period']\n",
    "\n",
    "df_contributor_stats = pd.DataFrame(columns = ['major_months','window','criteria_col','criteria_pct','general_pct',\n",
    "                                               'consecutive_periods','post_period_length','decline_type','decline_stat',\n",
    "                                               'total_contributors','total_contributors_consecutive_criteria','total_final_candidates',\n",
    "                                               'num_total_projects','num_projects_with_one_departure', 'truck_factor_pct'])\n",
    "\n",
    "for major_months in major_months_list:\n",
    "    for window in window_list:\n",
    "        df_contributors = GetContributorData(indir, major_months, window)\n",
    "        num_contributors = ContributorCount(df_contributors)\n",
    "        for criteria_col in criteria_col_list:\n",
    "            for criteria_pct in criteria_pct_list:\n",
    "                for general_pct in general_pct_list:\n",
    "                    criteria_analysis_col = f'{criteria_col}_{criteria_pct}th_pct'\n",
    "                    criteria_general_col = f'general_{criteria_col}_{general_pct}th_pct'\n",
    "                    criteria_cols = [criteria_col, criteria_analysis_col, criteria_general_col]\n",
    "                    criteria_analysis_decline_cols = [f'{criteria_col}_{int(100*decline_pct)}th_pct' for decline_pct in decline_pct_list]\n",
    "                    criteria_cols.extend(criteria_analysis_decline_cols)\n",
    "\n",
    "                    analysis_cols = identifiers + criteria_cols\n",
    "                    df_analysis = df_contributors[analysis_cols]\n",
    "\n",
    "                    potential_major_contributors = df_analysis.query(f'{criteria_col}>{criteria_analysis_col} & {criteria_col}>{criteria_general_col}')\\\n",
    "                        [['actor_id','repo_name']].drop_duplicates()\n",
    "                    df_potential = pd.merge(df_analysis, potential_major_contributors, on = ['actor_id','repo_name'])\n",
    "                    df_potential = df_potential.assign(criteria_exceed = (df_potential[criteria_col]>df_potential[criteria_analysis_col]).astype(int))\n",
    "                    df_potential = df_potential.groupby(['actor_id','repo_name']).apply(GetConsecutiveSum).reset_index(drop = True)\n",
    "\n",
    "                    consecutive_periods_list = consecutive_periods_major_months_dict[major_months]\n",
    "                    for consecutive_periods in consecutive_periods_list:\n",
    "                        has_consecutive_periods = df_potential.query(f'consecutive_periods>={consecutive_periods}')[['actor_id','repo_name']].drop_duplicates()\n",
    "                        df_potential_consecutive = pd.merge(df_potential, has_consecutive_periods)\n",
    "                        num_consecutive_periods = ContributorCount(df_potential_consecutive)\n",
    "\n",
    "                        departure_candidates = df_potential_consecutive[['actor_id','repo_name', 'consecutive_periods', 'time_period']]\\\n",
    "                            .sort_values('consecutive_periods', ascending = False)\\\n",
    "                            .drop_duplicates(['actor_id','repo_name'])\\\n",
    "                            .reset_index(drop = True)\\\n",
    "                            .rename({'time_period':'final_period'}, axis = 1)\n",
    "            \n",
    "                        for post_period_length in post_period_length_list:\n",
    "                            for decline_type in ['threshold_mean','threshold_pct']:\n",
    "                                decline_stat_list = decline_threshold_list if decline_type == \"threshold_mean\" else decline_pct_list\n",
    "                                for decline_stat in decline_stat_list:\n",
    "                                    break\n",
    "                                    df_candidates = GetCandidates(departure_candidates, post_period_length, decline_type, decline_stat, criteria_analysis_col, criteria_col, df_potential_consecutive)\n",
    "                                    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "3ff62b41-b391-4231-a45c-bb16dc436bbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_candidates.columns)\n",
    "num_final_contributors = ContributorCount(df_candidates)\n",
    "\n",
    "append_index = df_contributor_stats.shape[0]\n",
    "repo_count = df_candidates[['repo_name']].drop_duplicates().shape[0]\n",
    "uq_candidates = df_candidates[['actor_id','repo_name']].drop_duplicates()\n",
    "major_contributors = df_potential_consecutive[['actor_id','repo_name']].drop_duplicates()\n",
    "one_departure_repos = np.sum(uq_candidates['repo_name'].value_counts()==1)\n",
    "\n",
    "all_candidate_repos = df_candidates['repo_name'].unique().tolist()\n",
    "truck_indivs_repo = truck_indivs[truck_indivs['repo'].isin(all_candidate_repos)]\n",
    "truck_indivs_repo['actor_id'] = pd.to_numeric(truck_indivs_repo['actor_id'])\n",
    "truck_merged = pd.merge(truck_indivs_repo, uq_candidates.assign(presence=1), how = 'left', left_on = ['repo','actor_id'], right_on = ['repo_name','actor_id'])\n",
    "pct_truck_factor = 1-np.mean(truck_merged['presence'].isna())\n",
    "pct_truck_factor = 1-np.mean(truck_merged['presence'].isna())\n",
    "\n",
    "df_contributor_stats.loc[append_index, \n",
    "    ['major_months','window','criteria_col','criteria_pct','general_pct',\n",
    "     'consecutive_periods','post_period_length','decline_type','decline_stat',\n",
    "     'total_contributors','total_contributors_consecutive_criteria','total_final_candidates',\n",
    "     'num_total_projects','num_projects_with_one_departure','truck_factor_pct']] = \\\n",
    "    [major_months, window, criteria_col, criteria_pct, general_pct, consecutive_periods,\n",
    "     post_period_length, decline_type, decline_stat, num_contributors, num_consecutive_periods,\n",
    "     num_final_contributors, repo_count, one_departure_repos, pct_truck_factor]\n",
    "\n",
    "filename = f'contributors_major_months{major_months}_window{window}criteria_{criteria_col}_{criteria_pct}pct_general{general_pct}pct_consecutive{consecutive_periods}_post_period{post_period_length}'\n",
    "decline_suffix = f\"threshold_mean_{decline_stat}\" if decline_type == 'threshold_mean' else f\"threshold_pct_{decline_stat}\"\n",
    "decline_suffix = f\"{decline_suffix}.parquet\"\n",
    "filename = filename + decline_suffix\n",
    "df_candidates.to_parquet(outdir / filename)\n",
    "print(f\"exported {filename}\")\n",
    "\n",
    "df_contributor_stats.to_csv(outdir / 'contributor_stats_summary.csv')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "oss_hierarchy",
   "language": "python",
   "name": "oss_hierarchy"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
