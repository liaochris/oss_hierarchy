{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3aec51bb-6557-4e09-b7b8-9f4ecc9beb5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir('../')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1b6a0eb1-0bba-4d4e-a4c5-a64685ff23e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from source.lib.helpers import *\n",
    "import pandas as pd\n",
    "import re\n",
    "import warnings\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "pd.set_option('display.max_columns', None)\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "807e6bcf-275b-4779-80f0-981fac629cc6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: Pandarallel will run on 11 workers.\n",
      "INFO: Pandarallel will use standard multiprocessing data transfer (pipe) to transfer data between the main process and workers.\n"
     ]
    }
   ],
   "source": [
    "from source.derived.contributor_stats.calculate_contributions import LinkCommits, AssignPRAuthorship, AddPROpener, CalculateCommitAuthorStats, CleanCommittersInfo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fc0be729-c8fb-4df2-b62a-a36033a7757b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ProcessEventStudyPanel(df):\n",
    "    df = pd.read_parquet('issue/event_study_panel.parquet')\n",
    "    df_departure_date = df.query('treated == 1')[['repo_name','treatment_period','departed_actor_id']].drop_duplicates()\n",
    "\n",
    "    # ultimately repo_list should be made all repos\n",
    "    repo_list = df[df['treated']]['repo_name'].unique().tolist()\n",
    "    repo_filter = [(\"repo_name\", \"in\", repo_list)]\n",
    "    return df_departure_date, repo_list, repo_filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "61d60d6f-c0d0-445c-b044-f6142e075c06",
   "metadata": {},
   "outputs": [],
   "source": [
    "def CreateProblemMaps(df_problems):\n",
    "    issue_problem_map = df_problems.explode('issues')[['repo_name','problem_id','issues']].dropna()\n",
    "    pr_problem_map = df_problems.explode('prs')[['repo_name','problem_id','prs']].dropna()\n",
    "    return issue_problem_map, pr_problem_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c99f8aa8-ce4e-4581-bdaa-2eaf6d34b7bd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def ExtractImportantActors(df_contributor_characteristics):\n",
    "    df_contributor_characteristics['actor_id'] = pd.to_numeric(df_contributor_characteristics['actor_id'])\n",
    "    df_important_actors = (\n",
    "        df_contributor_characteristics\n",
    "        .loc[df_contributor_characteristics['important'] == 1]\n",
    "        .groupby(['repo_name', 'time_period'])['actor_id']\n",
    "        .agg(RemoveDuplicatesFlattened)\n",
    "        .reset_index(name='important_actors')\n",
    "    )\n",
    "    df_important_actors = (\n",
    "        df_important_actors\n",
    "        .groupby('repo_name', group_keys=False)\n",
    "        .apply(lambda g: g.sort_values('time_period').assign(\n",
    "            important_actors_rolling=[\n",
    "                RemoveDuplicatesFlattened(\n",
    "                    g['important_actors']\n",
    "                     .iloc[max(0, i - rolling_periods + 1): i + 1]\n",
    "                     .tolist()\n",
    "                )\n",
    "                for i in range(len(g))\n",
    "            ]\n",
    "        ))\n",
    "    )\n",
    "    return df_important_actors\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "47d2e574-ddf6-42e0-8d84-3bfb7fd8df9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def CreateBotList(df_issue_raw, df_pr_raw):\n",
    "    bots = df_issue_raw.query('actor_type == \"Bot\"')['actor_id'].unique().tolist()\n",
    "    bots = bots + df_pr_raw.query('pr_merged_by_type == \"Bot\"')['pr_merged_by_id'].unique().tolist()\n",
    "    return bots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "82d4ca22-f46d-44cb-9a9c-8c6f1b7757ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def SeparatePRsIssues(df_pr_raw, df_issue_raw, time_period):\n",
    "    pr_index = df_pr_raw[['repo_name','pr_number']].drop_duplicates().set_index(['repo_name','pr_number']).index\n",
    "    df_issue = df_issue_raw.loc[~df_issue_raw.set_index(['repo_name','issue_number']).index.isin(pr_index)]\n",
    "    df_issue = ImputeTimePeriod(df_issue, time_period)\n",
    "    \n",
    "    df_pr = ImputeTimePeriod(df_pr_raw, time_period)\n",
    "    df_pr_comments = df_issue_raw.loc[df_issue_raw.set_index(['repo_name','issue_number']).index.isin(pr_index)]\n",
    "    df_pr_comments = ImputeTimePeriod(df_pr_comments, time_period)\n",
    "\n",
    "    return df_issue, df_pr, df_pr_comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "17c66918-c4ad-4b66-93ae-a6ebf1d27297",
   "metadata": {},
   "outputs": [],
   "source": [
    "def CreateDiscussion(df_issue, problem_map, map_merge_col, bots):\n",
    "    df = df_issue.query('issue_action != \"closed\" & actor_id not in @bots').copy()\n",
    "    df = pd.merge(df, problem_map, how='left', left_on=['repo_name', 'issue_number'], right_on = ['repo_name',map_merge_col])\n",
    "    assert df['problem_id'].isna().mean() < 0.01\n",
    "    df = df[df['problem_id'].notna()]\n",
    "\n",
    "    df['body'] = df['issue_body'].fillna('') + df['issue_comment_body'].fillna('')\n",
    "    df = AddBodyMetrics(df)\n",
    "    return df\n",
    "    \n",
    "def AddBodyMetrics(df):\n",
    "    \"\"\"\n",
    "    Given a DataFrame with a 'body' column, add the following columns:\n",
    "      - cleaned_body: text after applying CleanText\n",
    "      - body_len: character length of 'body'\n",
    "      - cleaned_body_len: character length of 'cleaned_body'\n",
    "      - body_wd_len: word count of 'body'\n",
    "      - cleaned_body_wd_len: word count of 'cleaned_body'\n",
    "    \"\"\"\n",
    "    df['cleaned_body'] = df['body'].apply(CleanText)\n",
    "    df['body_len'] = df['body'].str.len()\n",
    "    df['cleaned_body_len'] = df['cleaned_body'].str.len()\n",
    "    df['body_wd_len'] = df['body'].str.split().str.len()\n",
    "    df['cleaned_body_wd_len'] = df['cleaned_body'].str.split().str.len()\n",
    "    \n",
    "    return df\n",
    "\n",
    "def CleanText(text):\n",
    "    fenced_code_block_pattern = re.compile(r'```[\\s\\S]*?```')\n",
    "    if not isinstance(text, str):\n",
    "        return ''\n",
    "    no_code = fenced_code_block_pattern.sub('', text)\n",
    "    cleaned = re.sub(r'[\\r\\n\\t\\f\\v]', ' ', no_code)\n",
    "    return cleaned.strip()\n",
    " \n",
    "\n",
    "def GetUniqueContributors(df_issue_disc, actor_type, close_time = True):\n",
    "    group_cols = ['repo_name', 'problem_id', 'time_period']\n",
    "    if close_time:\n",
    "        group_cols = group_cols + ['close_time','comment_close_time']\n",
    "    df_grouped = df_issue_disc.groupby(group_cols)['actor_id'].agg(\n",
    "        RemoveDuplicatesFlattened).reset_index().rename(columns={'actor_id':f'{actor_type}_actors'})\n",
    "    return df_grouped\n",
    "\n",
    "def GetContributorWeights(df_issue_disc, actor_type, close_time = True):\n",
    "    group_cols = ['repo_name', 'problem_id', 'time_period', 'actor_id']\n",
    "    if close_time:\n",
    "        group_cols + ['close_time','comment_close_time']\n",
    "    df = df_issue_disc.groupby(group_cols).agg(\n",
    "        contributions=('body', 'count'),\n",
    "        contributions_text_wt=('body_len', 'sum'),\n",
    "        contributions_clean_text_wt=('cleaned_body_len', 'sum'),\n",
    "        contributions_text_wd_wt=('body_wd_len', 'sum'),\n",
    "        contributions_clean_text_wd_wt=('cleaned_body_wd_len', 'sum')).reset_index()\n",
    "\n",
    "    df_grouped = df.groupby(['repo_name', 'problem_id', 'time_period']).apply(\n",
    "        lambda g: {\n",
    "            r['actor_id']: {\n",
    "                'contributions': r['contributions'],\n",
    "                'contributions_text_wt': r['contributions_text_wt'],\n",
    "                'contributions_clean_text_wt': r['contributions_clean_text_wt'],\n",
    "                'contributions_text_wd_wt': r['contributions_text_wd_wt'],\n",
    "                'contributions_clean_text_wd_wt': r['contributions_clean_text_wd_wt']\n",
    "            } for _, r in g.iterrows()\n",
    "        }\n",
    "    ).reset_index(name=f'{actor_type}_contributions_dict')\n",
    "    return df_grouped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "256a526f-43bc-4539-b977-d76f49c2f901",
   "metadata": {},
   "outputs": [],
   "source": [
    "def FilterPRComments(df_pr, df_pr_comments):\n",
    "    opened_prs = df_pr.loc[df_pr['pr_action'] == 'opened', ['repo_name', 'pr_number']]\n",
    "    opened_keys = set(zip(opened_prs['repo_name'], opened_prs['pr_number']))\n",
    "    \n",
    "    df_pr_comments['_key'] = list(zip(df_pr_comments['repo_name'], df_pr_comments['issue_number']))\n",
    "    mask = (df_pr_comments['issue_action'] == 'opened') & df_pr_comments['_key'].isin(opened_keys)\n",
    "    df_pr_comments = df_pr_comments.loc[~mask].drop(columns='_key')\n",
    "    return df_pr_comments\n",
    "\n",
    "def CreatePRDiscussion(df_pr, pr_problem_map, bots):\n",
    "    df = df_pr.query('actor_id not in @bots').copy()\n",
    "    df = df.merge(pr_problem_map, how='left', left_on=['repo_name', 'pr_number'], right_on=['repo_name', 'prs'])\n",
    "    assert df['problem_id'].notna().mean() > 0.99\n",
    "    df = df[df['problem_id'].notna()]\n",
    "    \n",
    "    df['pr_body'] = df['pr_body'].fillna(df.groupby('pr_number')['pr_body'].transform('first'))\n",
    "    df['pr_title'] = df['pr_title'].fillna(df.groupby('pr_number')['pr_title'].transform('first'))\n",
    "    df = df.query('pr_action not in [\"closed\",\"reopened\",\"synchronize\"]').copy()\n",
    "    \n",
    "    def ComputePRBody(row):\n",
    "        if row['type'] == \"PullRequestEvent\":\n",
    "            return row['pr_body'] or \"\"\n",
    "        if row['type'] == \"PullRequestReviewEvent\":\n",
    "            return row['pr_review_body'] or \"\"\n",
    "        return row['pr_review_comment_body'] or \"\"\n",
    "    \n",
    "    df['body'] = df.apply(ComputePRBody, axis=1)\n",
    "    df = AddBodyMetrics(df)\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0e29c53a-fc70-4fdb-9bbd-8033f08a9cd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def CombineContributionDicts(issue, pr):\n",
    "    if not isinstance(issue, dict) and not isinstance(pr, dict):\n",
    "        return np.nan\n",
    "    i, p = (issue if isinstance(issue, dict) else {}), (pr if isinstance(pr, dict) else {})\n",
    "    return {\n",
    "        k: CombineContributionDicts(i.get(k, {}), p.get(k, {}))\n",
    "            if isinstance(i.get(k), dict) or isinstance(p.get(k), dict)\n",
    "        else ( (i.get(k) if isinstance(i.get(k), (int, float)) else 0)\n",
    "               + (p.get(k) if isinstance(p.get(k), (int, float)) else 0) )\n",
    "        for k in set(i) | set(p)\n",
    "    }\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7fab2afb-bd5b-4f15-b270-2ac3550683a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def IndicatorCollaboration(df, imp_actor_col, collab_col):\n",
    "    df[collab_col] = [\n",
    "        int(len(set(all_acts) & set(imp_acts)) >= 2)\n",
    "        for all_acts, imp_acts in zip(df['all_actors'], df[imp_actor_col])\n",
    "    ]\n",
    "    return df\n",
    "\n",
    "def KeyIndicatorCollaboration(df, imp_actor_col, collab_col):\n",
    "    df[collab_col] = [\n",
    "        int(len(set(all_acts) & set(imp_acts)) >= 2) if dep_act in all_acts else np.nan\n",
    "        for all_acts, imp_acts, dep_act in zip(df['all_actors'], df[imp_actor_col], df['departed_actor_id'])\n",
    "    ]\n",
    "    return df\n",
    "\n",
    "def OtherIndicatorCollaboration(df, imp_actor_col, collab_col):\n",
    "    df[collab_col] = [\n",
    "        int(len(set(all_acts) & set(imp_acts)) >= 2) if dep_act not in all_acts else np.nan\n",
    "        for all_acts, imp_acts, dep_act in zip(df['all_actors'], df[imp_actor_col], df['departed_actor_id'])\n",
    "    ]\n",
    "    return df\n",
    "\n",
    "def ContinuousCollaboration(df, imp_actor_col, contr_col, collab_col):\n",
    "    def compute_collab(row):\n",
    "        contr_dict = row['contributions_dict']\n",
    "        imp_acts = set(row[imp_actor_col])\n",
    "        common = imp_acts & set(contr_dict.keys())\n",
    "        if len(common) < 2:\n",
    "            return 0\n",
    "        values = [contr_dict[actor].get(contr_col, 0) for actor in common]\n",
    "        top_two = sorted(values, reverse=True)[:2]\n",
    "        total = sum(top_two)\n",
    "        if total == 0:\n",
    "            return 0\n",
    "        top_two_shares = [v / total for v in top_two]\n",
    "        # Compute ((v1^2 + v2^2) / (2 * 1/4))\n",
    "        return 2 * sum(v**2 for v in top_two_shares)\n",
    "    \n",
    "    df[collab_col] = df.apply(compute_collab, axis=1)\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30140843-8dee-4e0e-a155-e2292ad33ab2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#def Main():\n",
    "\n",
    "time_period = 6\n",
    "rolling_periods = 3\n",
    "df_problems = pd.read_parquet('issue/matched_problems_strict.parquet')\n",
    "#df_problems = pd.read_parquet('issue/matched_problems_both.parquet')\n",
    "#df_problems = pd.read_parquet('issue/matched_problems_single.parquet')\n",
    "df_contributor_characteristics = pd.read_parquet('drive/output/derived/graph_structure/contributor_characteristics.parquet')\n",
    "issue_problem_map, pr_problem_map = CreateProblemMaps(df_problems)\n",
    "df_important_actors = ExtractImportantActors(df_contributor_characteristics)\n",
    "df_departure_date, repo_list, repo_filter = ProcessEventStudyPanel(df_problems)\n",
    "\n",
    "df_issue_raw = pd.read_parquet('drive/output/derived/data_export/df_issue.parquet', filters=repo_filter)\n",
    "df_pr_raw = pd.read_parquet('drive/output/derived/data_export/df_pr.parquet', filters=repo_filter)\n",
    "df_pr_commits = pd.read_parquet('drive/output/derived/data_export/df_pr_commits.parquet', filters=repo_filter)\n",
    "\n",
    "bots = CreateBotList(df_issue_raw, df_pr_raw)\n",
    "df_issue, df_pr, df_pr_comments = SeparatePRsIssues(df_pr_raw, df_issue_raw, time_period)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "92850fe5-a2bc-43ab-82c7-710af1532165",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "df_issue_disc = CreateDiscussion(df_issue, issue_problem_map, 'issues', bots)\n",
    "df_issue_disc_contr = GetUniqueContributors(df_issue_disc,'issue', close_time = False)\n",
    "df_issue_disc_contr_wt = GetContributorWeights(df_issue_disc,'issue', close_time = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f202dccc-afc0-4a72-a0f3-1efe9763f59a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pr_comments = FilterPRComments(df_pr, df_pr_comments)\n",
    "df_pr_disc = CreatePRDiscussion(df_pr, pr_problem_map, bots)\n",
    "df_pr_comments_disc = CreateDiscussion(df_pr_comments, pr_problem_map, 'prs', bots)\n",
    "\n",
    "sel_cols = ['repo_name','problem_id','time_period','actor_id','body','cleaned_body','body_len','cleaned_body_len','body_wd_len','cleaned_body_wd_len']\n",
    "df_pr_full_disc = pd.concat([df_pr_disc[sel_cols], df_pr_comments_disc[sel_cols]], ignore_index=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "17b531b2-4029-482d-8159-3b33170e58d6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "type\n",
       "PullRequestEvent                 451088\n",
       "PullRequestReviewCommentEvent    394579\n",
       "PullRequestReviewEvent           198283\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_pr['type'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "42d34a38-90d9-4c0a-aa1e-db8713d58c32",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "### GENERATE CLOSE TIME\n",
    "\n",
    "df_pr_open_date = pd.concat([df_pr, df_pr_comments.rename(columns={'issue_number':'pr_number'})]).query('pr_action == \"closed\" | issue_action == \"closed\"').sort_values(\n",
    "    ['repo_name','pr_number','created_at']).drop_duplicates(\n",
    "    ['repo_name','pr_number'])[['repo_name','pr_number','created_at']].rename(columns={'created_at':'closed_at'})\n",
    "df_pr_close_date = pd.concat([df_pr, df_pr_comments.rename(columns={'issue_number':'pr_number'})]).query('pr_action == \"opened\" | issue_action == \"opened\"').sort_values(\n",
    "    ['repo_name','pr_number','created_at']).drop_duplicates(\n",
    "    ['repo_name','pr_number'])[['repo_name','pr_number','created_at']].rename(columns={'created_at':'opened_at'})\n",
    "df_close_time = df_pr_open_date.merge(df_pr_close_date)\n",
    "df_close_time['close_time'] = (df_close_time['closed_at'] - df_close_time['opened_at']) / pd.Timedelta(days=1)\n",
    "pr_problem_map_close_time = pr_problem_map.merge(df_close_time.rename(columns={'pr_number':'prs'}))\n",
    "pr_problem_map_close_time_final = pr_problem_map_close_time.groupby(['repo_name','problem_id'], as_index = False)['close_time'].mean()\n",
    "df_pr_full_disc = pd.merge(df_pr_full_disc, pr_problem_map_close_time_final, how = 'left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "0ea2131d-dc30-4489-9fec-95992d9ae8e2",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'comment_close_time'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[24], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m df_pr_disc_contr \u001b[38;5;241m=\u001b[39m \u001b[43mGetUniqueContributors\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf_pr_full_disc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mpr\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      2\u001b[0m df_pr_disc_contr_wt \u001b[38;5;241m=\u001b[39m GetContributorWeights(df_pr_full_disc, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpr\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "Cell \u001b[0;32mIn[19], line 41\u001b[0m, in \u001b[0;36mGetUniqueContributors\u001b[0;34m(df_issue_disc, actor_type, close_time)\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m close_time:\n\u001b[1;32m     40\u001b[0m     group_cols \u001b[38;5;241m=\u001b[39m group_cols \u001b[38;5;241m+\u001b[39m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mclose_time\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcomment_close_time\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m---> 41\u001b[0m df_grouped \u001b[38;5;241m=\u001b[39m \u001b[43mdf_issue_disc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgroupby\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgroup_cols\u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mactor_id\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39magg(\n\u001b[1;32m     42\u001b[0m     RemoveDuplicatesFlattened)\u001b[38;5;241m.\u001b[39mreset_index()\u001b[38;5;241m.\u001b[39mrename(columns\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mactor_id\u001b[39m\u001b[38;5;124m'\u001b[39m:\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mactor_type\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_actors\u001b[39m\u001b[38;5;124m'\u001b[39m})\n\u001b[1;32m     43\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m df_grouped\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/site-packages/pandas/core/frame.py:9183\u001b[0m, in \u001b[0;36mDataFrame.groupby\u001b[0;34m(self, by, axis, level, as_index, sort, group_keys, observed, dropna)\u001b[0m\n\u001b[1;32m   9180\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m level \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m by \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   9181\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou have to supply one of \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mby\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m and \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlevel\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m-> 9183\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mDataFrameGroupBy\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   9184\u001b[0m \u001b[43m    \u001b[49m\u001b[43mobj\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   9185\u001b[0m \u001b[43m    \u001b[49m\u001b[43mkeys\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mby\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   9186\u001b[0m \u001b[43m    \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   9187\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlevel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlevel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   9188\u001b[0m \u001b[43m    \u001b[49m\u001b[43mas_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mas_index\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   9189\u001b[0m \u001b[43m    \u001b[49m\u001b[43msort\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msort\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   9190\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgroup_keys\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup_keys\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   9191\u001b[0m \u001b[43m    \u001b[49m\u001b[43mobserved\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mobserved\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   9192\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdropna\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdropna\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   9193\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/site-packages/pandas/core/groupby/groupby.py:1329\u001b[0m, in \u001b[0;36mGroupBy.__init__\u001b[0;34m(self, obj, keys, axis, level, grouper, exclusions, selection, as_index, sort, group_keys, observed, dropna)\u001b[0m\n\u001b[1;32m   1326\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropna \u001b[38;5;241m=\u001b[39m dropna\n\u001b[1;32m   1328\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m grouper \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1329\u001b[0m     grouper, exclusions, obj \u001b[38;5;241m=\u001b[39m \u001b[43mget_grouper\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1330\u001b[0m \u001b[43m        \u001b[49m\u001b[43mobj\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1331\u001b[0m \u001b[43m        \u001b[49m\u001b[43mkeys\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1332\u001b[0m \u001b[43m        \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1333\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlevel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlevel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1334\u001b[0m \u001b[43m        \u001b[49m\u001b[43msort\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msort\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1335\u001b[0m \u001b[43m        \u001b[49m\u001b[43mobserved\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mobserved\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mlib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mno_default\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mobserved\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1336\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdropna\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdropna\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1337\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1339\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m observed \u001b[38;5;129;01mis\u001b[39;00m lib\u001b[38;5;241m.\u001b[39mno_default:\n\u001b[1;32m   1340\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28many\u001b[39m(ping\u001b[38;5;241m.\u001b[39m_passed_categorical \u001b[38;5;28;01mfor\u001b[39;00m ping \u001b[38;5;129;01min\u001b[39;00m grouper\u001b[38;5;241m.\u001b[39mgroupings):\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/site-packages/pandas/core/groupby/grouper.py:1043\u001b[0m, in \u001b[0;36mget_grouper\u001b[0;34m(obj, key, axis, level, sort, observed, validate, dropna)\u001b[0m\n\u001b[1;32m   1041\u001b[0m         in_axis, level, gpr \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m, gpr, \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1042\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1043\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(gpr)\n\u001b[1;32m   1044\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(gpr, Grouper) \u001b[38;5;129;01mand\u001b[39;00m gpr\u001b[38;5;241m.\u001b[39mkey \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1045\u001b[0m     \u001b[38;5;66;03m# Add key to exclusions\u001b[39;00m\n\u001b[1;32m   1046\u001b[0m     exclusions\u001b[38;5;241m.\u001b[39madd(gpr\u001b[38;5;241m.\u001b[39mkey)\n",
      "\u001b[0;31mKeyError\u001b[0m: 'comment_close_time'"
     ]
    }
   ],
   "source": [
    "df_pr_disc_contr = GetUniqueContributors(df_pr_full_disc, 'pr')\n",
    "df_pr_disc_contr_wt = GetContributorWeights(df_pr_full_disc, 'pr')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d492ffd1-2d71-4255-853a-2acf0ae4708a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_all_disc_contr = pd.merge(df_pr_disc_contr.merge(df_pr_disc_contr_wt), df_issue_disc_contr.merge(df_issue_disc_contr_wt), how = 'outer')\n",
    "df_problems_contr = pd.merge(df_problems, df_all_disc_contr).merge(df_important_actors).merge(df_departure_date)\n",
    "\n",
    "df_problems_contr['important_actors'] = df_problems_contr.apply(lambda x: list(set(x['important_actors'] + [x['departed_actor_id']])), axis = 1)\n",
    "df_problems_contr['issue_actors'] = df_problems_contr['issue_actors'].apply(lambda x: x if isinstance(x, list) else [])\n",
    "df_problems_contr['pr_actors'] = df_problems_contr['pr_actors'].apply(lambda x: x if isinstance(x, list) else [])\n",
    "df_problems_contr['all_actors'] = [list(dict.fromkeys(issue + pr)) for issue, pr in zip(df_problems_contr['issue_actors'], df_problems_contr['pr_actors'])]\n",
    "df_problems_contr['problem_contr_count'] = df_problems_contr['all_actors'].apply(len)\n",
    "df_problems_contr['contributions_dict'] = df_problems_contr.apply(lambda x: CombineContributionDicts(x['issue_contributions_dict'], x['pr_contributions_dict']), axis = 1)\n",
    "df_problems_contr['total_prob_contr'] = df_problems_contr['contributions_dict'].apply(lambda d: sum(v.get('contributions', 0) for v in d.values()))\n",
    "\n",
    "df_all_actors_period = df_problems_contr.groupby(['repo_name','time_period'])['all_actors'].agg(RemoveDuplicatesFlattened).reset_index(name = 'all_actors_period')\n",
    "df_problems_contr = pd.merge(df_problems_contr, df_all_actors_period)\n",
    "df_problems_contr['important_actors_rolling'] = df_problems_contr.apply(lambda x: list(set(x['important_actors_rolling'] + [x['departed_actor_id']])), axis = 1)\n",
    "df_problems_contr['important_actors_rolling'] = df_problems_contr.apply(lambda x: [ele for ele in x['important_actors_rolling'] if ele in x['all_actors_period']], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bd37a4d-630a-4a25-a3e5-296dc5e982bc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_problems_contr = IndicatorCollaboration(df_problems_contr, 'important_actors', 'ind_collab')\n",
    "df_problems_contr = IndicatorCollaboration(df_problems_contr, 'important_actors_rolling', 'ind_collab_roll')\n",
    "df_problems_contr = KeyIndicatorCollaboration(df_problems_contr, 'important_actors_rolling', 'ind_key_collab_roll')\n",
    "df_problems_contr = OtherIndicatorCollaboration(df_problems_contr, 'important_actors_rolling', 'ind_other_collab_roll')\n",
    "df_problems_contr = ContinuousCollaboration(df_problems_contr, 'important_actors_rolling', 'contributions', 'cont_collab_roll')\n",
    "df_problems_contr = ContinuousCollaboration(df_problems_contr, 'important_actors_rolling', 'contributions_clean_text_wt', 'cont_collab_clean_char_roll')\n",
    "df_problems_contr = ContinuousCollaboration(df_problems_contr, 'important_actors_rolling', 'contributions_clean_text_wd_wt', 'cont_collab_clean_wd_roll')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ff1c2b9-7cee-4de5-9351-34a7ec6d2032",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_pr_comments_dis\n",
    "# Get number of REVIEW COMMENTS\n",
    "pr_review_comment_count = df_pr_disc.query('type == \"PullRequestReviewEvent\" | type == \"PullRequestReviewCommentEvent\"').groupby(\n",
    "    ['repo_name','time_period','pr_number'],as_index = False)['created_at'].count().rename(columns={'created_at':'review_comment_count'})\n",
    "# Get number of REVIEWS\n",
    "pr_review_count = df_pr_disc.query('type == \"PullRequestReviewEvent\"').groupby(\n",
    "    ['repo_name','time_period','pr_number'],as_index = False)['created_at'].count().rename(columns={'created_at':'review_count'})\n",
    "\n",
    "df_pr_review_data = pd.merge(pr_review_comment_count, pr_review_count, how = 'left')\n",
    "pr_problem_map_review = pr_problem_map.merge(df_pr_review_data.rename(columns={'pr_number':'prs'}))\n",
    "pr_problem_map_review_final = pr_problem_map_review.groupby(['repo_name','problem_id'], as_index = False)[['review_count','review_comment_count']].mean()\n",
    "\n",
    "\n",
    "df_problems_contr = df_problems_contr.merge(pr_problem_map_review_final, how = 'left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ec1709e-9cd0-44d4-b3e6-9f5a2fdda47d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_problems_contr_pre_period = df_problems_contr.query('time_period<treatment_period')\n",
    "# Identify repo_names where every row has at least 2 important_actors\n",
    "repo_valid = (\n",
    "    df_problems_contr.query('time_period<treatment_period')\n",
    "    .groupby('repo_name')['important_actors_rolling']\n",
    "    .apply(lambda s: s.str.len().min() >= 2)\n",
    ")\n",
    "\n",
    "# Filter to keep only those repo_names\n",
    "valid_repos = repo_valid[repo_valid].index\n",
    "df_problems_contr_filtered = df_problems_contr[df_problems_contr['repo_name'].isin(valid_repos)]\n",
    "df_problems_contr_filtered_pre_period = df_problems_contr_filtered.query('time_period<treatment_period')\n",
    "print(f\"{len(valid_repos)} projects have at least 2 important contributors (rolling) always in the pre-period, out of a total of {len(repo_valid)} projects\")\n",
    "print(f\"They represents {df_problems_contr_filtered_pre_period.shape[0]} out of {df_problems_contr_pre_period.shape[0]} problems in the pre period \\\n",
    "({df_problems_contr_filtered_pre_period.shape[0]/df_problems_contr_pre_period.shape[0]:.2f})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaae98b5-f1ee-48ac-8cb8-a10de891b86a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ConstructPROpeners(indir_committers_info, author_thresh, df_pr, df_pr_commits):\n",
    "    commit_cols = ['commits','commit additions','commit deletions','commit changes total','commit files changed count']\n",
    "    \n",
    "    committers_match = CleanCommittersInfo(indir_committers_info)\n",
    "    committers_match = committers_match[committers_match['actor_name'].apply(lambda x: \"[bot]\" in x)]\n",
    "\n",
    "    df_pr_commit_stats = LinkCommits(df_pr, df_pr_commits, committers_match, commit_cols, 'pr')\n",
    "    df_pr_commit_author_stats = AssignPRAuthorship(df_pr_commit_stats, df_pr, author_thresh, commit_cols)\n",
    "    df_pr_commit_author_stats = AddPROpener(df_pr_commit_author_stats, df_pr)\n",
    "    df_pr_commit_author_stats.drop('actor_id', inplace = True, axis = 1)\n",
    "    df_pr_commit_author_stats['commit_author_actor_id'] = df_pr_commit_author_stats.apply(\n",
    "        lambda x: x['commit_author_id'] if int(x['commit_author_id']) != x['pr_opener_actor_id'] else np.nan, axis = 1)\n",
    "    df_pr_commit_author_types = df_pr_commit_author_stats.query('~commit_author_actor_id.isna()').groupby(['repo_name','pr_number'], as_index = False)[['commit_author_actor_id']].agg(list)\n",
    "    df_pr_commit_author_types['commit_author_actor_id'] = df_pr_commit_author_types['commit_author_actor_id'].apply(lambda x: list(set([ele for ele in x if not pd.isnull(ele)])))\n",
    "    df_pr_opener = df_pr.query('pr_action == \"opened\" & type == \"PullRequestEvent\"').drop_duplicates(['repo_name','pr_number'])[['repo_name','pr_number','actor_id']].rename(columns={'actor_id':'pr_opener_id'})\n",
    "    df_pr_opener = pd.merge(df_pr_opener, df_pr_commit_author_types, how = 'left')\n",
    "    return df_pr_opener\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faef29b7-b819-4fad-adfa-165495e20838",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "indir_committers_info = Path('drive/output/scrape/link_committers_profile')\n",
    "author_thresh = 1/3\n",
    "df_pr_opener = ConstructPROpeners(indir_committers_info, author_thresh, df_pr, df_pr_commits)\n",
    "df_pr_opener['pr_number'] = df_pr_opener['pr_number'].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "811c0c4b-01b8-4220-a7bd-2489433bfa56",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "lookup = {\n",
    "    repo: dict(zip(g.pr_number, zip(g.pr_opener_id, g.commit_author_actor_id)))\n",
    "    for repo, g in df_pr_opener.groupby('repo_name')\n",
    "}\n",
    "\n",
    "def _unpack(r):\n",
    "    prs = r.prs\n",
    "    pairs = [lookup.get(r.repo_name, {}).get(pr) for pr in prs]\n",
    "    pairs = [p for p in pairs if p]\n",
    "    if not pairs:\n",
    "        return [], [], []\n",
    "    openers, authors = zip(*pairs)\n",
    "    openers = [ele for ele in openers if not pd.isnull(ele)]\n",
    "    authors = set([ele[0] if type(ele) == list else ele for ele in authors])\n",
    "    authors = [ele for ele in authors if not pd.isnull(ele)]\n",
    "    oset, aset = set(openers), set(authors)\n",
    "    return list(oset), list(aset), list(oset | aset)\n",
    "\n",
    "df_problems_contr_filtered[['pr_opener','commit_author','pr_authors']] = pd.DataFrame(\n",
    "    [ _unpack(row) for row in df_problems_contr_filtered.itertuples() ],\n",
    "    index=df_problems_contr_filtered.index,\n",
    "    columns=['pr_opener','commit_author','pr_authors']\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fe7be7b-2b1e-4291-a09a-e424e9d5af0c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "for col in ['pr_contributions_dict','issue_contributions_dict','contributions_dict']:\n",
    "    # ensure every entry is at least {} and then dump to JSON\n",
    "    df_problems_contr_filtered[col] = (\n",
    "        df_problems_contr_filtered[col]\n",
    "        .apply(lambda x: x if isinstance(x, dict) else {})\n",
    "        .apply(json.dumps)\n",
    "    )\n",
    "\n",
    "# now this will work\n",
    "df_problems_contr_filtered.to_parquet('issue/filtered_problem_data.parquet', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b387b682-1540-4923-82f6-51a324502f49",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "SummarizeAndPlotCollaboration(df_problems_contr_pre_period, last_n=5)\n",
    "print(\"Distribution of problem count in all project-time periods pre-departure\")\n",
    "print(pd.DataFrame(df_problems_contr_pre_period[['repo_name','time_period']].value_counts().describe()).T.round(2))\n",
    "print(\"Distribution of avg. # of contributors per problem  in all project-time periods pre-departure\")\n",
    "print(pd.DataFrame(df_problems_contr_pre_period.groupby(['repo_name','time_period'])['problem_contr_count'].mean().describe()).T.round(2))\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60b817e4-5aac-406d-9551-ea4abf5dd2a6",
   "metadata": {},
   "source": [
    "# BELOW IS ANALYSIS, move into separate script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0488f8c5-de8c-423e-8b81-a3af54381a8b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e18af6c-38a3-4d63-ba91-5c4baee349fd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "corr = df_problems_contr_filtered_pre_period[['ind_collab', 'cont_collab_roll']].corr().iloc[0, 1]\n",
    "corr_word = df_problems_contr_filtered_pre_period[['ind_collab', 'cont_collab_clean_wd_roll']].corr().iloc[0, 1]\n",
    "corr_char = df_problems_contr_filtered_pre_period[['ind_collab', 'cont_collab_clean_char_roll']].corr().iloc[0, 1]\n",
    "print(f\"At the problem level, the correlation between the indicator and continuous collaboration metric is {corr:.2f}\")\n",
    "print(f\"At the problem level, the correlation between the indicator and continuous collaboration metric (using words as shares) is {corr_word:.2f}\")\n",
    "print(f\"At the problem level, the correlation between the indicator and continuous collaboration metric (using characters as shares) is {corr_char:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0263d406-39aa-4e8b-aca0-f65a8bddb78d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def PrintMetricSummary(time_series, recent_series, metric_name, last_n):\n",
    "    print(f\"Distribution of {metric_name} per project-period, pre-departure\")\n",
    "    print(time_series.describe().to_frame().T.round(2))\n",
    "    print(f\"Std dev across all project-periods: {time_series.std():.2f}\")\n",
    "    print(f\"Std dev across most recent {last_n} periods: {recent_series.std():.2f}\")\n",
    "    print(f\"Std dev across projects (avg over last {last_n} periods): {recent_series.groupby('repo_name').mean().std():.2f}\")\n",
    "\n",
    "def SummarizeAndPlotCollaboration(df_problems, metric_col, display_name, last_n, ax, weight_col=None):\n",
    "    df = df_problems.copy()\n",
    "    df[display_name] = df[metric_col]\n",
    "\n",
    "    def compute_time_series():\n",
    "        if weight_col:\n",
    "            return df.groupby(['repo_name', 'time_period']).apply(lambda g: (g[display_name] * g[weight_col]).sum() / g[weight_col].sum())\n",
    "        return df.groupby(['repo_name', 'time_period'])[display_name].mean()\n",
    "\n",
    "    ts = compute_time_series().rename(display_name)\n",
    "    recent = ts.groupby(level='repo_name').tail(last_n)\n",
    "    avg_recent = recent.groupby(level='repo_name').mean()\n",
    "\n",
    "    PrintMetricSummary(ts, recent, display_name, last_n)\n",
    "\n",
    "    all_vals = ts.values\n",
    "    recent_vals = recent.values\n",
    "    avg_vals = avg_recent.values\n",
    "    combined = np.concatenate([all_vals, recent_vals, avg_vals])\n",
    "    combined = combined[~np.isnan(combined)]\n",
    "    if combined.size:\n",
    "        bins = np.linspace(combined.min(), combined.max(), 41)\n",
    "        def plot_bar(vals, label):\n",
    "            weights = np.ones_like(vals) / len(vals)\n",
    "            ax.hist(vals, bins=bins, weights=weights, alpha=0.5,\n",
    "                    edgecolor='black', label=label)\n",
    "        if all_vals.size:\n",
    "            plot_bar(all_vals, 'All project-periods')\n",
    "        if recent_vals.size:\n",
    "            plot_bar(recent_vals, f'Last {last_n} project-periods')\n",
    "        if avg_vals.size:\n",
    "            plot_bar(avg_vals, f'Projects (over last {last_n} periods)')\n",
    "        ax.set_xlabel(display_name)\n",
    "        ax.set_ylabel('Proportion')\n",
    "        ax.set_title(f'Aggregate distribution of {display_name}')\n",
    "        ax.legend()\n",
    "\n",
    "    return ts, recent\n",
    "\n",
    "def SummarizeProblemCounts(df_problems, metrics, last_n=5, nrows=1, weight_col = None):\n",
    "    n_metrics = len(metrics)\n",
    "    ncols = int(np.ceil(n_metrics / nrows))\n",
    "    fig, axes = plt.subplots(nrows, ncols, figsize=(6 * ncols, 5 * nrows))\n",
    "    axes = np.array(axes).reshape(-1)\n",
    "\n",
    "    for idx, (metric_col, display_name, description) in enumerate(metrics):\n",
    "        print(f\"Distribution of {description} per project-period, pre departure\")\n",
    "        mask = df_problems[metric_col].notna()\n",
    "        summary = (df_problems.loc[mask, ['repo_name', 'time_period']].value_counts().describe().to_frame().T.round(2))\n",
    "        print(summary)\n",
    "        SummarizeAndPlotCollaboration(df_problems, metric_col, display_name, last_n, axes[idx], weight_col)\n",
    "        print(\"\\n\")\n",
    "    for j in range(n_metrics, nrows * ncols):\n",
    "        fig.delaxes(axes[j])\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "085ee169-38cb-4708-92fb-4ab6ac2de343",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "metrics = [\n",
    "    ('ind_collab_roll',       'Ov. Indicator Collaboration',   'problem count'),\n",
    "    ('cont_collab_roll',       'Ov. Continuous Collaboration',   'problem count'),\n",
    "    ('ind_key_collab_roll',   'Key Indicator Collaboration',   'key problem count'),\n",
    "    ('ind_other_collab_roll', 'Other Indicator Collaboration', 'other problem count'),\n",
    "]\n",
    "\n",
    "SummarizeProblemCounts(df_problems_contr_filtered_pre_period, metrics, last_n=5, nrows=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbc7722d-575b-41f9-b178-b37a3e134da3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# look into why weighting makes projects less collaborative?\n",
    "\"\"\"\n",
    "metrics = [\n",
    "    ('ind_collab_roll',       'Ov. Indicator Collaboration',   'problem count'),\n",
    "    ('cont_collab_roll',       'Ov. Continuous Collaboration',   'problem count'),\n",
    "    ('ind_key_collab_roll',   'Key Indicator Collaboration',   'key problem count'),\n",
    "    ('ind_other_collab_roll', 'Other Indicator Collaboration', 'other problem count'),\n",
    "]\n",
    "\n",
    "SummarizeProblemCounts(df_problems_contr_filtered_pre_period, metrics, last_n=5, nrows=2, weight_col = 'total_prob_contr')\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d81cdcdf-4ae0-4a4f-afd9-d026c18cd012",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dept_present = df_problems_contr_filtered_pre_period[df_problems_contr_filtered_pre_period.apply(\n",
    "    lambda x: x['departed_actor_id'] in x['all_actors'], axis = 1)][['repo_name','time_period']].drop_duplicates()\n",
    "print(pd.merge(df_dept_present, df_problems_contr_filtered_pre_period).apply(lambda x: x['departed_actor_id'] in x['all_actors'], axis = 1).mean())\n",
    "print(1-df_problems_contr_filtered_pre_period['ind_key_collab_roll'].isna().mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fcfe637-55ec-47fa-a94c-6b2db62e8dae",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import statsmodels.formula.api as smf\n",
    "\n",
    "def run_collaboration_trend(df_problems_contr_filtered_pre_period, collab_col, last_n_periods=None, repo_fe=True, time_fe=True):\n",
    "    df_summary = (\n",
    "        df_problems_contr_filtered_pre_period\n",
    "        .groupby(['repo_name', 'time_period'])[collab_col]\n",
    "        .mean()\n",
    "        .reset_index()\n",
    "    )\n",
    "    df_summary = df_summary[~df_summary[collab_col].isna()]\n",
    "\n",
    "    df_summary['time_period_number'] = df_summary.groupby('repo_name')['time_period'].transform(\n",
    "        lambda x: x.map({\n",
    "            tp: i - (len(sorted(x.unique())) - 1)\n",
    "            for i, tp in enumerate(sorted(x.unique()))\n",
    "        })\n",
    "    )\n",
    "\n",
    "    if last_n_periods is not None:\n",
    "        df_summary = df_summary[df_summary['time_period_number'] >= -last_n_periods].copy()\n",
    "\n",
    "    repofe = '+ C(repo_name)' if repo_fe else ''\n",
    "    timefe = '+ C(time_period)' if time_fe else ''\n",
    "\n",
    "    model = smf.ols(\n",
    "        formula=f'{collab_col} ~ time_period_number {repofe} {timefe}',\n",
    "        data=df_summary\n",
    "    ).fit(\n",
    "        cov_type='cluster',\n",
    "        cov_kwds={'groups': df_summary['repo_name']}\n",
    "    )\n",
    "\n",
    "    coef_table = model.summary2().tables[1].copy()\n",
    "    coef_table = coef_table[~coef_table.index.str.startswith('C(repo_name)')]\n",
    "\n",
    "    return coef_table\n",
    "\n",
    "def GenerateReport(df_problems_contr, collab_col, annualize=False):\n",
    "    configs = [\n",
    "        {'last_n_periods': None, 'repo_fe': False, 'time_fe': False, 'period_label': 'All periods'},\n",
    "        {'last_n_periods': 5, 'repo_fe': False, 'time_fe': False, 'period_label': 'Last 5 periods'},\n",
    "        {'last_n_periods': None, 'repo_fe': True, 'time_fe': True, 'period_label': 'All periods (FE)'},\n",
    "        {'last_n_periods': 5, 'repo_fe': True, 'time_fe': True, 'period_label': 'Last 5 periods (FE)'}\n",
    "    ]\n",
    "    \n",
    "    results = []\n",
    "    for config in configs:\n",
    "        coef_table = run_collaboration_trend(\n",
    "            df_problems_contr,\n",
    "            collab_col=collab_col,\n",
    "            last_n_periods=config['last_n_periods'],\n",
    "            repo_fe=config['repo_fe'],\n",
    "            time_fe=config['time_fe']\n",
    "        )\n",
    "        if annualize:\n",
    "            coef_table.loc[['time_period_number'], 'Coef.'] *= 2\n",
    "            coef_table.loc[['time_period_number'], 'Std.Err.'] *= 2\n",
    "\n",
    "        results.append({\n",
    "            'Period Range': config['period_label'],\n",
    "            'Coefficient': f\"{coef_table.loc[['time_period_number']]['Coef.'].values[0]:.3f}\",\n",
    "            'Std.Err.': f\"({coef_table.loc[['time_period_number']]['Std.Err.'].values[0]:.3f})\",\n",
    "            'Period FE': 'Yes' if config['time_fe'] else 'No',\n",
    "            'Project FE': 'Yes' if config['repo_fe'] else 'No'\n",
    "        })\n",
    "    \n",
    "    result_df = pd.DataFrame(results)\n",
    "\n",
    "    print(result_df.to_markdown(index=False))\n",
    "\n",
    "for idx, (metric_col, display_name, description) in enumerate(metrics):\n",
    "    print(f\"Collaboration stability for {display_name}\")\n",
    "    print(metric_col)\n",
    "    GenerateReport(df_problems_contr_filtered_pre_period, collab_col = metric_col, annualize = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5ebca61-8bb2-44ef-b705-b53ee7285a86",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ComputeLagCorrelation(series):\n",
    "    return series.corr(series.shift(1))\n",
    "\n",
    "collab_type = 'ind_collab_roll'\n",
    "for min_denom in [1,5, 10, 20, 50]:\n",
    "    project_time_collab = (\n",
    "        df_problems_contr_filtered_pre_period\n",
    "        .groupby(['repo_name','time_period'])[collab_type]\n",
    "        .agg(count='size', mean_collab='mean')\n",
    "        .reset_index()\n",
    "        .sort_values(['repo_name','time_period'])\n",
    "    )\n",
    "    project_time_collab['mean_collab'] = project_time_collab['mean_collab'].where(\n",
    "        project_time_collab['count'] >= min_denom, np.nan\n",
    "    )\n",
    "    project_time_collab_recent = project_time_collab.groupby('repo_name').tail(5)\n",
    "    \n",
    "    print(f\"Minimum problem count in a time period: {min_denom}\")\n",
    "    print(\"Prop. missing months: {:.2f}\".format(project_time_collab['mean_collab'].isna().mean()))\n",
    "    print(\"Prop. missing months (recent): {:.2f}\".format(project_time_collab_recent['mean_collab'].isna().mean()))\n",
    "    print(\"Across all projects, the average absolute correlation between collaboration in time t and t-1 is {:.2f}\".format(\n",
    "        project_time_collab.groupby('repo_name')['mean_collab'].apply(ComputeLagCorrelation).mean()))\n",
    "    print(\"Across all projects, in the 5 periods prior to departure, the average absolute correlation between collaboration in time t and t-1 is {:.2f}\\n\".format(\n",
    "        project_time_collab_recent.groupby('repo_name')['mean_collab'].apply(ComputeLagCorrelation).mean()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "885277d7-fb6c-45eb-a773-1885c8aa27eb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "def CompareAboveBelowCollab(collab_type = 'ind_collab_roll', min_denom   = 5):\n",
    "    \n",
    "    ptc = (\n",
    "        df_problems_contr_filtered_pre_period\n",
    "        .groupby(['repo_name','time_period'])[collab_type]\n",
    "        .agg(count='size', mean_collab='mean')\n",
    "        .reset_index()\n",
    "        .sort_values(['repo_name','time_period'])\n",
    "    )\n",
    "    \n",
    "    rows = []\n",
    "    for baseline in (1, 2, 3, 4):\n",
    "        # — baseline window\n",
    "        base_df = ptc.groupby('repo_name').tail(baseline)\n",
    "        valid_baseline = base_df.groupby('repo_name')['count'].sum().ge(min_denom)\n",
    "        base_df = base_df[base_df['repo_name'].isin(valid_baseline[valid_baseline].index)]\n",
    "        avg_collab = WeightedMean(base_df['mean_collab'], base_df['count'])\n",
    "        base_wm = base_df.groupby('repo_name') \\\n",
    "                         .apply(lambda df: WeightedMean(df['mean_collab'], df['count']))\n",
    "        above_set = set(base_wm[base_wm > avg_collab].index)\n",
    "        below_set = set(base_wm[base_wm <= avg_collab].index)\n",
    "        above_n, below_n = len(above_set), len(below_set)\n",
    "    \n",
    "        for periods in range(baseline + 1, 6):\n",
    "            # — periods window\n",
    "            recent_df = ptc.groupby('repo_name').tail(periods)\n",
    "            valid_periods = recent_df.groupby('repo_name')['count'].sum().ge(min_denom)\n",
    "            recent_df = recent_df[recent_df['repo_name'].isin(valid_periods[valid_periods].index)]\n",
    "    \n",
    "            recent_wm = recent_df.groupby('repo_name') \\\n",
    "                                 .apply(lambda df: WeightedMean(df['mean_collab'], df['count']))\n",
    "    \n",
    "            kept_above = len(above_set & set(recent_wm[recent_wm > avg_collab].index))\n",
    "            kept_below = len(below_set & set(recent_wm[recent_wm <= avg_collab].index))\n",
    "    \n",
    "            rows.append({\n",
    "                'baseline': baseline,\n",
    "                'periods': periods,\n",
    "                'above_retention': kept_above / (above_n or 1),\n",
    "                'below_retention': kept_below / (below_n or 1),\n",
    "                'combined_retention': (kept_above + kept_below) / ((above_n + below_n) or 1)\n",
    "            })\n",
    "    \n",
    "    retention_df = pd.DataFrame(rows)\n",
    "    print(retention_df.to_string(index=False))\n",
    "\n",
    "CompareAboveBelowCollab(collab_type = 'ind_collab_roll', min_denom   = 5)\n",
    "CompareAboveBelowCollab(collab_type = 'ind_key_collab_roll', min_denom   = 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2e52469-86a3-40ab-a905-d9aac58893b5",
   "metadata": {},
   "source": [
    "# this is for overall collaboration. it's also true for key collaboration\n",
    "Goal: can I learn something about how collaborative projects are through my collaboration metric?\n",
    "1. There's some weak evidence that collaboration is trending downwards over time. I say weak because it's statistically insignificant\n",
    "2. Across projects, the average correlation between current and lagged collaboration is between -0.1 and 0.2, which also suggests that there isn't a persistent upwards/downwards trend in collaboration over time\n",
    "3. The good thing is that it looks like collaboration is reasonably stable over time. I know this because if I use 1, 2, 3, 4, 5 past time periods to bin projects into above/below average collaboration, the overlap between the new and old bins is always over 85% and frequently exceeds 90% (depending on the comparison made)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffbc61f3-b428-4da9-af98-061b58f9e204",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dept_present = df_problems_contr_filtered_pre_period[df_problems_contr_filtered_pre_period.apply(\n",
    "    lambda x: x['departed_actor_id'] in x['all_actors'], axis = 1)][['repo_name','time_period']].drop_duplicates()\n",
    "print(pd.merge(df_dept_present, df_problems_contr_filtered_pre_period).apply(lambda x: x['departed_actor_id'] in x['all_actors'], axis = 1).mean())\n",
    "print(1-df_problems_contr_filtered_pre_period['ind_key_collab_roll'].isna().mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30ada05f-be53-4813-bf7e-a33df24e2d3b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79d9f46c-2524-41ff-82c9-44ebdea26742",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
